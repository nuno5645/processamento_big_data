{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema de Estudo:** Deteção de Reviews Spam em Produtos de Moda e Acessórios na Amazon.\n",
    "\n",
    "Pretende-se que seja implementada uma solução computacional para estudo e análise de dados em larga escala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextualização do problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste projeto, pretende-se desenvolver e implementar um modelo computacional capaz de identificar reviews de produtos da categoria \"Clothing, Shoes, and Jewelry\" na Amazon como spam ou não spam. O objetivo é diferenciar reviews genuínas de tentativas de manipulação através de avaliações falsas que podem enganar consumidores e distorcer a percepção do produto. Este problema é particularmente desafiador devido à subjetividade e variabilidade do texto das reviews, bem como às diferentes motivações por trás das reviews spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criar uma sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Clothing_Shoes_and_Jewelry_Analysis\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Caminho para o arquivo JSON\n",
    "file_path = \"Clothing_Shoes_and_Jewelry.json\"\n",
    "\n",
    "# Carregar o dataset\n",
    "df = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|                 _id|      asin|            category|class|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+--------------------+----------+--------------------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|{5a132293741a2384...|0000031887|Clothing_Shoes_an...|  0.0| [0, 1]|    1.0|My 3-yr-old daugh...|03 21, 2013| A19PBP93OF896|Alinna Satake \"Ca...|Tiny and Poorly C...|    1363824000|\n",
      "|{5a132293741a2384...|0000031887|Clothing_Shoes_an...|  1.0| [1, 1]|    4.0|This was a really...|05 26, 2012|A2G0LNLN79Q6HR|       aj_18 \"Aj_18\"|Really Cute but r...|    1337990400|\n",
      "|{5a132293741a2384...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|Perfect red tutu ...| 11 4, 2013|A2XVJBSRI3SWDI|             abigail|           Nice tutu|    1383523200|\n",
      "|{5a132293741a2384...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    4.0|Bought it for my ...|01 23, 2014|A1P0IHU93EF9ZK|              Amanda|           i love it|    1390435200|\n",
      "|{5a132293741a2384...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|This is a great t...|02 12, 2011|A1KLRMWW2FWPL4|Amazon Customer \"...|Great tutu-  not ...|    1297468800|\n",
      "+--------------------+----------+--------------------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar as primeiras 5 linhas do dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- $oid: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- class: double (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver a estrutura dos dados\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5504331"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar o número de linhas do dataframe\n",
    "rows = df.count()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar o número de colunas do dataframe\n",
    "cols = len(df.columns)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id',\n",
       " 'asin',\n",
       " 'category',\n",
       " 'class',\n",
       " 'helpful',\n",
       " 'overall',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colunas do dataframe\n",
    "cols = df.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'_id':** Identificador único de cada review\n",
    "\n",
    "**'asin':** Amazon Standard Identification Number, que é um identificador único para produtos na plataforma da Amazon\n",
    "\n",
    "**'category':** Categoria do produto da review, que indica a que segmento do mercado o produto pertence, como roupas, sapatos ou joias\n",
    "\n",
    "**'class':** Distingue reviews como spam (1.0) ou não spam (0.0)\n",
    "\n",
    "**'helpful':** Array que contém dois números, onde o primeiro indica quantas pessoas acharam a review útil e o segundo quantas pessoas votaram na utilidade da review\n",
    "\n",
    "**'overall':** Nota geral dada ao produto pelo usuário\n",
    "\n",
    "**'reviewText':** Texto completo da review escrita pelo usuário\n",
    "\n",
    "**'reviewTime':** Data em que a review foi publicada\n",
    "\n",
    "**'reviewerID':** Identificador único da pessoa que escreveu a avaliação\n",
    "\n",
    "**'reviewerName':** Nome ou pseudonimo do revisor, conforme apresentado na Amazon no momento da review\n",
    "\n",
    "**'summary':** Resumo da review, que é uma breve descrição ou título que foi dada na avaliação\n",
    "\n",
    "**'unixReviewTime':** Representação em timestamp UNIX da data de publicação da review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Valores omissos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================================>                  (16 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-----+-------+-------+----------+----------+----------+------------+-------+--------------+\n",
      "|_id|asin|category|class|helpful|overall|reviewText|reviewTime|reviewerID|reviewerName|summary|unixReviewTime|\n",
      "+---+----+--------+-----+-------+-------+----------+----------+----------+------------+-------+--------------+\n",
      "|  0|   0|       0|    0|      0|      0|         0|         0|         0|       13180|      0|             0|\n",
      "+---+----+--------+-----+-------+-------+----------+----------+----------+------------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Verificar os valores omissos\n",
    "from pyspark.sql.functions import col, isnull, when, count\n",
    "\n",
    "missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:============================================>           (19 + 5) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|                 _id|      asin|            category|class|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+--------------------+----------+--------------------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|{5a13229b741a2384...|B0008EOEPK|Clothing_Shoes_an...|  1.0| [0, 0]|    4.0|Lee straight leg,...| 01 1, 2014|A2HJ1VX57R6M6L|         Smiling Bob|     Excellent Jeans|    1388534400|\n",
      "|{5a132298741a2384...|B00075ZYRW|Clothing_Shoes_an...|  1.0| [0, 0]|    4.0|Fits 3x. After wa...| 01 1, 2014|A3VTTTYIB5BANY|           fabfamily|liked them. order...|    1388534400|\n",
      "|{5a132296741a2384...|B00023JSDA|Clothing_Shoes_an...|  1.0| [1, 1]|    4.0|The quality/workm...|01 10, 2007|A3TTYJYJJ2Q5TA| Vicki H. \"CA_Vicki\"|     Garnet Necklace|    1168387200|\n",
      "|{5a132298741a2384...|B0006H0PLG|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|These are perfect...|01 10, 2013|A291VG071KOJLQ|Kara E. Henson Ol...|Perfect comfy fit...|    1357776000|\n",
      "|{5a132294741a2384...|B0000BX8L1|Clothing_Shoes_an...|  1.0| [0, 1]|    5.0|I bought two of t...|01 10, 2013|A2GJHM6USEHK2V|    Angela M. Zirkle|A Perfect First W...|    1357776000|\n",
      "+--------------------+----------+--------------------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Remover os duplicados recorrendo ao método dropDuplicates\n",
    "original_count = df.count()\n",
    "df = df.dropDuplicates()\n",
    "new_count = df.count()\n",
    "\n",
    "duplicates_count = original_count - new_count\n",
    "print(f\"Number of duplicate rows: {duplicates_count}\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remover caracteres especiais da coluna '_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|                 _id|      asin|            category|class|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+--------------------+----------+--------------------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  0.0| [0, 1]|    1.0|My 3-yr-old daugh...|03 21, 2013| A19PBP93OF896|Alinna Satake \"Ca...|Tiny and Poorly C...|    1363824000|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [1, 1]|    4.0|This was a really...|05 26, 2012|A2G0LNLN79Q6HR|       aj_18 \"Aj_18\"|Really Cute but r...|    1337990400|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|Perfect red tutu ...| 11 4, 2013|A2XVJBSRI3SWDI|             abigail|           Nice tutu|    1383523200|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    4.0|Bought it for my ...|01 23, 2014|A1P0IHU93EF9ZK|              Amanda|           i love it|    1390435200|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|This is a great t...|02 12, 2011|A1KLRMWW2FWPL4|Amazon Customer \"...|Great tutu-  not ...|    1297468800|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|Got this for our ...|12 26, 2012|A1GQPAM8Y45QN7|     Amazon Customer|                Tutu|    1356480000|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  0.0| [1, 1]|    2.0|the tutu color wa...|02 17, 2013|A2R3K1KX09QBYP|      alert consumer|not very good mat...|    1361059200|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|Just as described...|06 22, 2013| AEAN37KUOYSX4|     Amazon Customer|          Fantastic!|    1371859200|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|I bought this for...|01 19, 2013|A2G5TCU2WDFZ65|     Amazon Customer|         Very Cute!!|    1358553600|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|This really is a ...| 03 4, 2013|A2QEODSEIT1ME2|     Amazon Customer|           Very Nice|    1362355200|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [3, 4]|    4.0|I ordered this fo...|03 16, 2010|A3Q6CTO56DJ8UZ|      Amazing Amazon|       Good Quality!|    1268697600|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|Vey cute and perf...|02 18, 2014|A3CHBY0CB0O7PP|     Amazon Customer|                cute|    1392681600|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|Loved it and so d...|11 21, 2012|A1Z4XQ937SMPO3|             amstier|        super cute!!|    1353456000|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|Purchased it for ...|06 16, 2014|A1MCJONUQ78L9T|     Amazon Customer|My daughter loved...|    1402876800|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    4.0|Very cute, shorte...|12 10, 2013|A2PSIVW9I3TGHD|     Amazon Customer|        Good product|    1386633600|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|Our 3-year-old pe...|08 24, 2013| AZMKXP68HZ4CN|      Angela K. Ford|          Wonderful!|    1377302400|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  0.0| [0, 0]|    3.0|The waistband was...|12 28, 2012| A5GPS8FCXTPPI|          Ann C Peat|           Waistband|    1356652800|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|The tutu's was fo...|01 15, 2014| AM1CYMR007O6H|         anne foster|              Tutus!|    1389744000|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    5.0|My 5 year old dau...|12 25, 2013|A25G580SJ513IC|          Anne Klein|         Very happy!|    1387929600|\n",
      "|5a132293741a2384e...|0000031887|Clothing_Shoes_an...|  1.0| [0, 0]|    4.0|I just got this t...|10 24, 2013|A2F9MVWWC0IZYW|                 Amy|Perfect for the p...|    1382572800|\n",
      "+--------------------+----------+--------------------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Remover caracteres especiais da coluna '_id'\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "cleaned_df = df.withColumn(\"_id\", col(\"_id\").getField(\"$oid\"))\n",
    "cleaned_df = cleaned_df.withColumn(\"_id\", regexp_replace(col(\"_id\"), \"[{}]\", \"\"))\n",
    "\n",
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remover colunas que não são relevantes para o estudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|                 _id|      asin|class|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+--------------------+----------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|5a132293741a2384e...|0000031887|  0.0| [0, 1]|    1.0|My 3-yr-old daugh...|03 21, 2013| A19PBP93OF896|Alinna Satake \"Ca...|Tiny and Poorly C...|    1363824000|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [1, 1]|    4.0|This was a really...|05 26, 2012|A2G0LNLN79Q6HR|       aj_18 \"Aj_18\"|Really Cute but r...|    1337990400|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|Perfect red tutu ...| 11 4, 2013|A2XVJBSRI3SWDI|             abigail|           Nice tutu|    1383523200|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    4.0|Bought it for my ...|01 23, 2014|A1P0IHU93EF9ZK|              Amanda|           i love it|    1390435200|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|This is a great t...|02 12, 2011|A1KLRMWW2FWPL4|Amazon Customer \"...|Great tutu-  not ...|    1297468800|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|Got this for our ...|12 26, 2012|A1GQPAM8Y45QN7|     Amazon Customer|                Tutu|    1356480000|\n",
      "|5a132293741a2384e...|0000031887|  0.0| [1, 1]|    2.0|the tutu color wa...|02 17, 2013|A2R3K1KX09QBYP|      alert consumer|not very good mat...|    1361059200|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|Just as described...|06 22, 2013| AEAN37KUOYSX4|     Amazon Customer|          Fantastic!|    1371859200|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|I bought this for...|01 19, 2013|A2G5TCU2WDFZ65|     Amazon Customer|         Very Cute!!|    1358553600|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|This really is a ...| 03 4, 2013|A2QEODSEIT1ME2|     Amazon Customer|           Very Nice|    1362355200|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [3, 4]|    4.0|I ordered this fo...|03 16, 2010|A3Q6CTO56DJ8UZ|      Amazing Amazon|       Good Quality!|    1268697600|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|Vey cute and perf...|02 18, 2014|A3CHBY0CB0O7PP|     Amazon Customer|                cute|    1392681600|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|Loved it and so d...|11 21, 2012|A1Z4XQ937SMPO3|             amstier|        super cute!!|    1353456000|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|Purchased it for ...|06 16, 2014|A1MCJONUQ78L9T|     Amazon Customer|My daughter loved...|    1402876800|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    4.0|Very cute, shorte...|12 10, 2013|A2PSIVW9I3TGHD|     Amazon Customer|        Good product|    1386633600|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|Our 3-year-old pe...|08 24, 2013| AZMKXP68HZ4CN|      Angela K. Ford|          Wonderful!|    1377302400|\n",
      "|5a132293741a2384e...|0000031887|  0.0| [0, 0]|    3.0|The waistband was...|12 28, 2012| A5GPS8FCXTPPI|          Ann C Peat|           Waistband|    1356652800|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|The tutu's was fo...|01 15, 2014| AM1CYMR007O6H|         anne foster|              Tutus!|    1389744000|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    5.0|My 5 year old dau...|12 25, 2013|A25G580SJ513IC|          Anne Klein|         Very happy!|    1387929600|\n",
      "|5a132293741a2384e...|0000031887|  1.0| [0, 0]|    4.0|I just got this t...|10 24, 2013|A2F9MVWWC0IZYW|                 Amy|Perfect for the p...|    1382572800|\n",
      "+--------------------+----------+-----+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remover colunas que não são necessárias para o problema em questão\n",
    "cleaned_df = cleaned_df.drop( 'category')\n",
    "#cleaned_df = cleaned_df.drop( 'category', 'asin' , 'reviewerID', 'reviewerName') \n",
    "\n",
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Renomear algumas colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomear as colunas 'overall' e '-id' \n",
    "cleaned_df = cleaned_df.withColumnRenamed(\"overall\", \"productRating\")\\\n",
    "            .withColumnRenamed(\"_id\", \"id\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:================================================>      (21 + 3) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+-------------+--------------------+-----------+--------------------+--------------+\n",
      "|                  id|class|helpful|productRating|          reviewText| reviewTime|             summary|unixReviewTime|\n",
      "+--------------------+-----+-------+-------------+--------------------+-----------+--------------------+--------------+\n",
      "|5a13229b741a2384e...|  1.0| [0, 0]|          4.0|Lee straight leg,...| 01 1, 2014|     Excellent Jeans|    1388534400|\n",
      "|5a132298741a2384e...|  1.0| [0, 0]|          4.0|Fits 3x. After wa...| 01 1, 2014|liked them. order...|    1388534400|\n",
      "|5a132296741a2384e...|  1.0| [1, 1]|          4.0|The quality/workm...|01 10, 2007|     Garnet Necklace|    1168387200|\n",
      "|5a132298741a2384e...|  1.0| [0, 0]|          5.0|These are perfect...|01 10, 2013|Perfect comfy fit...|    1357776000|\n",
      "|5a132294741a2384e...|  1.0| [0, 1]|          5.0|I bought two of t...|01 10, 2013|A Perfect First W...|    1357776000|\n",
      "|5a132296741a2384e...|  1.0| [2, 2]|          5.0|I had this watch ...|01 10, 2013|Excelent watch fo...|    1357776000|\n",
      "|5a132296741a2384e...|  0.0| [0, 0]|          3.0|I bought these be...|01 10, 2013|sturdy but too he...|    1357776000|\n",
      "|5a13229a741a2384e...|  1.0| [0, 0]|          5.0|I have worn this ...|01 10, 2014| Loving my new bra's|    1389312000|\n",
      "|5a132296741a2384e...|  0.0| [0, 0]|          2.0|I run in 9.5 Asic...|01 10, 2014|Didn't work for m...|    1389312000|\n",
      "|5a132299741a2384e...|  1.0| [0, 0]|          5.0|We ordered the Wa...|01 10, 2014|Excited about our...|    1389312000|\n",
      "|5a132299741a2384e...|  1.0| [1, 1]|          5.0|I have owned my C...|01 11, 2007|Citizen Eco-Drive...|    1168473600|\n",
      "|5a132294741a2384e...|  1.0| [1, 1]|          4.0|This was a gift a...|01 11, 2007|Very nice for the...|    1168473600|\n",
      "|5a132294741a2384e...|  1.0| [2, 2]|          5.0|Gave these very p...|01 11, 2007|           Beautiful|    1168473600|\n",
      "|5a132294741a2384e...|  1.0| [4, 4]|          5.0|This is the secon...|01 11, 2011|             stylish|    1294704000|\n",
      "|5a132297741a2384e...|  1.0| [0, 0]|          5.0|These boots fit g...|01 11, 2013|     excellent boots|    1357862400|\n",
      "|5a132299741a2384e...|  1.0| [0, 0]|          4.0|The seams along t...|01 11, 2013|A couple of probl...|    1357862400|\n",
      "|5a132299741a2384e...|  1.0| [0, 0]|          5.0|Bought these back...|01 11, 2013|    Love these shoes|    1357862400|\n",
      "|5a132296741a2384e...|  1.0| [1, 1]|          5.0|I had always assu...|01 12, 2007|Beautiful Freshwa...|    1168560000|\n",
      "|5a132294741a2384e...|  1.0| [3, 3]|          4.0|For what I paid, ...|01 12, 2008|                Fine|    1200096000|\n",
      "|5a13229c741a2384e...|  1.0| [0, 0]|          5.0|Way worth the mon...|01 12, 2009|            good buy|    1231718400|\n",
      "+--------------------+-----+-------+-------------+--------------------+-----------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modifica a coluna 'helpful' para refletir a taxa de votos uteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocar o número de votos recebidos como uma métrica de 'Helpful'\n",
    "cleaned_df = cleaned_df.withColumn('reviewUpvotes', col('helpful')[0])\n",
    "\n",
    "#cleaned_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificar a coluna 'helpful' original para um valor float que mostra o número de pessoas que acharam útil entre o total de pessoas que a viram\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def helpful_ratio(helpful):\n",
    "    try:\n",
    "        return (helpful[0] / helpful[1])*100\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0  # retorna 0.0 se não houver votos\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# UDF para calcular a taxa de votos úteis \n",
    "ratio_udf = udf(helpful_ratio, FloatType())\n",
    "\n",
    "# Aplicar a UDF ao DataFrame\n",
    "cleaned_df =  cleaned_df.withColumn('helpful', ratio_udf(col('helpful')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+-------------+--------------------+-----------+--------------------+--------------+-------------+\n",
      "|                  id|class|helpful|productRating|          reviewText| reviewTime|             summary|unixReviewTime|reviewUpvotes|\n",
      "+--------------------+-----+-------+-------------+--------------------+-----------+--------------------+--------------+-------------+\n",
      "|5a132293741a2384e...|  0.0|    0.0|          1.0|My 3-yr-old daugh...|03 21, 2013|Tiny and Poorly C...|    1363824000|            0|\n",
      "|5a132293741a2384e...|  1.0|  100.0|          4.0|This was a really...|05 26, 2012|Really Cute but r...|    1337990400|            1|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|Perfect red tutu ...| 11 4, 2013|           Nice tutu|    1383523200|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          4.0|Bought it for my ...|01 23, 2014|           i love it|    1390435200|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|This is a great t...|02 12, 2011|Great tutu-  not ...|    1297468800|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|Got this for our ...|12 26, 2012|                Tutu|    1356480000|            0|\n",
      "|5a132293741a2384e...|  0.0|  100.0|          2.0|the tutu color wa...|02 17, 2013|not very good mat...|    1361059200|            1|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|Just as described...|06 22, 2013|          Fantastic!|    1371859200|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|I bought this for...|01 19, 2013|         Very Cute!!|    1358553600|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|This really is a ...| 03 4, 2013|           Very Nice|    1362355200|            0|\n",
      "|5a132293741a2384e...|  1.0|   75.0|          4.0|I ordered this fo...|03 16, 2010|       Good Quality!|    1268697600|            3|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|Vey cute and perf...|02 18, 2014|                cute|    1392681600|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|Loved it and so d...|11 21, 2012|        super cute!!|    1353456000|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|Purchased it for ...|06 16, 2014|My daughter loved...|    1402876800|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          4.0|Very cute, shorte...|12 10, 2013|        Good product|    1386633600|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|Our 3-year-old pe...|08 24, 2013|          Wonderful!|    1377302400|            0|\n",
      "|5a132293741a2384e...|  0.0|    0.0|          3.0|The waistband was...|12 28, 2012|           Waistband|    1356652800|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|The tutu's was fo...|01 15, 2014|              Tutus!|    1389744000|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          5.0|My 5 year old dau...|12 25, 2013|         Very happy!|    1387929600|            0|\n",
      "|5a132293741a2384e...|  1.0|    0.0|          4.0|I just got this t...|10 24, 2013|Perfect for the p...|    1382572800|            0|\n",
      "+--------------------+-----+-------+-------------+--------------------+-----------+--------------------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomear a coluna 'helpful' para 'helpfulTotalRatio'\n",
    "cleaned_df =cleaned_df.withColumnRenamed(\"helpful\", \"helpfulTotalRatio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'asin',\n",
       " 'class',\n",
       " 'helpfulTotalRatio',\n",
       " 'productRating',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime',\n",
       " 'reviewUpvotes']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converte a coluna 'reviewTime' para o formato de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+\n",
      "|                  id|      asin|class|helpfulTotalRatio|productRating|          reviewText|         reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|reviewUpvotes|\n",
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+\n",
      "|5a132293741a2384e...|0000031887|  0.0|              0.0|          1.0|My 3-yr-old daugh...|2013-03-21 00:00:00| A19PBP93OF896|Alinna Satake \"Ca...|Tiny and Poorly C...|    1363824000|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|            100.0|          4.0|This was a really...|2012-05-26 00:00:00|A2G0LNLN79Q6HR|       aj_18 \"Aj_18\"|Really Cute but r...|    1337990400|            1|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|Perfect red tutu ...|2013-11-04 00:00:00|A2XVJBSRI3SWDI|             abigail|           Nice tutu|    1383523200|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|Bought it for my ...|2014-01-23 00:00:00|A1P0IHU93EF9ZK|              Amanda|           i love it|    1390435200|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|This is a great t...|2011-02-12 00:00:00|A1KLRMWW2FWPL4|Amazon Customer \"...|Great tutu-  not ...|    1297468800|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|Got this for our ...|2012-12-26 00:00:00|A1GQPAM8Y45QN7|     Amazon Customer|                Tutu|    1356480000|            0|\n",
      "|5a132293741a2384e...|0000031887|  0.0|            100.0|          2.0|the tutu color wa...|2013-02-17 00:00:00|A2R3K1KX09QBYP|      alert consumer|not very good mat...|    1361059200|            1|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|Just as described...|2013-06-22 00:00:00| AEAN37KUOYSX4|     Amazon Customer|          Fantastic!|    1371859200|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|I bought this for...|2013-01-19 00:00:00|A2G5TCU2WDFZ65|     Amazon Customer|         Very Cute!!|    1358553600|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|This really is a ...|2013-03-04 00:00:00|A2QEODSEIT1ME2|     Amazon Customer|           Very Nice|    1362355200|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|             75.0|          4.0|I ordered this fo...|2010-03-16 00:00:00|A3Q6CTO56DJ8UZ|      Amazing Amazon|       Good Quality!|    1268697600|            3|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|Vey cute and perf...|2014-02-18 00:00:00|A3CHBY0CB0O7PP|     Amazon Customer|                cute|    1392681600|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|Loved it and so d...|2012-11-21 00:00:00|A1Z4XQ937SMPO3|             amstier|        super cute!!|    1353456000|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|Purchased it for ...|2014-06-16 00:00:00|A1MCJONUQ78L9T|     Amazon Customer|My daughter loved...|    1402876800|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|Very cute, shorte...|2013-12-10 00:00:00|A2PSIVW9I3TGHD|     Amazon Customer|        Good product|    1386633600|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|Our 3-year-old pe...|2013-08-24 00:00:00| AZMKXP68HZ4CN|      Angela K. Ford|          Wonderful!|    1377302400|            0|\n",
      "|5a132293741a2384e...|0000031887|  0.0|              0.0|          3.0|The waistband was...|2012-12-28 00:00:00| A5GPS8FCXTPPI|          Ann C Peat|           Waistband|    1356652800|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|The tutu's was fo...|2014-01-15 00:00:00| AM1CYMR007O6H|         anne foster|              Tutus!|    1389744000|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|My 5 year old dau...|2013-12-25 00:00:00|A25G580SJ513IC|          Anne Klein|         Very happy!|    1387929600|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|I just got this t...|2013-10-24 00:00:00|A2F9MVWWC0IZYW|                 Amy|Perfect for the p...|    1382572800|            0|\n",
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converter a coluna 'reviewTime' para o formato de data\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "cleaned_df = cleaned_df.withColumn(\"reviewTime\", to_timestamp(\"reviewTime\", \"MM dd, yyyy\"))\n",
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformação dos dados: remover pontuações, passar o texto para minusculas, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Converter todos os caracteres para minúsculas\n",
    "cleaned_df = cleaned_df.withColumn('reviewText', F.lower(cleaned_df['reviewText']))\n",
    "\n",
    "# Remover código HTML\n",
    "#cleaned_df = cleaned_df.withColumn('reviewText', F.regexp_replace(cleaned_df['reviewText'], '<[^>]+>', ''))\n",
    "\n",
    "# Remover URLs\n",
    "#cleaned_df = cleaned_df.withColumn('reviewText', F.regexp_replace(cleaned_df['reviewText'], 'http\\S+|www\\S+|https?\\:\\/\\/\\S+', ''))\n",
    "\n",
    "# Remover menções a usuários (não é comum em reviews da Amazon)\n",
    "#cleaned_df = cleaned_df.withColumn('reviewText', F.regexp_replace(cleaned_df['reviewText'], '@\\w+', ''))\n",
    "\n",
    "# Remover hashtags (não é comum em reviews da Amazon)\n",
    "#cleaned_df = cleaned_df.withColumn('reviewText', F.regexp_replace(cleaned_df['reviewText'], '#\\w+', ''))\n",
    "\n",
    "# Remover entidades HTML (&amp;, &lt;, etc.)\n",
    "#cleaned_df = cleaned_df.withColumn('reviewText', F.regexp_replace(cleaned_df['reviewText'], '&\\w+;', ''))\n",
    "\n",
    "# Substituir caracteres não alfanuméricos e pontuação por um espaço em branco\n",
    "cleaned_df = cleaned_df.withColumn('reviewText', F.regexp_replace(cleaned_df['reviewText'], '[^\\w\\s]', ' '))\n",
    "\n",
    "# Remover números (avaliações numéricas, preços, etc.)\n",
    "#cleaned_df = cleaned_df.withColumn('reviewText', F.regexp_replace(cleaned_df['reviewText'], '\\d+', ''))\n",
    "\n",
    "# Substituir dígitos por um espaço em branco\n",
    "#cleaned_df = cleaned_df.withColumn('reviewText', F.regexp_replace(cleaned_df['reviewText'], '\\d', ' '))\n",
    "\n",
    "# Remover espaços múltiplos e linhas novas\n",
    "#cleaned_df = cleaned_df.withColumn('reviewText', F.regexp_replace(cleaned_df['reviewText'], '\\s+', ' '))\n",
    "\n",
    "# Remover espaços no início e no fim\n",
    "#cleaned_df = cleaned_df.withColumn('reviewText', F.trim(cleaned_df['reviewText']))\n",
    "\n",
    "# cleaned_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+\n",
      "|                  id|      asin|class|helpfulTotalRatio|productRating|          reviewText|         reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|reviewUpvotes|\n",
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+\n",
      "|5a132293741a2384e...|0000031887|  0.0|              0.0|          1.0|my 3 yr old daugh...|2013-03-21 00:00:00| A19PBP93OF896|Alinna Satake \"Ca...|tiny and poorly c...|    1363824000|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|            100.0|          4.0|this was a really...|2012-05-26 00:00:00|A2G0LNLN79Q6HR|       aj_18 \"Aj_18\"|really cute but r...|    1337990400|            1|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|perfect red tutu ...|2013-11-04 00:00:00|A2XVJBSRI3SWDI|             abigail|           nice tutu|    1383523200|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|bought it for my ...|2014-01-23 00:00:00|A1P0IHU93EF9ZK|              Amanda|           i love it|    1390435200|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|this is a great t...|2011-02-12 00:00:00|A1KLRMWW2FWPL4|Amazon Customer \"...|great tutu  not c...|    1297468800|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|got this for our ...|2012-12-26 00:00:00|A1GQPAM8Y45QN7|     Amazon Customer|                tutu|    1356480000|            0|\n",
      "|5a132293741a2384e...|0000031887|  0.0|            100.0|          2.0|the tutu color wa...|2013-02-17 00:00:00|A2R3K1KX09QBYP|      alert consumer|not very good mat...|    1361059200|            1|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|just as described...|2013-06-22 00:00:00| AEAN37KUOYSX4|     Amazon Customer|           fantastic|    1371859200|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|i bought this for...|2013-01-19 00:00:00|A2G5TCU2WDFZ65|     Amazon Customer|           very cute|    1358553600|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|this really is a ...|2013-03-04 00:00:00|A2QEODSEIT1ME2|     Amazon Customer|           very nice|    1362355200|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|             75.0|          4.0|i ordered this fo...|2010-03-16 00:00:00|A3Q6CTO56DJ8UZ|      Amazing Amazon|        good quality|    1268697600|            3|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|vey cute and perf...|2014-02-18 00:00:00|A3CHBY0CB0O7PP|     Amazon Customer|                cute|    1392681600|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|loved it and so d...|2012-11-21 00:00:00|A1Z4XQ937SMPO3|             amstier|          super cute|    1353456000|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|purchased it for ...|2014-06-16 00:00:00|A1MCJONUQ78L9T|     Amazon Customer|my daughter loved it|    1402876800|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|very cute  shorte...|2013-12-10 00:00:00|A2PSIVW9I3TGHD|     Amazon Customer|        good product|    1386633600|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|our 3 year old pe...|2013-08-24 00:00:00| AZMKXP68HZ4CN|      Angela K. Ford|           wonderful|    1377302400|            0|\n",
      "|5a132293741a2384e...|0000031887|  0.0|              0.0|          3.0|the waistband was...|2012-12-28 00:00:00| A5GPS8FCXTPPI|          Ann C Peat|           waistband|    1356652800|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|the tutu s was fo...|2014-01-15 00:00:00| AM1CYMR007O6H|         anne foster|               tutus|    1389744000|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|my 5 year old dau...|2013-12-25 00:00:00|A25G580SJ513IC|          Anne Klein|          very happy|    1387929600|            0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|i just got this t...|2013-10-24 00:00:00|A2F9MVWWC0IZYW|                 Amy|perfect for the p...|    1382572800|            0|\n",
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "cleaned_df = cleaned_df.withColumn(\"summary\", lower(col(\"summary\")))\\\n",
    "                 .withColumn(\"summary\", regexp_replace(col(\"summary\"), \"[^\\w\\s]\", \"\"))\n",
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adiciona uma coluna que indica se a review foi feita no fim de semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, dayofweek, when\n",
    "\n",
    "# Adicionar uma coluna 'isWeekend' que indica se a review foi feita no fim de semana\n",
    "cleaned_df = cleaned_df.withColumn('isWeekend', when(dayofweek(col('reviewTime')) > 5, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+---------+\n",
      "|                  id|      asin|class|helpfulTotalRatio|productRating|          reviewText|         reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|reviewUpvotes|isWeekend|\n",
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+---------+\n",
      "|5a132293741a2384e...|0000031887|  0.0|              0.0|          1.0|my 3 yr old daugh...|2013-03-21 00:00:00| A19PBP93OF896|Alinna Satake \"Ca...|tiny and poorly c...|    1363824000|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|            100.0|          4.0|this was a really...|2012-05-26 00:00:00|A2G0LNLN79Q6HR|       aj_18 \"Aj_18\"|really cute but r...|    1337990400|            1|        1|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|perfect red tutu ...|2013-11-04 00:00:00|A2XVJBSRI3SWDI|             abigail|           nice tutu|    1383523200|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|bought it for my ...|2014-01-23 00:00:00|A1P0IHU93EF9ZK|              Amanda|           i love it|    1390435200|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|this is a great t...|2011-02-12 00:00:00|A1KLRMWW2FWPL4|Amazon Customer \"...|great tutu  not c...|    1297468800|            0|        1|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|got this for our ...|2012-12-26 00:00:00|A1GQPAM8Y45QN7|     Amazon Customer|                tutu|    1356480000|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  0.0|            100.0|          2.0|the tutu color wa...|2013-02-17 00:00:00|A2R3K1KX09QBYP|      alert consumer|not very good mat...|    1361059200|            1|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|just as described...|2013-06-22 00:00:00| AEAN37KUOYSX4|     Amazon Customer|           fantastic|    1371859200|            0|        1|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|i bought this for...|2013-01-19 00:00:00|A2G5TCU2WDFZ65|     Amazon Customer|           very cute|    1358553600|            0|        1|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|this really is a ...|2013-03-04 00:00:00|A2QEODSEIT1ME2|     Amazon Customer|           very nice|    1362355200|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|             75.0|          4.0|i ordered this fo...|2010-03-16 00:00:00|A3Q6CTO56DJ8UZ|      Amazing Amazon|        good quality|    1268697600|            3|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|vey cute and perf...|2014-02-18 00:00:00|A3CHBY0CB0O7PP|     Amazon Customer|                cute|    1392681600|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|loved it and so d...|2012-11-21 00:00:00|A1Z4XQ937SMPO3|             amstier|          super cute|    1353456000|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|purchased it for ...|2014-06-16 00:00:00|A1MCJONUQ78L9T|     Amazon Customer|my daughter loved it|    1402876800|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|very cute  shorte...|2013-12-10 00:00:00|A2PSIVW9I3TGHD|     Amazon Customer|        good product|    1386633600|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|our 3 year old pe...|2013-08-24 00:00:00| AZMKXP68HZ4CN|      Angela K. Ford|           wonderful|    1377302400|            0|        1|\n",
      "|5a132293741a2384e...|0000031887|  0.0|              0.0|          3.0|the waistband was...|2012-12-28 00:00:00| A5GPS8FCXTPPI|          Ann C Peat|           waistband|    1356652800|            0|        1|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|the tutu s was fo...|2014-01-15 00:00:00| AM1CYMR007O6H|         anne foster|               tutus|    1389744000|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|my 5 year old dau...|2013-12-25 00:00:00|A25G580SJ513IC|          Anne Klein|          very happy|    1387929600|            0|        0|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|i just got this t...|2013-10-24 00:00:00|A2F9MVWWC0IZYW|                 Amy|perfect for the p...|    1382572800|            0|        0|\n",
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criar colunas que indicam o comprimento da review e do sumário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+---------+------------+-------------+\n",
      "|                  id|      asin|class|helpfulTotalRatio|productRating|          reviewText|         reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|reviewUpvotes|isWeekend|reviewLength|summaryLength|\n",
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+---------+------------+-------------+\n",
      "|5a132293741a2384e...|0000031887|  0.0|              0.0|          1.0|my 3 yr old daugh...|2013-03-21 00:00:00| A19PBP93OF896|Alinna Satake \"Ca...|tiny and poorly c...|    1363824000|            0|        0|         672|           27|\n",
      "|5a132293741a2384e...|0000031887|  1.0|            100.0|          4.0|this was a really...|2012-05-26 00:00:00|A2G0LNLN79Q6HR|       aj_18 \"Aj_18\"|really cute but r...|    1337990400|            1|        1|         136|           28|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|perfect red tutu ...|2013-11-04 00:00:00|A2XVJBSRI3SWDI|             abigail|           nice tutu|    1383523200|            0|        0|         113|            9|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|bought it for my ...|2014-01-23 00:00:00|A1P0IHU93EF9ZK|              Amanda|           i love it|    1390435200|            0|        0|         143|            9|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|this is a great t...|2011-02-12 00:00:00|A1KLRMWW2FWPL4|Amazon Customer \"...|great tutu  not c...|    1297468800|            0|        1|         172|           28|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|got this for our ...|2012-12-26 00:00:00|A1GQPAM8Y45QN7|     Amazon Customer|                tutu|    1356480000|            0|        0|         109|            4|\n",
      "|5a132293741a2384e...|0000031887|  0.0|            100.0|          2.0|the tutu color wa...|2013-02-17 00:00:00|A2R3K1KX09QBYP|      alert consumer|not very good mat...|    1361059200|            1|        0|         236|           22|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|just as described...|2013-06-22 00:00:00| AEAN37KUOYSX4|     Amazon Customer|           fantastic|    1371859200|            0|        1|         236|            9|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|i bought this for...|2013-01-19 00:00:00|A2G5TCU2WDFZ65|     Amazon Customer|           very cute|    1358553600|            0|        1|         306|            9|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|this really is a ...|2013-03-04 00:00:00|A2QEODSEIT1ME2|     Amazon Customer|           very nice|    1362355200|            0|        0|         166|            9|\n",
      "|5a132293741a2384e...|0000031887|  1.0|             75.0|          4.0|i ordered this fo...|2010-03-16 00:00:00|A3Q6CTO56DJ8UZ|      Amazing Amazon|        good quality|    1268697600|            3|        0|         234|           12|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|vey cute and perf...|2014-02-18 00:00:00|A3CHBY0CB0O7PP|     Amazon Customer|                cute|    1392681600|            0|        0|         101|            4|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|loved it and so d...|2012-11-21 00:00:00|A1Z4XQ937SMPO3|             amstier|          super cute|    1353456000|            0|        0|         139|           10|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|purchased it for ...|2014-06-16 00:00:00|A1MCJONUQ78L9T|     Amazon Customer|my daughter loved it|    1402876800|            0|        0|         135|           20|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|very cute  shorte...|2013-12-10 00:00:00|A2PSIVW9I3TGHD|     Amazon Customer|        good product|    1386633600|            0|        0|         462|           12|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|our 3 year old pe...|2013-08-24 00:00:00| AZMKXP68HZ4CN|      Angela K. Ford|           wonderful|    1377302400|            0|        1|         134|            9|\n",
      "|5a132293741a2384e...|0000031887|  0.0|              0.0|          3.0|the waistband was...|2012-12-28 00:00:00| A5GPS8FCXTPPI|          Ann C Peat|           waistband|    1356652800|            0|        1|         123|            9|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|the tutu s was fo...|2014-01-15 00:00:00| AM1CYMR007O6H|         anne foster|               tutus|    1389744000|            0|        0|         187|            5|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          5.0|my 5 year old dau...|2013-12-25 00:00:00|A25G580SJ513IC|          Anne Klein|          very happy|    1387929600|            0|        0|         110|           10|\n",
      "|5a132293741a2384e...|0000031887|  1.0|              0.0|          4.0|i just got this t...|2013-10-24 00:00:00|A2F9MVWWC0IZYW|                 Amy|perfect for the p...|    1382572800|            0|        0|         701|           21|\n",
      "+--------------------+----------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+---------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "# Adicionar colunas que indicam o comprimento da review e do sumário\n",
    "cleaned_df = cleaned_df.withColumn('reviewLength', length(cleaned_df['reviewText']))\n",
    "cleaned_df = cleaned_df.withColumn('summaryLength', length(cleaned_df['summary']))\n",
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Calcular a popularidade do produto\n",
    "product_popularity_df = cleaned_df.groupBy('asin').agg(F.count('*').alias('productPopularity'))\n",
    "cleaned_df = cleaned_df.join(product_popularity_df, on='asin', how='left')\n",
    "\n",
    "# Calcular a média das avaliações do produto\n",
    "product_avg_rating_df = cleaned_df.groupBy('asin').agg(F.avg('productRating').alias('avgProductRating'))\n",
    "cleaned_df = cleaned_df.join(product_avg_rating_df, on='asin', how='left')\n",
    "\n",
    "# Criar um Bucketizer para dividir a coluna 'helpfulTotalRatio' em 3 categorias\n",
    "bucketizer = Bucketizer(splits=[0, 33.3, 66.6, 100], inputCol=\"helpfulTotalRatio\", outputCol=\"helpfulRatioCategory\")\n",
    "cleaned_df = bucketizer.setHandleInvalid(\"keep\").transform(cleaned_df)\n",
    "\n",
    "# Converter a coluna 'helpfulRatioCategory' de numérica para categórica\n",
    "cleaned_df = cleaned_df.withColumn(\"helpfulRatioCategory\", \n",
    "                   F.when(F.col(\"helpfulRatioCategory\") == 0, \"Low\")\\\n",
    "                    .when(F.col(\"helpfulRatioCategory\") == 1, \"Medium\")\\\n",
    "                    .otherwise(\"High\"))\n",
    "\n",
    "# Criar uma coluna que indica se a review contém uma pergunta\n",
    "cleaned_df = cleaned_df.withColumn('containsQuestion', (F.col('reviewText').like('%?%')).cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'id',\n",
       " 'class',\n",
       " 'helpfulTotalRatio',\n",
       " 'productRating',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime',\n",
       " 'reviewUpvotes',\n",
       " 'isWeekend',\n",
       " 'reviewLength',\n",
       " 'summaryLength',\n",
       " 'productPopularity',\n",
       " 'avgProductRating',\n",
       " 'helpfulRatioCategory',\n",
       " 'containsQuestion']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 141:==================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+---------+------------+-------------+-----------------+------------------+--------------------+----------------+\n",
      "|      asin|                  id|class|helpfulTotalRatio|productRating|          reviewText|         reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|reviewUpvotes|isWeekend|reviewLength|summaryLength|productPopularity|  avgProductRating|helpfulRatioCategory|containsQuestion|\n",
      "+----------+--------------------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+---------+------------+-------------+-----------------+------------------+--------------------+----------------+\n",
      "|0456787283|5a132293741a2384e...|  0.0|            100.0|          1.0|i bought these wi...|2013-03-16 00:00:00|A1KI7OVGTVEMPJ|         disneybound|     still too large|    1363392000|            1|        1|         265|           15|                1|               1.0|                High|               0|\n",
      "|0641674791|5a132293741a2384e...|  1.0|              0.0|          4.0|it is a great siz...|2012-11-26 00:00:00|A2H8WPJY8D4WDT|     Amazon Customer|great for travel ...|    1353888000|            0|        0|         105|           29|                1|               4.0|                 Low|               0|\n",
      "|1465014578|5a132293741a2384e...|  1.0|            100.0|          4.0|this a great size...|2013-01-26 00:00:00|A2OBG6WWO8566N|  D. Ambur \"karingo\"|           good size|    1359158400|            1|        1|          98|            9|                2|               4.5|                High|               0|\n",
      "|1465014578|5a132293741a2384e...|  1.0|            100.0|          5.0|perfect size and ...|2012-12-05 00:00:00|A3I3K19K2HHVLD|          Jacqueline|             awesome|    1354665600|            1|        0|         205|            7|                2|               4.5|                High|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|              0.0|          4.0|i ve tried other ...|2013-04-10 00:00:00| AMIGVTZZIWS6I|     Amazon Customer|great tool for le...|    1365552000|            0|        0|         336|           38|               62|3.7903225806451615|                 Low|               0|\n",
      "|1608299953|5a132293741a2384e...|  0.0|         78.78788|          1.0|i bought this sof...|2011-04-22 00:00:00|A2Z2PH0KX9MAWP| Anju Jolly \"Reader\"|rosetta stone issues|    1303430400|           26|        1|         679|           20|               62|3.7903225806451615|                High|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|              0.0|          5.0|i can t wait to u...|2010-08-26 00:00:00|A2MMDKNIM4ONRT|A. Rottger \"busy ...|really looking fo...|    1282780800|            0|        0|         483|           44|               62|3.7903225806451615|                 Low|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|              0.0|          5.0|since learning a ...|2013-04-09 00:00:00|A1F7YU6O5RU432|      Angela Streiff|suggest getting t...|    1365465600|            0|        0|        1818|           84|               62|3.7903225806451615|                 Low|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|             60.0|          4.0|being that i have...|2012-05-24 00:00:00|A27QXQQOLAMRRR|  Bone \"JLLaser-net\"|i wanted to learn...|    1337817600|            3|        0|        2602|           73|               62|3.7903225806451615|              Medium|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|        66.666664|          4.0|i was very excite...|2011-09-15 00:00:00| AIPJK5O8JYWCX|         Average Joe|good but dont exp...|    1316044800|            4|        0|        1430|           40|               62|3.7903225806451615|                High|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|        83.333336|          4.0|the first questio...|2010-09-21 00:00:00|A3NHUQ33CFH3VM|        Citizen John|intense way to learn|    1285027200|            5|        0|        1425|           20|               62|3.7903225806451615|                High|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|         98.04878|          4.0|the product itsel...|2011-12-04 00:00:00|A3INPLAFCMRI3I|              Cheryl|  4 stars but beware|    1322956800|          201|        0|        1971|           18|               62|3.7903225806451615|                High|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|             50.0|          5.0|so far this has b...|2011-01-20 00:00:00|A1IBKN1GXYJ3K4|                Dave|it really works i...|    1295481600|            1|        0|        1001|           40|               62|3.7903225806451615|              Medium|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|         84.61539|          5.0|okay  i admit it ...|2011-06-16 00:00:00|A253TILLU81VZK|         D. Connelly|helped me actuall...|    1308182400|           11|        0|         679|           55|               62|3.7903225806451615|                High|               0|\n",
      "|1608299953|5a132293741a2384e...|  0.0|        83.333336|          3.0|i m not sure what...|2012-09-06 00:00:00|A2NY8GRHTY3X7B|               DCott|     learning french|    1346889600|            5|        0|         614|           15|               62|3.7903225806451615|                High|               0|\n",
      "|1608299953|5a132293741a2384e...|  0.0|              0.0|          3.0|the french words ...|2014-02-03 00:00:00|A16NH7LQ483B63|          Don Larson|              french|    1391385600|            0|        0|         157|            6|               62|3.7903225806451615|                 Low|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|        83.333336|          4.0|i thought i d giv...|2011-08-21 00:00:00|A3M6LG4ENUNJ9U|      Daniel Findley|not perfect but e...|    1313884800|            5|        0|        2082|           25|               62|3.7903225806451615|                High|               0|\n",
      "|1608299953|5a132293741a2384e...|  1.0|              0.0|          5.0|rosetta stone is ...|2010-09-14 00:00:00|A1ZBEIL78MLR9Z|   Doffod \"Bookworm\"|an excellent way ...|    1284422400|            0|        0|         995|           25|               62|3.7903225806451615|                 Low|               0|\n",
      "|1608299953|5a132293741a2384e...|  0.0|         77.41936|          1.0|i bought this pro...|2012-09-21 00:00:00|A28QH3KX709FFQ| Don Quixote \"Don Q\"|do not buy if you...|    1348185600|           24|        1|         387|           40|               62|3.7903225806451615|                High|               0|\n",
      "|1608299953|5a132293741a2384e...|  0.0|        63.157894|          1.0|i have spent 4 ho...|2011-05-17 00:00:00|A2U6UH5W3VPKEL|   Dr. Dale C. Godby|             support|    1305590400|           12|        0|         236|            7|               62|3.7903225806451615|              Medium|               0|\n",
      "+----------+--------------------+-----+-----------------+-------------+--------------------+-------------------+--------------+--------------------+--------------------+--------------+-------------+---------+------------+-------------+-----------------+------------------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceamento do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:====================================>                  (16 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|class|  count|\n",
      "+-----+-------+\n",
      "|  0.0|1169449|\n",
      "|  1.0|4334882|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_df.groupBy('class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 191:=============================================>         (40 + 8) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|class|  count|\n",
      "+-----+-------+\n",
      "|  0.0|1169449|\n",
      "|  1.0|1170053|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calcular a taxa de balanceamento\n",
    "num_minority = cleaned_df.filter(cleaned_df['class'] == 0.0).count()\n",
    "num_majority = cleaned_df.filter(cleaned_df['class'] == 1.0).count()\n",
    "balance_ratio = num_minority / num_majority\n",
    "\n",
    "# Aplicar undersampling \n",
    "undersampled_df = cleaned_df.filter(cleaned_df['class'] == 0.0).union(\n",
    "    cleaned_df.filter(cleaned_df['class'] == 1.0).sample(False, balance_ratio)\n",
    ")\n",
    "\n",
    "undersampled_df.groupBy('class').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relações entre as variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construção do modelo com Algoritmos de Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo 1: Regressão Logística**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/04 14:30:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- class: double (nullable = true)\n",
      "\n",
      "+--------------------+-----+\n",
      "|          reviewText|class|\n",
      "+--------------------+-----+\n",
      "|the quality is go...|  1.0|\n",
      "|i thought it woul...|  0.0|\n",
      "|best present to g...|  1.0|\n",
      "|this is supposed ...|  0.0|\n",
      "|this hat arrived ...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, length, dayofweek, when\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LogisticRegressionExample\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.default.parallelism\", 200) \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.message.maxSize\", \"256\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# File paths\n",
    "json_file_path = \"Clothing_Shoes_and_Jewelry.json\"\n",
    "parquet_file_path = \"dataset/Clothing_Shoes_and_Jewelry.parquet\"\n",
    "\n",
    "# parquet_file_path_features = \"dataset/features.parquet\"\n",
    "parquet_file_path_features = None\n",
    "\n",
    "parquet_file_path_text = \"dataset/text.parquet\"\n",
    "# parquet_file_path_text = None\n",
    "\n",
    "# Check if the Parquet file exists\n",
    "if os.path.exists(parquet_file_path):\n",
    "    # Load the DataFrame from the Parquet file\n",
    "    if parquet_file_path_features is None and parquet_file_path_text is not None:\n",
    "        df = spark.read.parquet(parquet_file_path_text)\n",
    "    elif parquet_file_path_features is not None:\n",
    "        df = spark.read.parquet(parquet_file_path_features)\n",
    "    else:\n",
    "        df = spark.read.parquet(parquet_file_path)\n",
    "else:\n",
    "    # Load the DataFrame from the JSON file\n",
    "    df = spark.read.json(json_file_path)\n",
    "    \n",
    "    # Sample the DataFrame\n",
    "    df = df.sample(fraction=0.1, seed=12345)\n",
    "    \n",
    "    # Write the DataFrame to a Parquet file\n",
    "    df.write.parquet(parquet_file_path)\n",
    "    print(\"Data written to Parquet file.\")\n",
    "    \n",
    "\n",
    "# Continue with your data processing\n",
    "# Example: Show the DataFrame schema and first few rows\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, length, dayofweek, when\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Assuming a SparkSession is already created\n",
    "\n",
    "def helpful_ratio(helpful):\n",
    "    try:\n",
    "        return (helpful[0] / helpful[1]) * 100\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "ratio_udf = udf(helpful_ratio, FloatType())\n",
    "\n",
    "def feature_engineering(df, process=False):\n",
    "    \n",
    "    if process:\n",
    "        # Fase 1 Limpeza de Dados\n",
    "        df = df.withColumn(\"_id\", col(\"_id\").getField(\"$oid\"))\n",
    "        df = df.withColumn(\"_id\", regexp_replace(col(\"_id\"), \"[{}]\", \"\"))\n",
    "        df = df.drop('category')\n",
    "        df = df.withColumnRenamed(\"overall\", \"productRating\")\\\n",
    "            .withColumnRenamed(\"_id\", \"id\")\n",
    "        df = df.withColumn('reviewUpvotes', col('helpful')[0])\n",
    "        df = df.withColumn('helpful', ratio_udf(col('helpful')))\n",
    "        df = df.withColumnRenamed(\"helpful\", \"helpfulTotalRatio\")\n",
    "        df = df.withColumn('reviewLength', length(df['reviewText']))\n",
    "        df = df.withColumn('summaryLength', length(df['summary']))\n",
    "        df = df.withColumn('reviewTime', col('reviewTime').cast('timestamp'))\n",
    "        df = df.withColumn('isWeekend', when(dayofweek(col('reviewTime')) > 5, 1).otherwise(0))\n",
    "        \n",
    "        product_popularity_df = df.groupBy('asin').agg(F.count('*').alias('productPopularity'))\n",
    "        df = df.join(product_popularity_df, on='asin', how='left')\n",
    "        product_avg_rating_df = df.groupBy('asin').agg(F.avg('productRating').alias('avgProductRating'))\n",
    "        df = df.join(product_avg_rating_df, on='asin', how='left')\n",
    "        \n",
    "        bucketizer = Bucketizer(splits=[0, 33.3, 66.6, 100], inputCol=\"helpfulTotalRatio\", outputCol=\"helpfulRatioCategory\")\n",
    "        df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "        df = df.withColumn(\"helpfulRatioCategory\", \n",
    "                        F.when(F.col(\"helpfulRatioCategory\") == 0, \"Low\")\\\n",
    "                            .when(F.col(\"helpfulRatioCategory\") == 1, \"Medium\")\\\n",
    "                            .otherwise(\"High\"))\n",
    "        df = df.withColumn('containsQuestion', (F.col('reviewText').like('%?%')).cast('integer'))\n",
    "        df = df.withColumn('containsLink', (F.col('reviewText').like('%http%')).cast('integer'))\n",
    "\n",
    "        # Return all columns except 'reviewText' and 'summary'\n",
    "        cols_to_return = [col for col in df.columns if col not in ['reviewText', 'summary']]\n",
    "        df = df.select(*cols_to_return)\n",
    "        \n",
    "        df.write.parquet(\"dataset/features.parquet\", mode='overwrite')\n",
    "    else:\n",
    "        df = None\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n",
    "def text_processing(df, process=False):\n",
    "    \n",
    "    if process:\n",
    "        df = df.withColumn('reviewText', F.lower(df['reviewText']))\n",
    "        df = df.withColumn('reviewText', F.regexp_replace(df['reviewText'], '<[^>]+>', ''))\n",
    "        df = df.withColumn('reviewText', F.regexp_replace(df['reviewText'], 'http\\\\S+|www\\\\S+|https?://\\\\S+', ''))\n",
    "        df = df.withColumn('reviewText', F.regexp_replace(df['reviewText'], '@\\\\w+', ''))\n",
    "        df = df.withColumn('reviewText', F.regexp_replace(df['reviewText'], '#\\\\w+', ''))\n",
    "        df = df.withColumn('reviewText', F.regexp_replace(df['reviewText'], '&\\\\w+;', ''))\n",
    "        df = df.withColumn('reviewText', F.regexp_replace(df['reviewText'], '[^\\\\w\\\\s]', ' '))\n",
    "        df = df.withColumn('reviewText', F.regexp_replace(df['reviewText'], '\\\\d+', ''))\n",
    "        df = df.withColumn('reviewText', F.regexp_replace(df['reviewText'], '\\\\d', ' '))\n",
    "        df = df.withColumn('reviewText', F.regexp_replace(df['reviewText'], '\\\\s+', ' '))\n",
    "        df = df.withColumn('reviewText', F.trim(df['reviewText']))\n",
    "\n",
    "        df = df.withColumn('summary', F.lower(df['summary']))\n",
    "        df = df.withColumn('summary', F.regexp_replace(df['summary'], '<[^>]+>', ''))\n",
    "        df = df.withColumn('summary', F.regexp_replace(df['summary'], 'http\\\\S+|www\\\\S+|https?://\\\\S+', ''))\n",
    "        df = df.withColumn('summary', F.regexp_replace(df['summary'], '@\\\\w+', ''))\n",
    "        df = df.withColumn('summary', F.regexp_replace(df['summary'], '#\\\\w+', ''))\n",
    "        df = df.withColumn('summary', F.regexp_replace(df['summary'], '&\\\\w+;', ''))\n",
    "        df = df.withColumn('summary', F.regexp_replace(df['summary'], '[^\\\\w\\\\s]', ' '))\n",
    "        df = df.withColumn('summary', F.regexp_replace(df['summary'], '\\\\d+', ''))\n",
    "        df = df.withColumn('summary', F.regexp_replace(df['summary'], '\\\\d', ' '))\n",
    "        df = df.withColumn('summary', F.regexp_replace(df['summary'], '\\\\s+', ' '))\n",
    "        df = df.withColumn('summary', F.trim(df['summary']))\n",
    "\n",
    "        # Return only 'reviewText' and 'summary'\n",
    "        df = df.select('reviewText', 'class')\n",
    "        \n",
    "        df.write.parquet(\"dataset/text.parquet\", mode='overwrite')\n",
    "    else:\n",
    "        df = None\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df_features = feature_engineering(df, False)\n",
    "df_text = text_processing(df, False)\n",
    "\n",
    "# Save DataFrames to Parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|class| count|\n",
      "+-----+------+\n",
      "|  0.0|116697|\n",
      "|  1.0|433782|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum class count: 116697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          reviewText|class|\n",
      "+--------------------+-----+\n",
      "|i thought it woul...|  0.0|\n",
      "|this is supposed ...|  0.0|\n",
      "|i got this for my...|  0.0|\n",
      "|we got this to go...|  0.0|\n",
      "|beware that this ...|  0.0|\n",
      "|even for an adult...|  0.0|\n",
      "|is wonderful for ...|  0.0|\n",
      "|these suspenders ...|  0.0|\n",
      "|this is a super c...|  0.0|\n",
      "|i love the look o...|  0.0|\n",
      "|my th grade son u...|  0.0|\n",
      "|nice size and was...|  0.0|\n",
      "|bottoms fit a lit...|  0.0|\n",
      "|overall this was ...|  0.0|\n",
      "|unfortunately its...|  0.0|\n",
      "|i purchased this ...|  0.0|\n",
      "|the size itself i...|  0.0|\n",
      "|shipped wrong col...|  0.0|\n",
      "|these shoes were ...|  0.0|\n",
      "|the dress its per...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, class: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import to_timestamp, col, regexp_replace\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Assuming df is already defined and loaded with data\n",
    "# Checking for class imbalance\n",
    "class_distribution = df.groupBy(\"class\").count().orderBy(\"class\")\n",
    "class_distribution.show()\n",
    "\n",
    "# Undersample the majority class\n",
    "min_class_count = class_distribution.agg({\"count\": \"min\"}).collect()[0][0]\n",
    "print(\"Minimum class count:\", min_class_count)\n",
    "\n",
    "undersampled_df = df.groupBy(\"class\").applyInPandas(\n",
    "    lambda pdf: pdf.sample(n=min_class_count, random_state=42) if len(pdf) > min_class_count else pdf,\n",
    "    schema=df.schema\n",
    ")\n",
    "\n",
    "undersampled_df.show()\n",
    "\n",
    "# Preprocessing stages\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = undersampled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_df.persist(StorageLevel.DISK_ONLY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 14:06:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:41 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/06/04 14:06:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:59 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:06:59 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:43 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:43 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:43 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:07:55 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Accuracy = 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 14:07:59 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "24/06/04 14:08:02 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0:\n",
      "  Precision = 0.89\n",
      "  Recall = 0.89\n",
      "  F1 Score = 0.89\n",
      "Class 1.0:\n",
      "  Precision = 0.89\n",
      "  Recall = 0.89\n",
      "  F1 Score = 0.89\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "model = LogisticRegression(labelCol=\"class\")\n",
    "model_name = \"Logistic Regression\"\n",
    "\n",
    "print(f\"Training {model_name}...\")\n",
    "\n",
    "# Pipeline construction\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, model])\n",
    "\n",
    "# Train the model using the training data\n",
    "trained_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = trained_model.transform(test_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"class\").rdd.map(lambda row: (row[0], row[1]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "labels = predictions.select(\"class\").distinct().collect()\n",
    "labels = [row['class'] for row in labels]\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1Score = metrics.fMeasure(label)\n",
    "    print(f\"Class {label}:\")\n",
    "    print(f\"  Precision = {precision:.2f}\")\n",
    "    print(f\"  Recall = {recall:.2f}\")\n",
    "    print(f\"  F1 Score = {f1Score:.2f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "confusion_matrix = confusion_matrix.astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 14:30:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:30:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:30:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:30:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:30:58 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/06/04 14:31:50 WARN DAGScheduler: Broadcasting large task binary with size 1032.7 KiB\n",
      "24/06/04 14:31:51 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "24/06/04 14:31:54 WARN MemoryStore: Not enough space to cache rdd_86_5 in memory! (computed 951.0 MiB so far)\n",
      "24/06/04 14:31:54 WARN BlockManager: Persisting block rdd_86_5 to disk instead.\n",
      "24/06/04 14:31:55 WARN BlockManager: Block rdd_86_58 could not be removed as it was not found on disk or in memory\n",
      "24/06/04 14:31:55 ERROR Executor: Exception in task 58.0 in stage 37.0 (TID 2103)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4915/0x0000000801cf6840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/06/04 14:31:55 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 58.0 in stage 37.0 (TID 2103),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4915/0x0000000801cf6840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/06/04 14:31:55 WARN TaskSetManager: Lost task 58.0 in stage 37.0 (TID 2103) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4915/0x0000000801cf6840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/06/04 14:31:55 ERROR TaskSetManager: Task 58 in stage 37.0 failed 1 times; aborting job\n",
      "24/06/04 14:31:55 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 58 in stage 37.0 failed 1 times, most recent failure: Lost task 58.0 in stage 37.0 (TID 2103) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4915/0x0000000801cf6840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.DecisionTreeClassifier.$anonfun$train$1(DecisionTreeClassifier.scala:143)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:116)\n",
      "\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:48)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4915/0x0000000801cf6840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sx/jbvz3q8j557gm8d9cx0yqmzw0000gn/T/ipykernel_17770/2594825011.py\", line 11, in <module>\n",
      "    trained_model = pipeline.fit(train_df)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50363)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model using the training data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Make predictions on the testing data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Decision Tree Model\n",
    "model = DecisionTreeClassifier(labelCol=\"class\", maxDepth=5)\n",
    "model_name = \"Decision Tree\"\n",
    "\n",
    "print(f\"Training {model_name}...\")\n",
    "\n",
    "# Pipeline construction\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, model])\n",
    "\n",
    "# Train the model using the training data\n",
    "trained_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = trained_model.transform(test_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"class\").rdd.map(lambda row: (row[0], row[1]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "labels = predictions.select(\"class\").distinct().collect()\n",
    "labels = [row['class'] for row in labels]\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1Score = metrics.fMeasure(label)\n",
    "    print(f\"Class {label}:\")\n",
    "    print(f\"  Precision = {precision:.2f}\")\n",
    "    print(f\"  Recall = {recall:.2f}\")\n",
    "    print(f\"  F1 Score = {f1Score:.2f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "confusion_matrix = confusion_matrix.astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 14:26:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:26:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:26:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:26:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:26:49 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/06/04 14:28:06 WARN DAGScheduler: Broadcasting large task binary with size 1032.7 KiB\n",
      "24/06/04 14:28:07 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "24/06/04 14:28:11 WARN BlockManager: Block rdd_86_58 could not be removed as it was not found on disk or in memory\n",
      "24/06/04 14:28:11 ERROR Executor: Exception in task 58.0 in stage 37.0 (TID 2103)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4830/0x0000000801d18040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/06/04 14:28:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 58.0 in stage 37.0 (TID 2103),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4830/0x0000000801d18040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/06/04 14:28:11 WARN TaskSetManager: Lost task 58.0 in stage 37.0 (TID 2103) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4830/0x0000000801d18040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/06/04 14:28:11 ERROR TaskSetManager: Task 58 in stage 37.0 failed 1 times; aborting job\n",
      "24/06/04 14:28:11 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 58 in stage 37.0 failed 1 times, most recent failure: Lost task 58.0 in stage 37.0 (TID 2103) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4830/0x0000000801d18040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4830/0x0000000801d18040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sx/jbvz3q8j557gm8d9cx0yqmzw0000gn/T/ipykernel_16314/3026335176.py\", line 11, in <module>\n",
      "    trained_model = pipeline.fit(train_df)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50193)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model using the training data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Make predictions on the testing data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "model = RandomForestClassifier(labelCol=\"class\")\n",
    "model_name = \"Random Forest\"\n",
    "\n",
    "print(f\"Training {model_name}...\")\n",
    "\n",
    "# Pipeline construction\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, model])\n",
    "\n",
    "# Train the model using the training data\n",
    "trained_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = trained_model.transform(test_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"class\").rdd.map(lambda row: (row[0], row[1]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "labels = predictions.select(\"class\").distinct().collect()\n",
    "labels = [row['class'] for row in labels]\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1Score = metrics.fMeasure(label)\n",
    "    print(f\"Class {label}:\")\n",
    "    print(f\"  Precision = {precision:.2f}\")\n",
    "    print(f\"  Recall = {recall:.2f}\")\n",
    "    print(f\"  F1 Score = {f1Score:.2f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "confusion_matrix = confusion_matrix.astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient-Boosted Trees...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 14:16:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:16:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:16:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:16:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:16:53 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/06/04 14:17:56 WARN DAGScheduler: Broadcasting large task binary with size 1032.7 KiB\n",
      "24/06/04 14:17:57 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "24/06/04 14:18:03 WARN MemoryStore: Not enough space to cache rdd_77_5 in memory! (computed 951.0 MiB so far)\n",
      "24/06/04 14:18:03 WARN BlockManager: Persisting block rdd_77_5 to disk instead.\n",
      "24/06/04 14:18:04 WARN BlockManager: Block rdd_77_58 could not be removed as it was not found on disk or in memory\n",
      "24/06/04 14:18:04 ERROR Executor: Exception in task 58.0 in stage 32.0 (TID 1902)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4911/0x0000000801d22040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1469/0x0000000800b86840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1656/0x0000000800c78040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "24/06/04 14:18:04 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 58.0 in stage 32.0 (TID 1902),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4911/0x0000000801d22040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1469/0x0000000800b86840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1656/0x0000000800c78040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "24/06/04 14:18:04 WARN TaskSetManager: Lost task 58.0 in stage 32.0 (TID 1902) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4911/0x0000000801d22040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1469/0x0000000800b86840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1656/0x0000000800c78040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "24/06/04 14:18:04 ERROR TaskSetManager: Task 58 in stage 32.0 failed 1 times; aborting job\n",
      "24/06/04 14:18:04 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 58 in stage 32.0 failed 1 times, most recent failure: Lost task 58.0 in stage 32.0 (TID 1902) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4911/0x0000000801d22040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1469/0x0000000800b86840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1656/0x0000000800c78040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:367)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:201)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4911/0x0000000801d22040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1469/0x0000000800b86840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1656/0x0000000800c78040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sx/jbvz3q8j557gm8d9cx0yqmzw0000gn/T/ipykernel_11292/163387582.py\", line 11, in <module>\n",
      "    trained_model = pipeline.fit(train_df)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 49951)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model using the training data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Make predictions on the testing data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Gradient-Boosted Trees Model\n",
    "model = GBTClassifier(labelCol=\"class\")\n",
    "model_name = \"Gradient-Boosted Trees\"\n",
    "\n",
    "print(f\"Training {model_name}...\")\n",
    "\n",
    "# Pipeline construction\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, model])\n",
    "\n",
    "# Train the model using the training data\n",
    "trained_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = trained_model.transform(test_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"class\").rdd.map(lambda row: (row[0], row[1]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "labels = predictions.select(\"class\").distinct().collect()\n",
    "labels = [row['class'] for row in labels]\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1Score = metrics.fMeasure(label)\n",
    "    print(f\"Class {label}:\")\n",
    "    print(f\"  Precision = {precision:.2f}\")\n",
    "    print(f\"  Recall = {recall:.2f}\")\n",
    "    print(f\"  F1 Score = {f1Score:.2f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "confusion_matrix = confusion_matrix.astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 14:11:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:45 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/06/04 14:11:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:59 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:59 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:11:59 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:43 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:43 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:59 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:12:59 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:43 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:43 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:43 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:43 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:13:54 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Test Accuracy = 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 14:13:58 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "24/06/04 14:14:00 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0:\n",
      "  Precision = 0.90\n",
      "  Recall = 0.90\n",
      "  F1 Score = 0.90\n",
      "Class 1.0:\n",
      "  Precision = 0.90\n",
      "  Recall = 0.90\n",
      "  F1 Score = 0.90\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyMAAAJuCAYAAACjYt3DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABgl0lEQVR4nO3df3xPdf/H8efHfvqxfWxmvwr5HYaGYlSI5rekK6SWoVFCGtW1SuiHlYguIpef0Yq6RIkWJUp+0wpJCuGyMczYzMx2vn/4+lyfTxs25hw/Hvfrdm5fn3Nen/fnfT6Xr2uvPc/7HJthGIYAAAAAwGQlrJ4AAAAAgJsTzQgAAAAAS9CMAAAAALAEzQgAAAAAS9CMAAAAALAEzQgAAAAAS9CMAAAAALAEzQgAAAAAS9CMAAAAALAEzQiAK7Z+/Xo9+OCDqlixory8vBQUFKSIiAgNHTpUkpSamipPT0/16NHjgmOcOHFCpUqVUufOnSVJs2fPls1mk81m08qVK/PVG4ahatWqyWazqUWLFoWaZ15enubOnavWrVsrICBAHh4eCgwMVMeOHbV48WLl5eUV+dyLYuLEiapWrZo8PT1ls9l0/PjxYh3//He2d+/eYh23MFq0aCGbzaYqVarIMIx8x7///nvHf5+zZ88u8vgHDx7UyJEjlZSUVKT3RUdH67bbbivy5wEAzEEzAuCKLFmyRE2bNtWJEyc0ZswYLVu2TO+++66aNWum+fPnS5LKly+vzp07a9GiRUpLSytwnHnz5ikrK0t9+/Z12e/j46MZM2bkq1+1apX+/PNP+fj4FGqep0+fVvv27dWrVy8FBgZqypQpWrFihd5//32Fhobq4Ycf1uLFi4t49oWXlJSkwYMHq2XLllqxYoXWrl1b6LkXVocOHbR27VqFhIQU67iF5ePjoz179mjFihX5js2cOVO+vr6XPfbBgwc1atSoIjcjw4cP18KFCy/7cwEAV5e71RMAcH0bM2aMKleurK+//lru7v/7J6VHjx4aM2aM43Xfvn21YMECJSQkaODAgfnGmTlzpoKCgtShQweX/d27d1dCQoLee+89lx9mZ8yYoYiICJ04caJQ84yNjdXXX3+tDz74QI8//rjLsa5du+q5555TVlZWoca6HNu3b5ckxcTE6K677roqn1G+fHmVL1/+qoxdGBUrVpSPj49mzpypVq1aOfafPHlSn376qR599FFNmzbNlLmcOnVKpUqVUtWqVU35PADA5SEZAXBFjh49qoCAAJdG5LwSJf73T0ybNm106623atasWfnqduzYofXr1+vxxx/PN84jjzwiSfr4448d+9LT07VgwQL16dOnUHNMSUnR9OnT1aZNm3yNyHnVq1dXvXr1HK/37dunxx57TIGBgfLy8lKtWrU0btw4l0u59u7dK5vNprFjx+qdd95R5cqVVaZMGUVERGjdunWOuhYtWuixxx6TJDVu3Fg2m03R0dGSpNtuu83xZ2ctWrRwufwsLy9Pr7/+umrWrKmSJUuqbNmyqlevnt59911HzYUu05o5c6bq168vb29v+fv768EHH9SOHTtcaqKjo1WmTBn98ccfat++vcqUKaMKFSpo6NChys7Ovuj366xPnz767LPPXC5BmzdvniQVeJneH3/8od69e6t69eoqVaqUbrnlFnXq1Elbt2511KxcuVJ33nmnJKl3796Oy71GjhzpMvetW7cqMjJSPj4+jmbo75dpzZs3TzabTZMmTXKZx4gRI+Tm5qbly5cX+lwBAFeOZgTAFYmIiND69es1ePBgrV+/Xjk5OQXWlShRQtHR0dqyZYt+/vlnl2PnG5SCmgtfX1/94x//0MyZMx37Pv74Y5UoUULdu3cv1By/++475eTkqEuXLoWqT01NVdOmTbVs2TK99tpr+uKLL9S6dWsNGzaswFTnvffe0/LlyzVhwgQlJCQoMzNT7du3V3p6uiRp8uTJevnllx3nunbtWg0fPrxQczlvzJgxGjlypB555BEtWbJE8+fPV9++fS+57iQ+Pl59+/ZVnTp19Nlnn+ndd9/VL7/8ooiICO3atculNicnR507d1arVq30+eefq0+fPho/frzeeuutQs+zR48ecnNzc2keZ8yYoX/84x8FXqZ18OBBlStXTm+++aYSExP13nvvyd3dXY0bN9bOnTslSQ0aNHD8HXn55Ze1du1arV27Vk888YRjnDNnzqhz586677779Pnnn2vUqFEXnN+TTz6poUOHatOmTZKkFStW6PXXX9eLL76o+++/v9DnCgAoBgYAXIEjR44Yd999tyHJkGR4eHgYTZs2NeLj442TJ0+61O7evduw2WzG4MGDHftycnKM4OBgo1mzZi61s2bNMiQZGzduNL777jtDkrFt2zbDMAzjzjvvNKKjow3DMIw6deoYzZs3v+gc33zzTUOSkZiYWKhz+uc//2lIMtavX++y/6mnnjJsNpuxc+dOwzAMY8+ePYYko27dusbZs2cddRs2bDAkGR9//HGB5+OsUqVKRq9evfLNoXnz5i7n1bFjR+OOO+646LzPf8aePXsMwzCMtLQ0o2TJkkb79u1d6vbt22d4eXkZPXv2dOzr1auXIcn45JNPXGrbt29v1KxZ86Kfe36+derUcYzVqFEjwzAMY/v27YYkY+XKlcbGjRsNScasWbMuOM7Zs2eNM2fOGNWrVzeeffZZx/6Lvff83GfOnFngsUqVKrnsO336tBEeHm5UrlzZ+PXXX42goCCjefPmLv8dAgDMQTIC4IqUK1dOP/zwgzZu3Kg333xTDzzwgH7//XfFxcWpbt26OnLkiKO2cuXKatmypRISEnTmzBlJ0ldffaWUlJSLXnLVvHlzVa1aVTNnztTWrVu1cePGQl+idTlWrFih2rVr51vbER0dLcMw8i3Q7tChg9zc3Byvz1/u9ddffxXbnO666y79/PPPGjBggL7++utCrZVZu3atsrKy8l0GVqFCBd1333369ttvXfbbbDZ16tTJZV+9evWKfB59+vTRpk2btHXrVs2YMUNVq1bVvffeW2Dt2bNnNXr0aNWuXVuenp5yd3eXp6endu3ale9Sskt56KGHClXn5eWlTz75REePHlWDBg1kGIY+/vhjl/8OAQDmoBkBUCwaNWqkF154QZ9++qkOHjyoZ599Vnv37nVZxC6dW8h+9OhRffHFF5LOXbZUpkwZdevW7YJj22w29e7dWx9++KHef/991ahRQ/fcc0+h51axYkVJ0p49ewpVf/To0QLvSBUaGuo47qxcuXIur728vCSpWBfEx8XFaezYsVq3bp3atWuncuXKqVWrVo5LjQpyfp4XOpe/n0epUqXk7e3tss/Ly0unT58u0lzvvfdeVa9eXVOnTtXcuXPVp08f2Wy2AmtjY2M1fPhwdenSRYsXL9b69eu1ceNG1a9fv0jfX6lSpYp0t65q1arpnnvu0enTp/Xoo49adgcyALjZ0YwAKHYeHh4aMWKEJGnbtm0ux7p27So/Pz/NnDlTqamp+vLLL9W9e3eVKVPmomNGR0fryJEjev/999W7d+8izadly5by8PDQokWLClVfrlw5JScn59t/8OBBSVJAQECRPv9ivL29C1wg7pwoSZK7u7tiY2O1ZcsWHTt2TB9//LH279+vNm3a6NSpUwWOfb5JutC5FOd5/F3v3r01ZcoUHTt2TL169bpg3YcffqjHH39co0ePVps2bXTXXXepUaNG+c7/Ui7U7FzI9OnTtWTJEt11112aNGmS1q9fX6T3AwCKB80IgCtS0A+6khyX2JxPE87z9vZWz549tWzZMr311lvKyckp1CVXt9xyi5577jl16tTpoj/cFiQ4OFhPPPGEvv76a82ZM6fAmj///FO//PKLJKlVq1b69ddftWXLFpeaOXPmyGazqWXLlkX6/Iu57bbbHJ973u+//+5YvF2QsmXL6h//+IeefvppHTt27IIPOYyIiFDJkiX14Ycfuuw/cOCAVqxY4XL73eLWq1cvderUSc8995xuueWWC9bZbDZHknTekiVL9N///tdlX3GmTVu3btXgwYP1+OOP64cfflC9evXUvXv3Cz4DBwBw9fCcEQBX5Pwtezt16qTbb79deXl5SkpK0rhx41SmTBk988wz+d7Tt29fvffee3rnnXd0++23q2nTpoX6rDfffPOy5/nOO+9o9+7dio6O1tdff60HH3xQQUFBOnLkiJYvX65Zs2Zp3rx5qlevnp599lnNmTNHHTp00KuvvqpKlSppyZIlmjx5sp566inVqFHjsufxd1FRUXrsscc0YMAAPfTQQ/rrr780ZsyYfM8L6dSpk8LCwtSoUSOVL19ef/31lyZMmKBKlSqpevXqBY5dtmxZDR8+XC+++KIef/xxPfLIIzp69KhGjRolb29vR3p1NYSGhhYqierYsaNmz56t22+/XfXq1dPmzZv19ttv69Zbb3Wpq1q1qkqWLKmEhATVqlVLZcqUUWhoaL5m91IyMzPVrVs3Va5cWZMnT5anp6c++eQTNWjQQL179y50egYAKB40IwCuyMsvv6zPP/9c48ePV3JysrKzsxUSEqLWrVsrLi5OtWrVyvee8PBwhYeH66effrqqC9GdeXt7a8mSJUpISNAHH3yg/v3768SJE/Lz81OjRo00c+ZMx+Lt8uXLa82aNYqLi1NcXJxOnDihKlWqaMyYMYqNjS3WefXs2VMHDx7U+++/r1mzZiksLExTpkzJd2vali1basGCBZo+fbpOnDih4OBg3X///Ro+fLg8PDwuOH5cXJwCAwP1r3/9S/Pnz1fJkiXVokULjR49+oJNjJneffddeXh4KD4+XhkZGWrQoIE+++wzx62QzytVqpRmzpypUaNGKTIyUjk5ORoxYoTjWSOF9eSTT2rfvn3auHGjSpcuLUmqUqWKpk+frocfflgTJkzQkCFDiunsAACXYjMMw7B6EgAAAABuPqwZAQAAAGAJmhEAAAAAlqAZAQAAAGAJmhEAAAAAlqAZAQAAAGAJmhEAAAAAlqAZAQAAAGCJG/KhhyXDB1o9BQAoVmkbJ1k9BQAoVt7X8E+hZv4smfXTzf3vO8kIAAAAAEtcwz0pAAAAYAEbv683C980AAAAAEuQjAAAAADObDarZ3DTIBkBAAAAYAmSEQAAAMAZa0ZMwzcNAAAAwBIkIwAAAIAz1oyYhmQEAAAAgCVIRgAAAABnrBkxDd80AAAAAEuQjAAAAADOWDNiGpIRAAAAAJYgGQEAAACcsWbENHzTAAAAACxBMwIAAADAElymBQAAADhjAbtpSEYAAAAAWIJkBAAAAHDGAnbT8E0DAAAAsATJCAAAAOCMNSOmIRkBAAAAYAmSEQAAAMAZa0ZMwzcNAAAAwBIkIwAAAIAz1oyYhmQEAAAAgCVIRgAAAABnrBkxDd80AAAAAEvQjAAAAADObCXM24ogPj5ed955p3x8fBQYGKguXbpo586dLjWGYWjkyJEKDQ1VyZIl1aJFC23fvt2lJjs7W4MGDVJAQIBKly6tzp0768CBAy41aWlpioqKkt1ul91uV1RUlI4fP+5Ss2/fPnXq1EmlS5dWQECABg8erDNnzhTpnGhGAAAAgOvAqlWr9PTTT2vdunVavny5zp49q8jISGVmZjpqxowZo3feeUeTJk3Sxo0bFRwcrPvvv18nT5501AwZMkQLFy7UvHnztHr1amVkZKhjx47Kzc111PTs2VNJSUlKTExUYmKikpKSFBUV5Tiem5urDh06KDMzU6tXr9a8efO0YMECDR06tEjnZDMMw7iC7+SaVDJ8oNVTAIBilbZxktVTAIBi5X0Nr1wu2fI10z4r67vhl/3e1NRUBQYGatWqVbr33ntlGIZCQ0M1ZMgQvfDCC5LOpSBBQUF666231L9/f6Wnp6t8+fKaO3euunfvLkk6ePCgKlSooKVLl6pNmzbasWOHateurXXr1qlx48aSpHXr1ikiIkK//fabatasqa+++kodO3bU/v37FRoaKkmaN2+eoqOjdfjwYfn6+hbqHEhGAAAAAItkZ2frxIkTLlt2dnah3pueni5J8vf3lyTt2bNHKSkpioyMdNR4eXmpefPmWrNmjSRp8+bNysnJcakJDQ1VWFiYo2bt2rWy2+2ORkSSmjRpIrvd7lITFhbmaEQkqU2bNsrOztbmzZsLff40IwAAAIAzE9eMxMfHO9ZlnN/i4+MvOUXDMBQbG6u7775bYWFhkqSUlBRJUlBQkEttUFCQ41hKSoo8PT3l5+d30ZrAwMB8nxkYGOhS8/fP8fPzk6enp6OmMK7hgAwAAAC4scXFxSk2NtZln5eX1yXfN3DgQP3yyy9avXp1vmO2vz200TCMfPv+7u81BdVfTs2lkIwAAAAAFvHy8pKvr6/LdqlmZNCgQfriiy/03Xff6dZbb3XsDw4OlqR8ycThw4cdKUZwcLDOnDmjtLS0i9YcOnQo3+empqa61Pz9c9LS0pSTk5MvMbkYmhEAAADAmc1m3lYEhmFo4MCB+uyzz7RixQpVrlzZ5XjlypUVHBys5cuXO/adOXNGq1atUtOmTSVJDRs2lIeHh0tNcnKytm3b5qiJiIhQenq6NmzY4KhZv3690tPTXWq2bdum5ORkR82yZcvk5eWlhg0bFvqcuEwLAAAAuA48/fTT+uijj/T555/Lx8fHkUzY7XaVLFlSNptNQ4YM0ejRo1W9enVVr15do0ePVqlSpdSzZ09Hbd++fTV06FCVK1dO/v7+GjZsmOrWravWrVtLkmrVqqW2bdsqJiZGU6dOlST169dPHTt2VM2aNSVJkZGRql27tqKiovT222/r2LFjGjZsmGJiYgp9Jy2JZgQAAABwVcSHEZplypQpkqQWLVq47J81a5aio6MlSc8//7yysrI0YMAApaWlqXHjxlq2bJl8fHwc9ePHj5e7u7u6deumrKwstWrVSrNnz5abm5ujJiEhQYMHD3bcdatz586aNOl/t5l3c3PTkiVLNGDAADVr1kwlS5ZUz549NXbs2CKdE88ZAYDrAM8ZAXCjuaafM9L6TdM+K+ubf5r2Wdeia/ivAQAAAGCBIq7lwOW7NjMoAAAAADc8khEAAADA2TW6ZuRGxDcNAAAAwBIkIwAAAIAz1oyYhmQEAAAAgCVIRgAAAABnrBkxDd80AAAAAEuQjAAAAADOWDNiGpIRAAAAAJYgGQEAAACcsWbENHzTAAAAACxBMgIAAAA4Y82IaUhGAAAAAFiCZAQAAABwxpoR0/BNAwAAALAEzQgAAAAAS3CZFgAAAOCMy7RMwzcNAAAAwBIkIwAAAIAzbu1rGpIRAAAAAJYgGQEAAACcsWbENHzTAAAAACxBMgIAAAA4Y82IaUhGAAAAAFiCZAQAAABwxpoR0/BNAwAAALAEyQgAAADgjDUjpiEZAQAAAGAJkhEAAADAiY1kxDQkIwAAAAAsQTICAAAAOCEZMQ/JCAAAAABLkIwAAAAAzghGTEMyAgAAAMASNCMAAAAALMFlWgAAAIATFrCbh2QEAAAAgCVIRgAAAAAnJCPmIRkBAAAAYAmSEQAAAMAJyYh5SEYAAAAAWIJkBAAAAHBCMmIekhEAAAAAliAZAQAAAJwRjJiGZAQAAACAJUhGAAAAACesGTEPyQgAAAAAS5CMAAAAAE5IRsxDMgIAAADAEiQjAAAAgBOSEfOQjAAAAACwBMkIAAAA4IRkxDwkIwAAAAAsQTICAAAAOCMYMQ3JCAAAAABL0IwAAAAAsASXaQEAAABOWMBuHpIRAAAAAJYgGQEAAACckIyYh2QEAAAAgCVoRgAAAAAnNpvNtK0ovv/+e3Xq1EmhoaGy2WxatGhRoeb99ttvO2patGiR73iPHj1cxklLS1NUVJTsdrvsdruioqJ0/Phxl5p9+/apU6dOKl26tAICAjR48GCdOXOmSOcj0YwAAAAA14XMzEzVr19fkyZNKvB4cnKyyzZz5kzZbDY99NBDLnUxMTEudVOnTnU53rNnTyUlJSkxMVGJiYlKSkpSVFSU43hubq46dOigzMxMrV69WvPmzdOCBQs0dOjQIp8Ta0YAAAAAZ9fokpF27dqpXbt2FzweHBzs8vrzzz9Xy5YtVaVKFZf9pUqVyld73o4dO5SYmKh169apcePGkqRp06YpIiJCO3fuVM2aNbVs2TL9+uuv2r9/v0JDQyVJ48aNU3R0tN544w35+voW+pxIRgAAAACLZGdn68SJEy5bdnb2FY976NAhLVmyRH379s13LCEhQQEBAapTp46GDRumkydPOo6tXbtWdrvd0YhIUpMmTWS327VmzRpHTVhYmKMRkaQ2bdooOztbmzdvLtI8aUYAAAAAJ2auGYmPj3eszTi/xcfHX/E5fPDBB/Lx8VHXrl1d9j/66KP6+OOPtXLlSg0fPlwLFixwqUlJSVFgYGC+8QIDA5WSkuKoCQoKcjnu5+cnT09PR01hcZkWAAAAYJG4uDjFxsa67PPy8rricWfOnKlHH31U3t7eLvtjYmIcfw4LC1P16tXVqFEjbdmyRQ0aNJBU8K2NDcNw2V+YmsKgGQEAAACcmPmcES8vr2JpPpz98MMP2rlzp+bPn3/J2gYNGsjDw0O7du1SgwYNFBwcrEOHDuWrS01NdaQhwcHBWr9+vcvxtLQ05eTk5EtMLoXLtAAAAIAbyIwZM9SwYUPVr1//krXbt29XTk6OQkJCJEkRERFKT0/Xhg0bHDXr169Xenq6mjZt6qjZtm2bkpOTHTXLli2Tl5eXGjZsWKS5kowAAAAATq7VJ7BnZGTojz/+cLzes2ePkpKS5O/vr4oVK0qSTpw4oU8//VTjxo3L9/4///xTCQkJat++vQICAvTrr79q6NChCg8PV7NmzSRJtWrVUtu2bRUTE+O45W+/fv3UsWNH1axZU5IUGRmp2rVrKyoqSm+//baOHTumYcOGKSYmpkh30pJIRgAAAIDrwqZNmxQeHq7w8HBJUmxsrMLDw/XKK684aubNmyfDMPTII4/ke7+np6e+/fZbtWnTRjVr1tTgwYMVGRmpb775Rm5ubo66hIQE1a1bV5GRkYqMjFS9evU0d+5cx3E3NzctWbJE3t7eatasmbp166YuXbpo7NixRT4nm2EYRpHfdY0rGT7Q6ikAQLFK21jwA64A4HrlfQ1fnxPa/zPTPuvg1K6XLrqBkYwAAAAAsMQ13JMCAAAAFrg2l4zckEhGAAAAAFiCZgQAAACAJbhMCwAAAHByrd7a90ZEMgIAAADAEiQjAAAAgBOSEfOQjAAAAACwBMkIAAAA4IRkxDwkIwAAAAAsQTICAAAAOCMYMQ3JCAAAAABLkIwAAAAATlgzYh6SEQAAAACWIBkBAAAAnJCMmIdkBAAAAIAlSEYAAAAAJyQj5qEZwU1jWJ9IdbmvvmrcFqSs7Byt/3m3Xnr3c+3667Cj5oH76qvvQ3crvFYFBfiVUePu8frl9/+6jNOnazN1b9dId9x+q3zLlFTwPc8pPSMr3+e1vbuOXuzXTmHVQ5WZdUY/bvlDPYZNdxxvWLuiXhv8gMJrV5BhSJu3/6WXJizK93kAUBQzpk3Vt8uXac+e3fLy9tYdd4RrSOww3Va5iqNmynsTlfjVEqWkpMjDw0O1a9fRwGeeVb169SVJ6cePa/J7E7V2zWodSklR2bJ+atmqtZ4e9Ix8fHwc45xIT9eb8a9r1XcrJEnNW96nf744XL6+vuaeNIDrFpdp4aZxT4Nqen/+92r++Fh1fGqS3Nzc9OWUgSrl7emoKVXSU2t//lPDJ35+wXFKeXto+Zpf9fbMZRes6dLqDs14/XHN+WKd7ur+pu7r/Y7mJ25yHC9TyktfTH5a+1PSdG/UWLXq/Y5OZp7WF5Oflrs7/28J4PJt2rhB3R95VHM//kRTp83S2dxcPRnTV6dOnXLUVKp0m+JeekULFi7W7LkfKfSWW/RUTB8dO3ZMknQ49bBSDx9W7LAX9J+Fi/XqG/H6cfUPGjn8JZfP+ufzQ7Xzt980eep0TZ46XTt/+00v/fN5U88XuBpsNptp283OZhiGYfUkilvJ8IFWTwHXgQC/Mtq/4k217jteP2750+VYxRB/7Vz6aoHJyHn3NKyuZdOfyZeMuLmV0M4lo/Ta+0v1waK1Bb63Qe2K+jHheVVv+7IOHDouSapTLVSbPn1RtTuN1J4DR4rnJHHDSNs4yeop4Dp17NgxtbwnQjM/+FANG91ZYE1GRoaaNW6of8+YrcZNIgqsWfb1V3rxhee0blOS3N3dtfvPP/Vg5/aa+/EnjkTll5+TFNWzuz7/8iuXJAYoiPc1fH1O5SFLTPusPRM6mPZZ1yJ+BYublm8Zb0lSWvqpS1QWTfjtFXRLkJ/y8gyt/fgF7V72hhZNekq1qgQ7an7fe0ipaSfVq0tTebi7ydvLQ9FdIrT9j4Pal3ysWOcD4OaWcfKkJMnXbi/weM6ZM1rw6Xz5+PioRs2aFxknQ2XKlJG7+7mfIH/++Sf5+Pg4GhFJqlf/Dvn4+Cgp6adiPAPAAjYTt5ucpT3pgQMHNGXKFK1Zs0YpKSmy2WwKCgpS06ZN9eSTT6pChQqXHCM7O1vZ2dku+4y8XNlKuF2taeMG8dbQh/Tjlj/065/JxTpu5VsDJEkvP9leL4z7TH8dPKpnolpp2fQhqtflVaWdOKWMU9lq88S7+nR8f8XFtJUk7frrsDo//Z5yc/OKdT4Abl6GYWjsmHiFN2io6tVruBxbtfI7vTAsVqdPZymgfHm9P22m/Pz8Cxzn+PE0/fv9yfrHw90d+44eOSI//3L5av38y+noEdJdAIVjWTKyevVq1apVSwsXLlT9+vX1+OOP67HHHlP9+vW1aNEi1alTRz/++OMlx4mPj5fdbnfZzh7abMIZ4Ho2/p/dVLd6qHrFzS72sUv8//Wfb03/Wou+TdJPO/ar34gPZchQ1/vDJUneXh6aOvIxrf15t5o/Plb39X5HO3Yna+HEp+Tt5VHscwJwc4p//VXt+v13vfX2O/mO3XlXY32yYJHmJMxTs7vv0XNDh+jo0aP56jIyMjTwqf6qUrWq+g9wvQy6wMvdDUP8uhfXO9aMmMeyZOTZZ5/VE088ofHjx1/w+JAhQ7Rx48aLjhMXF6fY2FiXfYH3vFBs88SN550XHlbH5nXVuu8E/ffw8WIfP/lIuiTpt93/S1zO5JzV3gNHVSH43G8du7drpIqh/mrea5zOL9vqFTdbyd+PUacW9fTp1zTUAK5M/BuvaeXKFZr5wYcKCg7Od7xUqVKqWKmSKlaqpHr171CndpFa9Nl/1Demv6MmMzNDA/o/oVKlSmn8v96Th8f/fllSLiBAxwpoXtLSjqlcQP7EBAAKYlkysm3bNj355JMXPN6/f39t27btkuN4eXnJ19fXZeMSLVzI+Bce1gP31Vfb/v/SXwfz/49ocfhpx36dzs5R9duCHPvc3UuoYqi/Yz1IKW9P5eUZcr5/RJ5hyDD+l6wAwOUwDEOjX39V336zTNNmfqBbb730Jc/n33fmzBnH64yMDD0Z01ceHh56d9IUeXl5udTXrx+ukydPausvvzj2/fLLzzp58qTuuCO8eE4GwA3PsmQkJCREa9asUc0LLJZbu3atQkJCTJ4VbmQT4rqpe7tGevjZfysj87SCyp27V356xmmdzs6RJPn5llKFYD+FBJ5b6Fnj/xuKQ0dP6NDRc4tAg8r5KKicr6pWPLc2JKx6qE5mntb+lDSlnTilk5mnNf0/qzX8yfY6kJKmfcnH9Gyv1pKkz5ZvkSR9u+43jR7SRRPiumnKvFUqYbNpWO9Inc3N1apNv5v3pQC44Yx+bZS+WvqlJkycrNKlSutIaqokqYyPj7y9vXXq1ClN//f7atHyPgWUL6/048c1f95HOnQoRfe3ObeGLTMzQ0/G9NHp01ka/ebbyszIUGZGhiTJz99fbm5uqlK1qprdfY9eHfGyho98VZL06sjhurd5S+6khesel0+Zx7Jb+06ePFnPPvusYmJidP/99ysoKEg2m00pKSlavny5pk+frgkTJlw0PbkQbu2LgmT9VPCtUWNemasPF6+XJD3WqbGmvRqVr+b195fqjalLJUkv9W+vl59sf9Fx3N1L6LVBD+iRDneqpJeHNm77S8+9/R/t2J3iqL+v8e16qX871a4Worw8Qz//dkAj31usDVv3Xump4gbErX1RWPXrFPxLvldfj9cDD3ZVdna2/vn8UG395WcdT0tT2bJlVSesrmL6P6WwuvUkSRs3rNcTvR8vcJyly77VLbfcKuncwxH//tDDuJde4aGHKJRr+da+VYd+Zdpn/TmunWmfdS2y9Dkj8+fP1/jx47V582bl5uZKktzc3NSwYUPFxsaqW7dulzUuzQiAGw3NCIAbzbXcjFQbZl4z8sfYm7sZsfSvQffu3dW9e3fl5OToyP/fBjAgIMBlgRwAAACAG9M10ZN6eHiwPgQAAADXBNaMmIcnsAMAAACwxDWRjAAAAADXCoIR85CMAAAAALAEyQgAAADghDUj5iEZAQAAAGAJkhEAAADACcGIeUhGAAAAAFiCZAQAAABwUqIE0YhZSEYAAAAAWIJkBAAAAHDCmhHzkIwAAAAAsATJCAAAAOCE54yYh2QEAAAAgCVoRgAAAABYgsu0AAAAACdcpWUekhEAAAAAliAZAQAAAJywgN08JCMAAAAALEEyAgAAADghGTEPyQgAAAAAS5CMAAAAAE4IRsxDMgIAAADAEiQjAAAAgBPWjJiHZAQAAACAJUhGAAAAACcEI+YhGQEAAABgCZIRAAAAwAlrRsxDMgIAAADAEiQjAAAAgBOCEfOQjAAAAACwBM0IAAAA4MRms5m2FcX333+vTp06KTQ0VDabTYsWLXI5Hh0dnW/8Jk2auNRkZ2dr0KBBCggIUOnSpdW5c2cdOHDApSYtLU1RUVGy2+2y2+2KiorS8ePHXWr27dunTp06qXTp0goICNDgwYN15syZIp2PRDMCAAAAXBcyMzNVv359TZo06YI1bdu2VXJysmNbunSpy/EhQ4Zo4cKFmjdvnlavXq2MjAx17NhRubm5jpqePXsqKSlJiYmJSkxMVFJSkqKiohzHc3Nz1aFDB2VmZmr16tWaN2+eFixYoKFDhxb5nFgzAgAAADi5VteMtGvXTu3atbtojZeXl4KDgws8lp6erhkzZmju3Llq3bq1JOnDDz9UhQoV9M0336hNmzbasWOHEhMTtW7dOjVu3FiSNG3aNEVERGjnzp2qWbOmli1bpl9//VX79+9XaGioJGncuHGKjo7WG2+8IV9f30KfE8kIAAAAYJHs7GydOHHCZcvOzr7s8VauXKnAwEDVqFFDMTExOnz4sOPY5s2blZOTo8jISMe+0NBQhYWFac2aNZKktWvXym63OxoRSWrSpInsdrtLTVhYmKMRkaQ2bdooOztbmzdvLtJ8aUYAAAAAi8THxzvWZpzf4uPjL2usdu3aKSEhQStWrNC4ceO0ceNG3XfffY7mJiUlRZ6envLz83N5X1BQkFJSUhw1gYGB+cYODAx0qQkKCnI57ufnJ09PT0dNYXGZFgAAAODEzIcexsXFKTY21mWfl5fXZY3VvXt3x5/DwsLUqFEjVapUSUuWLFHXrl0v+D7DMFzOuaDzv5yawiAZAQAAACzi5eUlX19fl+1ym5G/CwkJUaVKlbRr1y5JUnBwsM6cOaO0tDSXusOHDzuSjuDgYB06dCjfWKmpqS41f09A0tLSlJOTky8xuRSaEQAAAMCJzWbedjUdPXpU+/fvV0hIiCSpYcOG8vDw0PLlyx01ycnJ2rZtm5o2bSpJioiIUHp6ujZs2OCoWb9+vdLT011qtm3bpuTkZEfNsmXL5OXlpYYNGxZpjlymBQAAAFwHMjIy9Mcffzhe79mzR0lJSfL395e/v79Gjhyphx56SCEhIdq7d69efPFFBQQE6MEHH5Qk2e129e3bV0OHDlW5cuXk7++vYcOGqW7duo67a9WqVUtt27ZVTEyMpk6dKknq16+fOnbsqJo1a0qSIiMjVbt2bUVFRentt9/WsWPHNGzYMMXExBTpTloSzQgAAADgwsw1I0WxadMmtWzZ0vH6/FqTXr16acqUKdq6davmzJmj48ePKyQkRC1bttT8+fPl4+PjeM/48ePl7u6ubt26KSsrS61atdLs2bPl5ubmqElISNDgwYMdd93q3Lmzy7NN3NzctGTJEg0YMEDNmjVTyZIl1bNnT40dO7bI52QzDMMo8ruucSXDB1o9BQAoVmkbL/yAKwC4Hnlfw78Sj3jre9M+a+0L95r2Wdeia/ivAQAAAGC+azQYuSGxgB0AAACAJUhGAAAAACfX6pqRGxHJCAAAAABLkIwAAAAATghGzEMyAgAAAMASJCMAAACAE9aMmIdkBAAAAIAlSEYAAAAAJyQj5iEZAQAAAGAJkhEAAADACcGIeUhGAAAAAFiCZgQAAACAJbhMCwAAAHDCAnbzkIwAAAAAsATJCAAAAOCEYMQ8JCMAAAAALEEyAgAAADhhzYh5SEYAAAAAWIJkBAAAAHBCMGIekhEAAAAAliAZAQAAAJyUIBoxDckIAAAAAEuQjAAAAABOCEbMQzICAAAAwBIkIwAAAIATnjNiHpIRAAAAAJYgGQEAAACclCAYMQ3JCAAAAABLkIwAAAAATlgzYh6SEQAAAACWIBkBAAAAnBCMmIdkBAAAAIAlaEYAAAAAWILLtAAAAAAnNnGdlllIRgAAAABYgmQEAAAAcMJDD81DMgIAAADAEiQjAAAAgBMeemgekhEAAAAAliAZAQAAAJwQjJiHZAQAAACAJUhGAAAAACcliEZMQzICAAAAwBIkIwAAAIATghHzkIwAAAAAsATJCAAAAOCE54yYh2QEAAAAgCVIRgAAAAAnBCPmIRkBAAAAYAmSEQAAAMAJzxkxD8kIAAAAAEvQjAAAAACwRKEu0/riiy8KPWDnzp0vezIAAACA1bhIyzyFaka6dOlSqMFsNptyc3OvZD4AAAAAbhKFakby8vKu9jwAAACAawIPPTTPFa0ZOX36dHHNAwAAAMBNpsjNSG5url577TXdcsstKlOmjHbv3i1JGj58uGbMmFHsEwQAAADMVMJm3nazK3Iz8sYbb2j27NkaM2aMPD09Hfvr1q2r6dOnF+vkAAAAANy4ityMzJkzR//+97/16KOPys3NzbG/Xr16+u2334p1cgAAAIDZbDabadvNrsjNyH//+19Vq1Yt3/68vDzl5OQUy6QAAAAA3PiK3IzUqVNHP/zwQ779n376qcLDw4tlUgAAAIBVbDbztqL4/vvv1alTJ4WGhspms2nRokWOYzk5OXrhhRdUt25dlS5dWqGhoXr88cd18OBBlzFatGiRL53p0aOHS01aWpqioqJkt9tlt9sVFRWl48ePu9Ts27dPnTp1UunSpRUQEKDBgwfrzJkzRTshFfLWvs5GjBihqKgo/fe//1VeXp4+++wz7dy5U3PmzNGXX35Z5AkAAAAAuLTMzEzVr19fvXv31kMPPeRy7NSpU9qyZYuGDx+u+vXrKy0tTUOGDFHnzp21adMml9qYmBi9+uqrjtclS5Z0Od6zZ08dOHBAiYmJkqR+/fopKipKixcvlnTuhlYdOnRQ+fLltXr1ah09elS9evWSYRiaOHFikc6pyM1Ip06dNH/+fI0ePVo2m02vvPKKGjRooMWLF+v+++8v6nAAAADANeVaXcvRrl07tWvXrsBjdrtdy5cvd9k3ceJE3XXXXdq3b58qVqzo2F+qVCkFBwcXOM6OHTuUmJiodevWqXHjxpKkadOmKSIiQjt37lTNmjW1bNky/frrr9q/f79CQ0MlSePGjVN0dLTeeOMN+fr6FvqcLus5I23atNGqVauUkZGhU6dOafXq1YqMjLycoQAAAICbVnZ2tk6cOOGyZWdnF8vY6enpstlsKlu2rMv+hIQEBQQEqE6dOho2bJhOnjzpOLZ27VrZ7XZHIyJJTZo0kd1u15o1axw1YWFhjkZEOtcfZGdna/PmzUWaY5GTkfM2bdqkHTt2yGazqVatWmrYsOHlDgUAAABcM8x8/kd8fLxGjRrlsm/EiBEaOXLkFY17+vRp/fOf/1TPnj1dkopHH31UlStXVnBwsLZt26a4uDj9/PPPjlQlJSVFgYGB+cYLDAxUSkqKoyYoKMjluJ+fnzw9PR01hVXkZuTAgQN65JFH9OOPPzq6rOPHj6tp06b6+OOPVaFChaIOCQAAANyU4uLiFBsb67LPy8vrisbMyclRjx49lJeXp8mTJ7sci4mJcfw5LCxM1atXV6NGjbRlyxY1aNBAUsGXqRmG4bK/MDWFUeTLtPr06aOcnBzt2LFDx44d07Fjx7Rjxw4ZhqG+ffsWdTgAAADgmmLmc0a8vLzk6+vrsl1JM5KTk6Nu3bppz549Wr58+SXXbzRo0EAeHh7atWuXJCk4OFiHDh3KV5eamupIQ4KDg/MlIGlpacrJycmXmFxKkZuRH374QVOmTFHNmjUd+2rWrKmJEycWeMtfAAAAAFff+UZk165d+uabb1SuXLlLvmf79u3KyclRSEiIJCkiIkLp6enasGGDo2b9+vVKT09X06ZNHTXbtm1TcnKyo2bZsmXy8vIq8tKNIl+mVbFixQIfbnj27FndcsstRR0OAAAAuKZcm/fSkjIyMvTHH384Xu/Zs0dJSUny9/dXaGio/vGPf2jLli368ssvlZub60gv/P395enpqT///FMJCQlq3769AgIC9Ouvv2ro0KEKDw9Xs2bNJEm1atVS27ZtFRMTo6lTp0o6d2vfjh07OsKIyMhI1a5dW1FRUXr77bd17NgxDRs2TDExMUW6k5Z0GcnImDFjNGjQIG3atEmGYUg6t5j9mWee0dixY4s6HAAAAIBC2LRpk8LDwx0PGo+NjVV4eLheeeUVHThwQF988YUOHDigO+64QyEhIY7t/F2wPD099e2336pNmzaqWbOmBg8erMjISH3zzTdyc3NzfE5CQoLq1q2ryMhIRUZGql69epo7d67juJubm5YsWSJvb281a9ZM3bp1U5cuXS6rF7AZ5zuKi/Dz83NZjJKZmamzZ8/K3f1csHL+z6VLl9axY8eKPIniVjJ8oNVTAIBilbZxktVTAIBi5X3Z93S9+p6Yv820z5rePcy0z7oWFeqvwYQJE67yNAAAAADcbArVjPTq1etqzwMAAADATeaKArKsrKx8i9mLumgFAAAAuJYU8VEZuAJFXsCemZmpgQMHKjAwUGXKlJGfn5/LBgAAAACFUeRm5Pnnn9eKFSs0efJkeXl5afr06Ro1apRCQ0M1Z86cqzFHAAAAwDRmPvTwZlfky7QWL16sOXPmqEWLFurTp4/uueceVatWTZUqVVJCQoIeffTRqzFPAAAAADeYIicjx44dU+XKlSWdWx9y/la+d999t77//vvinR0AAABgMpvNvO1mV+RmpEqVKtq7d68kqXbt2vrkk08knUtMypYtW5xzAwAAAHADK/JlWr1799bPP/+s5s2bKy4uTh06dNDEiRN19uxZvfPOO1djjgAAAIBpShBZmKbIzcizzz7r+HPLli3122+/adOmTapatarq169frJMDAAAAcOMq8mVaf1exYkV17dpV/v7+6tOnT3HMCQAAALAMa0bMc8XNyHnHjh3TBx98UFzDAQAAALjBXdET2AEAAIAbDc//ME+xJSMAAAAAUBQ3ZDJybMMkq6cAAMXKr+kwq6cAAMUqa8NYq6dwQfy23jyFbka6du160ePHjx+/0rkAAAAAuIkUuhmx2+2XPP74449f8YQAAAAAK7FmxDyFbkZmzZp1NecBAAAA4CZzQ64ZAQAAAC5XCYIR07A+BwAAAIAlaEYAAAAAWILLtAAAAAAnXKZlHpIRAAAAAJa4rGZk7ty5atasmUJDQ/XXX39JkiZMmKDPP/+8WCcHAAAAmM1ms5m23eyK3IxMmTJFsbGxat++vY4fP67c3FxJUtmyZTVhwoTinh8AAACAG1SRm5GJEydq2rRpeumll+Tm5ubY36hRI23durVYJwcAAACYrYTNvO1mV+RmZM+ePQoPD8+338vLS5mZmcUyKQAAAAA3viI3I5UrV1ZSUlK+/V999ZVq165dHHMCAAAALGOzmbfd7Ip8a9/nnntOTz/9tE6fPi3DMLRhwwZ9/PHHio+P1/Tp06/GHAEAAADcgIrcjPTu3Vtnz57V888/r1OnTqlnz5665ZZb9O6776pHjx5XY44AAACAaUoQWZjmsh56GBMTo5iYGB05ckR5eXkKDAws7nkBAAAAuMFd0RPYAwICimseAAAAwDWBp4Kbp8jNSOXKlS/6gJbdu3df0YQAAAAA3ByK3IwMGTLE5XVOTo5++uknJSYm6rnnniuueQEAAACWYMmIeYrcjDzzzDMF7n/vvfe0adOmK54QAAAAgJtDsV0S165dOy1YsKC4hgMAAAAsUcJmM2272RVbM/Kf//xH/v7+xTUcAAAAgBtckS/TCg8Pd1nAbhiGUlJSlJqaqsmTJxfr5AAAAACzEViYp8jNSJcuXVxelyhRQuXLl1eLFi10++23F9e8AAAAANzgitSMnD17VrfddpvatGmj4ODgqzUnAAAAwDIlSEZMU6Q1I+7u7nrqqaeUnZ19teYDAAAA4CZR5AXsjRs31k8//XQ15gIAAADgJlLkNSMDBgzQ0KFDdeDAATVs2FClS5d2OV6vXr1imxwAAABgNm65a55CNyN9+vTRhAkT1L17d0nS4MGDHcdsNpsMw5DNZlNubm7xzxIAAADADafQzcgHH3ygN998U3v27Lma8wEAAAAsRTBinkI3I4ZhSJIqVap01SYDAAAA4OZRpDUjNtpEAAAA3OC4ta95itSM1KhR45INybFjx65oQgAAAABuDkVqRkaNGiW73X615gIAAABYziaiEbMUqRnp0aOHAgMDr9ZcAAAAANxECt2MsF4EAAAANwPWjJin0E9gP383LQAAAAAoDoVORvLy8q7mPAAAAIBrAsmIeQqdjAAAAABAcSrSAnYAAADgRsdaafOQjAAAAACwBMkIAAAA4IQ1I+YhGQEAAABgCZIRAAAAwAlLRsxDMgIAAADAEjQjAAAAwHXg+++/V6dOnRQaGiqbzaZFixa5HDcMQyNHjlRoaKhKliypFi1aaPv27S412dnZGjRokAICAlS6dGl17txZBw4ccKlJS0tTVFSU7Ha77Ha7oqKidPz4cZeaffv2qVOnTipdurQCAgI0ePBgnTlzpsjnRDMCAAAAOClhs5m2FUVmZqbq16+vSZMmFXh8zJgxeueddzRp0iRt3LhRwcHBuv/++3Xy5ElHzZAhQ7Rw4ULNmzdPq1evVkZGhjp27Kjc3FxHTc+ePZWUlKTExEQlJiYqKSlJUVFRjuO5ubnq0KGDMjMztXr1as2bN08LFizQ0KFDi/hNSzbDMIwiv+sal5Vj9QwAoHj5Nxtm9RQAoFhlbRhr9RQuaMIPe0z7rCH3VL6s99lsNi1cuFBdunSRdC4VCQ0N1ZAhQ/TCCy9IOpeCBAUF6a233lL//v2Vnp6u8uXLa+7cuerevbsk6eDBg6pQoYKWLl2qNm3aaMeOHapdu7bWrVunxo0bS5LWrVuniIgI/fbbb6pZs6a++uordezYUfv371doaKgkad68eYqOjtbhw4fl6+tb6PMgGQEAAACclLCZt2VnZ+vEiRMuW3Z2dpHnvGfPHqWkpCgyMtKxz8vLS82bN9eaNWskSZs3b1ZOTo5LTWhoqMLCwhw1a9euld1udzQiktSkSRPZ7XaXmrCwMEcjIklt2rRRdna2Nm/eXLTvushnCgAAAKBYxMfHO9ZmnN/i4+OLPE5KSookKSgoyGV/UFCQ41hKSoo8PT3l5+d30ZrAwMB84wcGBrrU/P1z/Pz85Onp6agpLG7tCwAAADgx89a+cXFxio2Nddnn5eV12ePZ/jZ5wzDy7fu7v9cUVH85NYVBMgIAAABYxMvLS76+vi7b5TQjwcHBkpQvmTh8+LAjxQgODtaZM2eUlpZ20ZpDhw7lGz81NdWl5u+fk5aWppycnHyJyaXQjAAAAABOSshm2lZcKleurODgYC1fvtyx78yZM1q1apWaNm0qSWrYsKE8PDxcapKTk7Vt2zZHTUREhNLT07VhwwZHzfr165Wenu5Ss23bNiUnJztqli1bJi8vLzVs2LBI8+YyLQAAAOA6kJGRoT/++MPxes+ePUpKSpK/v78qVqyoIUOGaPTo0apevbqqV6+u0aNHq1SpUurZs6ckyW63q2/fvho6dKjKlSsnf39/DRs2THXr1lXr1q0lSbVq1VLbtm0VExOjqVOnSpL69eunjh07qmbNmpKkyMhI1a5dW1FRUXr77bd17NgxDRs2TDExMUW6k5ZEMwIAAAC4MHPNSFFs2rRJLVu2dLw+v9akV69emj17tp5//nllZWVpwIABSktLU+PGjbVs2TL5+Pg43jN+/Hi5u7urW7duysrKUqtWrTR79my5ubk5ahISEjR48GDHXbc6d+7s8mwTNzc3LVmyRAMGDFCzZs1UsmRJ9ezZU2PHFv12zTxnBACuAzxnBMCN5lp+zsjkNXtN+6wBTW8z7bOuRSQjAAAAgJMS12gyciNiATsAAAAAS5CMAAAAAE5KXKuLRm5AJCMAAAAALEEyAgAAADghGDEPyQgAAAAAS5CMAAAAAE5YM2IekhEAAAAAliAZAQAAAJwQjJiHZAQAAACAJWhGAAAAAFiCy7QAAAAAJ/y23jx81wAAAAAsQTICAAAAOLGxgt00JCMAAAAALEEyAgAAADghFzEPyQgAAAAAS5CMAAAAAE5KsGbENCQjAAAAACxBMgIAAAA4IRcxD8kIAAAAAEuQjAAAAABOWDJiHpIRAAAAAJYgGQEAAACc8AR285CMAAAAALAEyQgAAADghN/Wm4fvGgAAAIAlSEYAAAAAJ6wZMQ/JCAAAAABL0IwAAAAAsASXaQEAAABOuEjLPCQjAAAAACxBMgIAAAA4YQG7eUhGAAAAAFiCZAQAAABwwm/rzcN3DQAAAMASJCMAAACAE9aMmIdkBAAAAIAlSEYAAAAAJ+Qi5iEZAQAAAGAJkhEAAADACUtGzEMyAgAAAMASJCMAAACAkxKsGjENyQgAAAAAS5CMAAAAAE5YM2IekhEAAAAAliAZAQAAAJzYWDNiGpIRAAAAAJYgGQEAAACcsGbEPCQjAAAAACxBMwIAAADAElymBQAAADjhoYfmIRkBAAAAYAmSEQAAAMAJC9jNQzICAAAAwBIkIwAAAIATkhHzkIwAAAAAsATJCAAAAODExt20TEMyAgAAAMASJCMAAACAkxIEI6YhGQEAAABgCZoRAAAAwInNxP8UxW233SabzZZve/rppyVJ0dHR+Y41adLEZYzs7GwNGjRIAQEBKl26tDp37qwDBw641KSlpSkqKkp2u112u11RUVE6fvz4FX2nF0IzAgAAAFwHNm7cqOTkZMe2fPlySdLDDz/sqGnbtq1LzdKlS13GGDJkiBYuXKh58+Zp9erVysjIUMeOHZWbm+uo6dmzp5KSkpSYmKjExEQlJSUpKirqqpwTa0YAAAAAJ9fqc0bKly/v8vrNN99U1apV1bx5c8c+Ly8vBQcHF/j+9PR0zZgxQ3PnzlXr1q0lSR9++KEqVKigb775Rm3atNGOHTuUmJiodevWqXHjxpKkadOmKSIiQjt37lTNmjWL9ZxIRgAAAACLZGdn68SJEy5bdnb2Jd935swZffjhh+rTp49sTt3TypUrFRgYqBo1aigmJkaHDx92HNu8ebNycnIUGRnp2BcaGqqwsDCtWbNGkrR27VrZ7XZHIyJJTZo0kd1ud9QUJ5oRAAAAwImZa0bi4+MdazPOb/Hx8Zec46JFi3T8+HFFR0c79rVr104JCQlasWKFxo0bp40bN+q+++5zNDcpKSny9PSUn5+fy1hBQUFKSUlx1AQGBub7vMDAQEdNceIyLQAAAMAicXFxio2Nddnn5eV1yffNmDFD7dq1U2hoqGNf9+7dHX8OCwtTo0aNVKlSJS1ZskRdu3a94FiGYbikK7YCrlP7e01xoRkBAAAAnJj5nBEvL69CNR/O/vrrL33zzTf67LPPLloXEhKiSpUqadeuXZKk4OBgnTlzRmlpaS7pyOHDh9W0aVNHzaFDh/KNlZqaqqCgoCLNszC4TAsAAAC4jsyaNUuBgYHq0KHDReuOHj2q/fv3KyQkRJLUsGFDeXh4OO7CJUnJycnatm2boxmJiIhQenq6NmzY4KhZv3690tPTHTXFiWQEAAAAuE7k5eVp1qxZ6tWrl9zd//ejfEZGhkaOHKmHHnpIISEh2rt3r1588UUFBATowQcflCTZ7Xb17dtXQ4cOVbly5eTv769hw4apbt26jrtr1apVS23btlVMTIymTp0qSerXr586duxY7HfSkmhGAAAAABdFfRihmb755hvt27dPffr0cdnv5uamrVu3as6cOTp+/LhCQkLUsmVLzZ8/Xz4+Po668ePHy93dXd26dVNWVpZatWql2bNny83NzVGTkJCgwYMHO+661blzZ02aNOmqnI/NMAzjqoxsoawcq2cAAMXLv9kwq6cAAMUqa8NYq6dwQT/8nmbaZ91Tw+/SRTcwkhEAAADAybX60MMbEc0Ibmozpk3Vt98s0949u+Xl7a36d4RryLPDdFvlKgXWvzbqFS34dL6GvRCnx6KiJUnp6cc15b2JWrtmtQ6lpKhsWT+1vK+1Bgx6xiUW/WvvHo0fN0ZJP21RTk6OqlWvoYGDh+jOu5qYcaoAbkDDet2nLi3rqkal8srKPqv1W/fqpYlLtGtfqkvdSzGR6tulscr6lNLG7fs05O3PtGP3ubvlVAzx087PXypw/Efj5uizb3+RJFWrGKDRgzoqon5lebq7afufyRr5fqK+3/yno75hrQp6bWB7hd9+qwzD0OZf9+uliUv0y66DV+kbAHC9425auKlt3rRB3R95VHM++kTv/3uWcs/m6ql+fZV16lS+2hXffqOtv/ys8n97EFDq4cNKPXxYscNe0KefLdarb8Trxx9/0KhXXP/HfeCA/jp7Nlf/nvGBPvrkM9W8vZYGPf2kjhxx/aEBAArrngZV9P6nP6p534nqOGiq3NxK6MuJ/VTK29NRM/Txlhr8yL169u2Fujv6XR06ekJLJvZTmVLnbiV64NBx3dZulMv26tSvlXEqW1+v+c0xzsJ3+srdzU3tBryvpr0m6OffD+qzd/oqqNy5X7qUKeWlL/4Vo/0px3Vv73+pVb/3dDIzW1/8K0bubvy4geuLzcTtZse/DripTZ46Qw906apq1aqr5u23a9Tr8UpOPqhff93uUnfo0CG9OfpVjX5rrNzdPVyOVateQ+MmTFTzFvepQsWKuqtxhAYOHqJVK1fo7NmzkqS0tGPav+8v9Xmin2rUvF2VKt2mZ54dqtNZWfrzjz9MO18AN5YHnpmuD5ds0o7dh7R1V7L6vzpfFUP8FF7rVkfN0z3u0ZjZ3+rzldv06+4UPTFqnkp6e6p7m3BJUl6eoUNHT7psnVuE6T/fJCkz64wkqZy9lKpVLK9xc1Zo2x/J+nP/EQ1/b6lKl/RUrSrnnjtQo1J5+dtL6bWpidq1L1U7dh/SG9OXKaicjyoE39zXxAO4MJoRwElGxklJ5259d15eXp5ejntOvaL7qlq16oUb52SGypQp47jlXtmyfqpSpaoWf7FIWadO6ezZs/rPJ/NVrlyAatWuU/wnAuCm5FvGW5KUln4u3b0t1F8hAb76Zt1OR82ZnFz9sOVPNal3W4FjhN9+i+6oeYs++Px/zxg4mn5KO3YfUs/2DVXK21NubiX0xINNlHL0hH7acUCS9PtfqUpNy1CvBxrLw91N3l7uiu58l7b/maJ9KeYtBgaKQwmbzbTtZndNrxnZv3+/RowYoZkzZ16wJjs7W9nZ2S778koU/UmWgGEYGjcmXuENGqpa9RqO/bNmTJObm7t6PvZ4ocY5fjxN06ZO1kMPd3fss9lsmjJtlp4d9JSaNm6gEiVKyL9cOb03dbp8fX2L/VwA3JzeGtJZPybt1q+7UyRJwf9/CdXhYxkudYePZahiSMFpRa/OjbVj9yGt2/qXy/6Og6bqk7G9lbrydeXlGTp8LEMPDJ6u9IzTkqSMU9lq89QUffp2b8X1Ofe8gl37UtV58DTl5uYV63kCuHFc08nIsWPH9MEHH1y0Jj4+Xna73WV7+614k2aIG0n8G6/q999/15tj3nHs+3X7Nn304Ry9+ka8bIX47UVGRoYGDeivKlWrqv9TAx37DcNQ/Osj5VeunGZ+kKAPP/5ULVq20uCn+ys19fBVOR8AN5fxzz2outVC1OvlhHzH/n4Xf5st/z5J8vZyV/c24frgiw35jk14vqtSj2Wodb/Juqf3v7T4++367J0+jobH28tdU1/urrW/7FXzPhN1X8wk7dh9SAsnPCFvr2v6d59APqwZMY+l/zp88cUXFz2+e/fuS44RFxen2NhYl315JUhFUDRvjn5Nq75boZkffKig4GDH/i1bNunYsaNqd39Lx77c3Fy98/ZbSpg7R18tW+HYn5mZoQH9n1CpUqX0zrvvycPjf2tLNqxfp+9XrdT3azaqTJkykqSXatfRurVrtPjzRerzRD8TzhLAjeqdYV3U8d46at1/sv57ON2xP+XouUtPg8r5OP4sSeX9yuRLSyTpwfvqqZS3hxKWbnLZ3+LOamp/d22FtB6uk5nnrkYYMuYztbqruh7r0Ehj53yn7m0aqGKIn5r3nehodHoNT1Dyt6+p071h+nR5UnGfNoAbgKXNSJcuXWSz2Qr87cx5l/pttJdX/kuyeOghCsswDL05+jWt+Ha5ps+aq1tureByvGOnB9SkSVOXfU/176uOnR7QA126OvZlZGRoQP++8vDw1ISJU/L9nTx9OkuSVKKE69/nEiVsysvj8gUAl2/8sAfVuUWYIp+aor8OHnM5tvfgMSUfOaFWjWvo59/P3V7Xw91N9zSoqpcnLck3VnTnxlry/a86cjzTZX8pr3N358rLc/3f6zzDkO3//10r5e2hPMNw+d/086///m8fcM3jr6xpLL1MKyQkRAsWLFBeXl6B25YtW6ycHm4Co18fpSVffqH4t8apdOnSOnIkVUeOpOr06XPXQJct66dq1Wu4bO7uHioXEOB4FklmZoae6tdHWadOaeSrbygzM8MxTm5uriSpXv075Ovrq+Ev/lM7f/tNf+3do3fGvqX/Hviv7rm3hVWnD+A6N+H5rurRroF6DU9QxqlsBZXzUVA5H5fLot6b94Oei26lzi3CVLtKsKaN6K6s02c0/+ufXMaqcms53R1eWbM+X5/vc9Zv3au0k1maPqKH6lYPcTxz5LZQfyX+uEOS9O363+XnU1ITnu+qmrcFqlaVIP17eHedzc3Tqk3cNRBAwSxNRho2bKgtW7aoS5cuBR6/VGoCXKlP538sSXqid5TL/lGvx7skHxfz6/bt2vrLz5KkTu3vdzm25Otvdcstt8rPz1/vvT9dk/41Qf369tLZszmqWq26Jkx8TzVvv70YzgTAzaj/P84lt8unDnDZHzNqnj5ccu5Sq3FzvpO3l4cmPN9Vfj4ltXH7PnUcNE0Zp1xv/tKr0106mHpC36z/Pd/nHE0/pQeemaaRT7XTV5OflIebm3bsSdHDw2Zr665kSefupvXQ0Jl66YlIrZwxSHl5hn7+/b964JlpLpeIAdcDG9GIaWyGhT/t//DDD8rMzFTbtm0LPJ6ZmalNmzapefPmRRqXy7QA3Gj8mw2zegoAUKyyNoy1egoXtP7P9EsXFZPGVe2XLrqBWZqM3HPPPRc9Xrp06SI3IgAAAMCV4PEf5rmmb+0LAAAA4MbFjb8BAAAAJwQj5iEZAQAAAGAJkhEAAADAGdGIaUhGAAAAAFiCZgQAAACAJbhMCwAAAHDCQw/NQzICAAAAwBIkIwAAAIATHnpoHpIRAAAAAJYgGQEAAACcEIyYh2QEAAAAgCVIRgAAAABnRCOmIRkBAAAAYAmSEQAAAMAJzxkxD8kIAAAAAEuQjAAAAABOeM6IeUhGAAAAAFiCZAQAAABwQjBiHpIRAAAAAJYgGQEAAACcEY2YhmQEAAAAgCVIRgAAAAAnPGfEPCQjAAAAACxBMwIAAADAElymBQAAADjhoYfmIRkBAAAAYAmSEQAAAMAJwYh5SEYAAAAAWIJkBAAAAHBGNGIakhEAAAAAliAZAQAAAJzw0EPzkIwAAAAAsATJCAAAAOCE54yYh2QEAAAAgCVIRgAAAAAnBCPmIRkBAAAAYAmSEQAAAMAZ0YhpSEYAAAAAWIJkBAAAAHDCc0bMQzICAAAAwBIkIwAAAIATnjNiHpIRAAAAAJagGQEAAABgCS7TAgAAAJxwlZZ5SEYAAAAAWIJkBAAAAHBGNGIakhEAAADgOjBy5EjZbDaXLTg42HHcMAyNHDlSoaGhKlmypFq0aKHt27e7jJGdna1BgwYpICBApUuXVufOnXXgwAGXmrS0NEVFRclut8tutysqKkrHjx+/KudEMwIAAAA4sZn4n6KqU6eOkpOTHdvWrVsdx8aMGaN33nlHkyZN0saNGxUcHKz7779fJ0+edNQMGTJECxcu1Lx587R69WplZGSoY8eOys3NddT07NlTSUlJSkxMVGJiopKSkhQVFXVlX+oFcJkWAAAAcJ1wd3d3SUPOMwxDEyZM0EsvvaSuXbtKkj744AMFBQXpo48+Uv/+/ZWenq4ZM2Zo7ty5at26tSTpww8/VIUKFfTNN9+oTZs22rFjhxITE7Vu3To1btxYkjRt2jRFRERo586dqlmzZrGeD8kIAAAA4MRmM2/Lzs7WiRMnXLbs7OwLzm3Xrl0KDQ1V5cqV1aNHD+3evVuStGfPHqWkpCgyMtJR6+XlpebNm2vNmjWSpM2bNysnJ8elJjQ0VGFhYY6atWvXym63OxoRSWrSpInsdrujpjjRjAAAAAAWiY+Pd6zNOL/Fx8cXWNu4cWPNmTNHX3/9taZNm6aUlBQ1bdpUR48eVUpKiiQpKCjI5T1BQUGOYykpKfL09JSfn99FawIDA/N9dmBgoKOmOHGZFgAAAODEzJtpxcXFKTY21mWfl5dXgbXt2rVz/Llu3bqKiIhQ1apV9cEHH6hJkyaSJJvNdfaGYeTb93d/rymovjDjXA6SEQAAAMAiXl5e8vX1ddku1Iz8XenSpVW3bl3t2rXLsY7k7+nF4cOHHWlJcHCwzpw5o7S0tIvWHDp0KN9npaam5ktdigPNCAAAAODMZuJ2BbKzs7Vjxw6FhISocuXKCg4O1vLlyx3Hz5w5o1WrVqlp06aSpIYNG8rDw8OlJjk5Wdu2bXPUREREKD09XRs2bHDUrF+/Xunp6Y6a4sRlWgAAAMB1YNiwYerUqZMqVqyow4cP6/XXX9eJEyfUq1cv2Ww2DRkyRKNHj1b16tVVvXp1jR49WqVKlVLPnj0lSXa7XX379tXQoUNVrlw5+fv7a9iwYapbt67j7lq1atVS27ZtFRMTo6lTp0qS+vXrp44dOxb7nbQkmhEAAADAxeU8/8MMBw4c0COPPKIjR46ofPnyatKkidatW6dKlSpJkp5//nllZWVpwIABSktLU+PGjbVs2TL5+Pg4xhg/frzc3d3VrVs3ZWVlqVWrVpo9e7bc3NwcNQkJCRo8eLDjrludO3fWpEmTrso52QzDMK7KyBbKyrF6BgBQvPybDbN6CgBQrLI2jLV6Che0O/W0aZ9Vpby3aZ91LSIZAQAAAJxchZtG4QJYwA4AAADAEiQjAAAAgBOCEfOQjAAAAACwBMkIAAAA4IxoxDQkIwAAAAAsQTMCAAAAwBJcpgUAAAA4uVYfengjIhkBAAAAYAmSEQAAAMAJDz00D8kIAAAAAEuQjAAAAABOCEbMQzICAAAAwBIkIwAAAIAT1oyYh2QEAAAAgCVIRgAAAAAXRCNmIRkBAAAAYAmSEQAAAMAJa0bMQzICAAAAwBIkIwAAAIATghHzkIwAAAAAsATJCAAAAOCENSPmIRkBAAAAYAmSEQAAAMCJjVUjpiEZAQAAAGAJmhEAAAAAluAyLQAAAMAZV2mZhmQEAAAAgCVIRgAAAAAnBCPmIRkBAAAAYAmSEQAAAMAJDz00D8kIAAAAAEuQjAAAAABOeOiheUhGAAAAAFiCZAQAAABwRjBiGpIRAAAAAJYgGQEAAACcEIyYh2QEAAAAgCVIRgAAAAAnPGfEPCQjAAAAACxBMgIAAAA44Tkj5iEZAQAAAGAJkhEAAADACWtGzEMyAgAAAMASNCMAAAAALEEzAgAAAMASNCMAAAAALMECdgAAAMAJC9jNQzICAAAAwBIkIwAAAIATHnpoHpIRAAAAAJYgGQEAAACcsGbEPCQjAAAAACxBMgIAAAA4IRgxD8kIAAAAAEuQjAAAAADOiEZMQzICAAAAwBIkIwAAAIATnjNiHpIRAAAAAJYgGQEAAACc8JwR85CMAAAAALAEzQgAAADgxGbiVhTx8fG688475ePjo8DAQHXp0kU7d+50qYmOjpbNZnPZmjRp4lKTnZ2tQYMGKSAgQKVLl1bnzp114MABl5q0tDRFRUXJbrfLbrcrKipKx48fL+KML41mBAAAALgOrFq1Sk8//bTWrVun5cuX6+zZs4qMjFRmZqZLXdu2bZWcnOzYli5d6nJ8yJAhWrhwoebNm6fVq1crIyNDHTt2VG5urqOmZ8+eSkpKUmJiohITE5WUlKSoqKhiPyebYRhGsY9qsawcq2cAAMXLv9kwq6cAAMUqa8NYq6dwQadyzPvxuJTH5S9QSU1NVWBgoFatWqV7771X0rlk5Pjx41q0aFGB70lPT1f58uU1d+5cde/eXZJ08OBBVahQQUuXLlWbNm20Y8cO1a5dW+vWrVPjxo0lSevWrVNERIR+++031axZ87Ln/HckIwAAAIBFsrOzdeLECZctOzu7UO9NT0+XJPn7+7vsX7lypQIDA1WjRg3FxMTo8OHDjmObN29WTk6OIiMjHftCQ0MVFhamNWvWSJLWrl0ru93uaEQkqUmTJrLb7Y6a4kIzAgAAAFgkPj7esS7j/BYfH3/J9xmGodjYWN19990KCwtz7G/Xrp0SEhK0YsUKjRs3Ths3btR9993naHBSUlLk6ekpPz8/l/GCgoKUkpLiqAkMDMz3mYGBgY6a4sKtfQEAAAAnZj70MC4uTrGxsS77vLy8Lvm+gQMH6pdfftHq1atd9p+/9EqSwsLC1KhRI1WqVElLlixR165dLzieYRiyOd3T2FbA/Y3/XlMcaEYAAAAAi3h5eRWq+XA2aNAgffHFF/r+++916623XrQ2JCRElSpV0q5duyRJwcHBOnPmjNLS0lzSkcOHD6tp06aOmkOHDuUbKzU1VUFBQUWa66VwmRYAAADgxGYzbysKwzA0cOBAffbZZ1qxYoUqV658yfccPXpU+/fvV0hIiCSpYcOG8vDw0PLlyx01ycnJ2rZtm6MZiYiIUHp6ujZs2OCoWb9+vdLT0x01xYW7aQHAdYC7aQG40VzLd9M6fda8z/IuwnVKAwYM0EcffaTPP//c5Y5WdrtdJUuWVEZGhkaOHKmHHnpIISEh2rt3r1588UXt27dPO3bskI+PjyTpqaee0pdffqnZs2fL399fw4YN09GjR7V582a5ublJOrf25ODBg5o6daokqV+/fqpUqZIWL15cfCevG7QZAcyQnZ2t+Ph4xcXFFTleBYBrEf+uAde2C63XmDVrlqKjo5WVlaUuXbrop59+0vHjxxUSEqKWLVvqtddeU4UKFRz1p0+f1nPPPaePPvpIWVlZatWqlSZPnuxSc+zYMQ0ePFhffPGFJKlz586aNGmSypYtW7znRDMCXJ4TJ07IbrcrPT1dvr6+Vk8HAK4Y/64BMBtrRgAAAABYgmYEAAAAgCVoRgAAAABYgmYEuExeXl4aMWIEizwB3DD4dw2A2VjADgAAAMASJCMAAAAALEEzAgAAAMASNCMAAAAALEEzAgAAAMASNCPAZZo8ebIqV64sb29vNWzYUD/88IPVUwKAy/L999+rU6dOCg0Nlc1m06JFi6yeEoCbBM0IcBnmz5+vIUOG6KWXXtJPP/2ke+65R+3atdO+ffusnhoAFFlmZqbq16+vSZMmWT0VADcZbu0LXIbGjRurQYMGmjJlimNfrVq11KVLF8XHx1s4MwC4MjabTQsXLlSXLl2sngqAmwDJCFBEZ86c0ebNmxUZGemyPzIyUmvWrLFoVgAAANcfmhGgiI4cOaLc3FwFBQW57A8KClJKSopFswIAALj+0IwAl8lms7m8Ngwj3z4AAABcGM0IUEQBAQFyc3PLl4IcPnw4X1oCAACAC6MZAYrI09NTDRs21PLly132L1++XE2bNrVoVgAAANcfd6snAFyPYmNjFRUVpUaNGikiIkL//ve/tW/fPj355JNWTw0AiiwjI0N//PGH4/WePXuUlJQkf39/VaxY0cKZAbjRcWtf4DJNnjxZY8aMUXJyssLCwjR+/Hjde++9Vk8LAIps5cqVatmyZb79vXr10uzZs82fEICbBs0IAAAAAEuwZgQAAACAJWhGAAAAAFiCZgQAAACAJWhGAAAAAFiCZgQAAACAJWhGAAAAAFiCZgQAAACAJWhGAAAAAFiCZgQArtDIkSN1xx13OF5HR0erS5cups9j7969stlsSkpKumqf8fdzvRxmzBMAcH2gGQFwQ4qOjpbNZpPNZpOHh4eqVKmiYcOGKTMz86p/9rvvvqvZs2cXqtbsH8xbtGihIUOGmPJZAABcirvVEwCAq6Vt27aaNWuWcnJy9MMPP+iJJ55QZmampkyZkq82JydHHh4exfK5dru9WMYBAOBGRzIC4Ibl5eWl4OBgVahQQT179tSjjz6qRYsWSfrf5UYzZ85UlSpV5OXlJcMwlJ6ern79+ikwMFC+vr6677779PPPP7uM++abbyooKEg+Pj7q27evTp8+7XL875dp5eXl6a233lK1atXk5eWlihUr6o033pAkVa5cWZIUHh4um82mFi1aON43a9Ys1apVS97e3rr99ts1efJkl8/ZsGGDwsPD5e3trUaNGumnn3664u/shRdeUI0aNVSqVClVqVJFw4cPV05OTr66qVOnqkKFCipVqpQefvhhHT9+3OX4peYOAIBEMgLgJlKyZEmXH6z/+OMPffLJJ1qwYIHc3NwkSR06dJC/v7+WLl0qu92uqVOnqlWrVvr999/l7++vTz75RCNGjNB7772ne+65R3PnztW//vUvValS5YKfGxcXp2nTpmn8+PG6++67lZycrN9++03SuYbirrvu0jfffKM6derI09NTkjRt2jSNGDFCkyZNUnh4uH766SfFxMSodOnS6tWrlzIzM9WxY0fdd999+vDDD7Vnzx4988wzV/wd+fj4aPbs2QoNDdXWrVsVExMjHx8fPf/88/m+t8WLF+vEiRPq27evnn76aSUkJBRq7gAAOBgAcAPq1auX8cADDzher1+/3ihXrpzRrVs3wzAMY8SIEYaHh4dx+PBhR823335r+Pr6GqdPn3YZq2rVqsbUqVMNwzCMiIgI48knn3Q53rhxY6N+/foFfvaJEycMLy8vY9q0aQXOc8+ePYYk46effnLZX6FCBeOjjz5y2ffaa68ZERERhmEYxtSpUw1/f38jMzPTcXzKlCkFjuWsefPmxjPPPHPB4383ZswYo2HDho7XI0aMMNzc3Iz9+/c79n311VdGiRIljOTk5ELN/ULnDAC4+ZCMALhhffnllypTpozOnj2rnJwcPfDAA5o4caLjeKVKlVS+fHnH682bNysjI0PlypVzGScrK0t//vmnJGnHjh168sknXY5HRETou+++K3AOO3bsUHZ2tlq1alXoeaempmr//v3q27evYmJiHPvPnj3rWI+yY8cO1a9fX6VKlXKZx5X6z3/+owkTJuiPP/5QRkaGzp49K19fX5eaihUr6tZbb3X53Ly8PO3cuVNubm6XnDsAAOfRjAC4YbVs2VJTpkyRh4eHQkND8y1QL126tMvrvLw8hYSEaOXKlfnGKlu27GXNoWTJkkV+T15enqRzlzs1btzY5dj5y8kMw7is+VzMunXr1KNHD40aNUpt2rSR3W7XvHnzNG7cuIu+z2azOf5vYeYOAMB5NCMAblilS5dWtWrVCl3foEEDpaSkyN3dXbfddluBNbVq1dK6dev0+OOPO/atW7fugmNWr15dJUuW1Lfffqsnnngi3/Hza0Ryc3Md+4KCgnTLLbdo9+7devTRRwsct3bt2po7d66ysrIcDc/F5lEYP/74oypVqqSXXnrJse+vv/7KV7dv3z4dPHhQoaGhkqS1a9eqRIkSqlGjRqHmDgDAeTQjAPD/WrdurYiICHXp0kVvvfWWatasqYMHD2rp0qXq0qWLGjVqpGeeeUa9evVSo0aNdPfddyshIUHbt2+/4AJ2b29vvfDCC3r++efl6empZs2aKTU1Vdu3b1ffvn0VGBiokiVLKjExUbfeequ8vb1lt9s1cuRIDR48WL6+vmrXrp2ys7O1adMmpaWlKTY2Vj179tRLL72kvn376uWXX9bevXs1duzYQp1nampqvueaBAcHq1q1atq3b5/mzZunO++8U0uWLNHChQsLPKdevXpp7NixOnHihAYPHqxu3bopODhYki45dwAAzuPWvgDw/2w2m5YuXap7771Xffr0UY0aNdSjRw/t3btXQUFBkqTu3bvrlVde0QsvvKCGDRvqr7/+0lNPPXXRcYcPH66hQ4fqlVdeUa1atdS9e3cdPnxYkuTu7q5//etfmjp1qkJDQ/XAAw9Ikp544glNnz5ds2fPVt26ddW8eXPNnj3bcSvgMmXKaPHixfr1118VHh6ul156SW+99VahzvOjjz5SeHi4y/b+++/rgQce0LPPPquBAwfqjjvu0Jo1azR8+PB8769WrZq6du2q9u3bKzIyUmFhYS637r3U3AEAOM9mXI0LjwEAAADgEkhGAAAAAFiCZgQAAACAJWhGAAAAAFiCZgQAAACAJWhGAAAAAFiCZgQAAACAJWhGAAAAAFiCZgQAAACAJWhGAAAAAFiCZgQAAACAJWhGAAAAAFji/wA+gugLwcRYjgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVM Model\n",
    "model = LinearSVC(labelCol=\"class\")\n",
    "model_name = \"SVM\"\n",
    "\n",
    "print(f\"Training {model_name}...\")\n",
    "\n",
    "# Pipeline construction\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, model])\n",
    "\n",
    "# Train the model using the training data\n",
    "trained_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = trained_model.transform(test_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"class\").rdd.map(lambda row: (row[0], row[1]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "labels = predictions.select(\"class\").distinct().collect()\n",
    "labels = [row['class'] for row in labels]\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1Score = metrics.fMeasure(label)\n",
    "    print(f\"Class {label}:\")\n",
    "    print(f\"  Precision = {precision:.2f}\")\n",
    "    print(f\"  Recall = {recall:.2f}\")\n",
    "    print(f\"  F1 Score = {f1Score:.2f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "confusion_matrix = confusion_matrix.astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Por questões de produtividade, devem ser considerados dois conjuntos de dados aquando do desenvolvimento da solução. Assim, para além dos dados originais na sua integra, deve ser utilizado um conjunto de dados de menor dimensão (sub-conjunto dos anteriores), para o caso de tarefas intensivas e frequentes, inerentes ao próprio processo de desenvolvimento da solução.\n",
    "\n",
    "• Cada notebook (ou módulo) deverá ser autónomo em termos de fontes de dados. Sugere se que estruturem o código por forma a ler e gravar os dados entre cada uma das etapas do projeto. Isto é particularmente importante para a parte da visualização: a geração\n",
    "de um gráfico ou tabela não deverá implicar a realização da simulação/processamento no mesmo instante. Preferencialmente deverá importar os dados já processados a partir de ficheiros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/naveedhn/amazon-product-review-spam-and-non-spam/data?select=Clothing_Shoes_and_Jewelry     https://ieeexplore.ieee.org/abstract/document/9027828\n",
    "\n",
    "https://www.kaggle.com/code/abhilashsampath/amazon-review-spam-detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
