{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construção do modelo com Algoritmos de Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo 1: Regressão Logística**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/08 14:56:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/08 14:56:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- class: double (nullable = true)\n",
      "\n",
      "+--------------------+-----+\n",
      "|          reviewText|class|\n",
      "+--------------------+-----+\n",
      "|shirt was good ju...|  1.0|\n",
      "|well spent money ...|  1.0|\n",
      "|i ordered the siz...|  1.0|\n",
      "|i live in these f...|  1.0|\n",
      "|i love these this...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, length, dayofweek, when\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LogisticRegressionExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.parquet(\"dataset/text.parquet\")\n",
    "\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|  0.0| 1184|\n",
      "|  1.0| 4354|\n",
      "+-----+-----+\n",
      "\n",
      "Minimum class count: 1184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          reviewText|class|\n",
      "+--------------------+-----+\n",
      "|i love analog wat...|  0.0|\n",
      "|i had an older ci...|  0.0|\n",
      "|they are a little...|  0.0|\n",
      "|overall these a f...|  0.0|\n",
      "|it looked so arti...|  0.0|\n",
      "|it s like it s sh...|  0.0|\n",
      "|i love the way th...|  0.0|\n",
      "|looks like what i...|  0.0|\n",
      "|i was looking for...|  0.0|\n",
      "|the medium i orde...|  0.0|\n",
      "|as a tall woman l...|  0.0|\n",
      "|i didn t realize ...|  0.0|\n",
      "|i was very disapp...|  0.0|\n",
      "|no magnetic closu...|  0.0|\n",
      "|i purchased this ...|  0.0|\n",
      "|it s not lined so...|  0.0|\n",
      "|its a beautiful a...|  0.0|\n",
      "|i bought this bel...|  0.0|\n",
      "|not only the size...|  0.0|\n",
      "|like smaller than...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|  0.0| 1184|\n",
      "|  1.0| 1184|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, class: double]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import to_timestamp, col, regexp_replace\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Assuming df is already defined and loaded with data\n",
    "# Checking for class imbalance\n",
    "class_distribution = df.groupBy(\"class\").count().orderBy(\"class\")\n",
    "class_distribution.show()\n",
    "\n",
    "# Undersample the majority class\n",
    "min_class_count = class_distribution.agg({\"count\": \"min\"}).collect()[0][0]\n",
    "print(\"Minimum class count:\", min_class_count)\n",
    "\n",
    "undersampled_df = df.groupBy(\"class\").applyInPandas(\n",
    "    lambda pdf: pdf.sample(n=min_class_count, random_state=42) if len(pdf) > min_class_count else pdf,\n",
    "    schema=df.schema\n",
    ")\n",
    "\n",
    "undersampled_df.show()\n",
    "\n",
    "class_distribution_undersampled_df = undersampled_df.groupBy(\"class\").count().orderBy(\"class\")\n",
    "class_distribution_undersampled_df.show()\n",
    "\n",
    "# Preprocessing stages\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = undersampled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 14:57:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/06/08 14:57:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:57:19 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Accuracy = 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 14:57:19 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0:\n",
      "  Precision = 0.96\n",
      "  Recall = 0.91\n",
      "  F1 Score = 0.93\n",
      "Class 1.0:\n",
      "  Precision = 0.90\n",
      "  Recall = 0.95\n",
      "  F1 Score = 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 14:57:19 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "model = LogisticRegression(labelCol=\"class\")\n",
    "model_name = \"Logistic Regression\"\n",
    "\n",
    "print(f\"Training {model_name}...\")\n",
    "\n",
    "# Pipeline construction\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, model])\n",
    "\n",
    "# Train the model using the training data\n",
    "trained_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = trained_model.transform(test_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"class\").rdd.map(lambda row: (row[0], row[1]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "labels = predictions.select(\"class\").distinct().collect()\n",
    "labels = [row['class'] for row in labels]\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1Score = metrics.fMeasure(label)\n",
    "    print(f\"Class {label}:\")\n",
    "    print(f\"  Precision = {precision:.2f}\")\n",
    "    print(f\"  Recall = {recall:.2f}\")\n",
    "    print(f\"  F1 Score = {f1Score:.2f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "confusion_matrix = confusion_matrix.astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 14:53:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:53:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:53:53 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/06/08 14:54:03 WARN DAGScheduler: Broadcasting large task binary with size 1033.7 KiB\n",
      "24/06/08 14:54:03 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n",
      "24/06/08 14:54:04 WARN MemoryStore: Not enough space to cache rdd_101_0 in memory! (computed 419.0 MiB so far)\n",
      "24/06/08 14:54:04 WARN BlockManager: Persisting block rdd_101_0 to disk instead.\n",
      "24/06/08 14:54:08 WARN MemoryStore: Not enough space to cache rdd_101_0 in memory! (computed 419.0 MiB so far)\n",
      "24/06/08 14:54:10 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n",
      "24/06/08 14:54:11 WARN MemoryStore: Not enough space to cache rdd_101_0 in memory! (computed 419.0 MiB so far)\n",
      "24/06/08 14:54:13 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n",
      "24/06/08 14:54:13 WARN MemoryStore: Not enough space to cache rdd_101_0 in memory! (computed 419.0 MiB so far)\n",
      "24/06/08 14:54:16 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n",
      "24/06/08 14:54:16 WARN MemoryStore: Not enough space to cache rdd_101_0 in memory! (computed 419.0 MiB so far)\n",
      "24/06/08 14:54:16 ERROR Executor: Exception in task 0.0 in stage 48.0 (TID 85)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4534/0x0000000801b84840.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4476/0x0000000801b5f840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1658/0x0000000800c2c840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1619/0x0000000800c01440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/06/08 14:54:16 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 48.0 (TID 85),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4534/0x0000000801b84840.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4476/0x0000000801b5f840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1658/0x0000000800c2c840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1619/0x0000000800c01440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/06/08 14:54:16 WARN TaskSetManager: Lost task 0.0 in stage 48.0 (TID 85) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4534/0x0000000801b84840.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4476/0x0000000801b5f840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1658/0x0000000800c2c840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1619/0x0000000800c01440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/06/08 14:54:16 ERROR TaskSetManager: Task 0 in stage 48.0 failed 1 times; aborting job\n",
      "24/06/08 14:54:16 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 85) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4534/0x0000000801b84840.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4476/0x0000000801b5f840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1658/0x0000000800c2c840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1619/0x0000000800c01440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.DecisionTreeClassifier.$anonfun$train$1(DecisionTreeClassifier.scala:143)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:116)\n",
      "\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:48)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4534/0x0000000801b84840.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4476/0x0000000801b5f840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1658/0x0000000800c2c840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1619/0x0000000800c01440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sx/jbvz3q8j557gm8d9cx0yqmzw0000gn/T/ipykernel_61233/3423632880.py\", line 11, in <module>\n",
      "    trained_model = pipeline.fit(train_df)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 53198)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model using the training data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Make predictions on the testing data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Decision Tree Model\n",
    "model = DecisionTreeClassifier(labelCol=\"class\")\n",
    "model_name = \"Decision Tree\"\n",
    "\n",
    "print(f\"Training {model_name}...\")\n",
    "\n",
    "# Pipeline construction\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, model])\n",
    "\n",
    "# Train the model using the training data\n",
    "trained_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = trained_model.transform(test_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"class\").rdd.map(lambda row: (row[0], row[1]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "labels = predictions.select(\"class\").distinct().collect()\n",
    "labels = [row['class'] for row in labels]\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1Score = metrics.fMeasure(label)\n",
    "    print(f\"Class {label}:\")\n",
    "    print(f\"  Precision = {precision:.2f}\")\n",
    "    print(f\"  Recall = {recall:.2f}\")\n",
    "    print(f\"  F1 Score = {f1Score:.2f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "confusion_matrix = confusion_matrix.astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 14:26:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:26:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:26:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:26:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:26:49 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/06/04 14:28:06 WARN DAGScheduler: Broadcasting large task binary with size 1032.7 KiB\n",
      "24/06/04 14:28:07 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "24/06/04 14:28:11 WARN BlockManager: Block rdd_86_58 could not be removed as it was not found on disk or in memory\n",
      "24/06/04 14:28:11 ERROR Executor: Exception in task 58.0 in stage 37.0 (TID 2103)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4830/0x0000000801d18040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/06/04 14:28:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 58.0 in stage 37.0 (TID 2103),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4830/0x0000000801d18040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/06/04 14:28:11 WARN TaskSetManager: Lost task 58.0 in stage 37.0 (TID 2103) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4830/0x0000000801d18040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/06/04 14:28:11 ERROR TaskSetManager: Task 58 in stage 37.0 failed 1 times; aborting job\n",
      "24/06/04 14:28:11 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 58 in stage 37.0 failed 1 times, most recent failure: Lost task 58.0 in stage 37.0 (TID 2103) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4830/0x0000000801d18040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4830/0x0000000801d18040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x0000000800b7e040.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1655/0x0000000800c77c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sx/jbvz3q8j557gm8d9cx0yqmzw0000gn/T/ipykernel_16314/3026335176.py\", line 11, in <module>\n",
      "    trained_model = pipeline.fit(train_df)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50193)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model using the training data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Make predictions on the testing data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "model = RandomForestClassifier(labelCol=\"class\")\n",
    "model_name = \"Random Forest\"\n",
    "\n",
    "print(f\"Training {model_name}...\")\n",
    "\n",
    "# Pipeline construction\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, model])\n",
    "\n",
    "# Train the model using the training data\n",
    "trained_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = trained_model.transform(test_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"class\").rdd.map(lambda row: (row[0], row[1]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "labels = predictions.select(\"class\").distinct().collect()\n",
    "labels = [row['class'] for row in labels]\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1Score = metrics.fMeasure(label)\n",
    "    print(f\"Class {label}:\")\n",
    "    print(f\"  Precision = {precision:.2f}\")\n",
    "    print(f\"  Recall = {recall:.2f}\")\n",
    "    print(f\"  F1 Score = {f1Score:.2f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "confusion_matrix = confusion_matrix.astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient-Boosted Trees...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 14:16:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:16:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:16:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:16:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/04 14:16:53 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/06/04 14:17:56 WARN DAGScheduler: Broadcasting large task binary with size 1032.7 KiB\n",
      "24/06/04 14:17:57 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "24/06/04 14:18:03 WARN MemoryStore: Not enough space to cache rdd_77_5 in memory! (computed 951.0 MiB so far)\n",
      "24/06/04 14:18:03 WARN BlockManager: Persisting block rdd_77_5 to disk instead.\n",
      "24/06/04 14:18:04 WARN BlockManager: Block rdd_77_58 could not be removed as it was not found on disk or in memory\n",
      "24/06/04 14:18:04 ERROR Executor: Exception in task 58.0 in stage 32.0 (TID 1902)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4911/0x0000000801d22040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1469/0x0000000800b86840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1656/0x0000000800c78040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "24/06/04 14:18:04 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 58.0 in stage 32.0 (TID 1902),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4911/0x0000000801d22040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1469/0x0000000800b86840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1656/0x0000000800c78040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "24/06/04 14:18:04 WARN TaskSetManager: Lost task 58.0 in stage 32.0 (TID 1902) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4911/0x0000000801d22040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1469/0x0000000800b86840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1656/0x0000000800c78040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "24/06/04 14:18:04 ERROR TaskSetManager: Task 58 in stage 32.0 failed 1 times; aborting job\n",
      "24/06/04 14:18:04 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 58 in stage 32.0 failed 1 times, most recent failure: Lost task 58.0 in stage 32.0 (TID 1902) (computaildenuno.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4911/0x0000000801d22040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1469/0x0000000800b86840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1656/0x0000000800c78040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:367)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:201)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda$4911/0x0000000801d22040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1469/0x0000000800b86840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1656/0x0000000800c78040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sx/jbvz3q8j557gm8d9cx0yqmzw0000gn/T/ipykernel_11292/163387582.py\", line 11, in <module>\n",
      "    trained_model = pipeline.fit(train_df)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 49951)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/nuno/miniconda3/envs/learn-env-m1tf/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/nuno/.local/lib/python3.9/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model using the training data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Make predictions on the testing data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-env-m1tf/lib/python3.9/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Gradient-Boosted Trees Model\n",
    "model = GBTClassifier(labelCol=\"class\")\n",
    "model_name = \"Gradient-Boosted Trees\"\n",
    "\n",
    "print(f\"Training {model_name}...\")\n",
    "\n",
    "# Pipeline construction\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, model])\n",
    "\n",
    "# Train the model using the training data\n",
    "trained_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = trained_model.transform(test_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"class\").rdd.map(lambda row: (row[0], row[1]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "labels = predictions.select(\"class\").distinct().collect()\n",
    "labels = [row['class'] for row in labels]\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1Score = metrics.fMeasure(label)\n",
    "    print(f\"Class {label}:\")\n",
    "    print(f\"  Precision = {precision:.2f}\")\n",
    "    print(f\"  Recall = {recall:.2f}\")\n",
    "    print(f\"  F1 Score = {f1Score:.2f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "confusion_matrix = confusion_matrix.astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 14:55:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/06/08 14:55:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/06/08 14:55:32 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Test Accuracy = 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 14:55:32 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0:\n",
      "  Precision = 0.78\n",
      "  Recall = 0.64\n",
      "  F1 Score = 0.70\n",
      "Class 1.0:\n",
      "  Precision = 0.66\n",
      "  Recall = 0.79\n",
      "  F1 Score = 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 14:55:33 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxEAAAJuCAYAAADPZI/GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABGPklEQVR4nO3deXgUVdr+8bsSQidhCSRAQpCdgOyyCC9BJcgmq8goIChhHRAVEAQngwg4SiCjgMMmsu/giiIqoLigILJFARkQDZuQl1UiIYRA6veHP/qtNkCloEkn+P141TXTp06qnm7nYvJwn9NlmKZpCgAAAACyyc/XBQAAAADIW2giAAAAADhCEwEAAADAEZoIAAAAAI7QRAAAAABwhCYCAAAAgCM0EQAAAAAcoYkAAAAA4AhNBAAAAABHaCIA3LTNmzfroYceUpkyZeRyuRQeHq5GjRpp2LBhkqQTJ04of/786tq16zWvkZKSouDgYHXo0EGSNH/+fBmGIcMw9MUXX2SZb5qmKlWqJMMwFBMTk606MzMztWjRIjVv3lzFihVTQECASpQooXbt2mnVqlXKzMx0/N6dmDJliipVqqT8+fPLMAz99ttvXr3+lc/swIEDXr1udsTExMgwDFWoUEGmaWY5/9VXX7n/fc6fP9/x9Y8ePaoxY8YoMTHR0c/17NlT5cqVc3w/AMD10UQAuCmrV69WdHS0UlJSlJCQoLVr1+q1115T48aNtWLFCklS8eLF1aFDB61cuVJnzpy56nWWL1+utLQ09enTx2O8UKFCmjNnTpb5X375pX7++WcVKlQoW3VeuHBBbdq0UWxsrEqUKKEZM2Zo/fr1ev311xUZGalHHnlEq1atcvjusy8xMVGDBg1S06ZNtX79em3atCnbtWdX27ZttWnTJpUsWdKr182uQoUKKSkpSevXr89ybu7cuSpcuPANX/vo0aMaO3as4yZi1KhReu+99274vgCAq8vn6wIA5G0JCQkqX7681qxZo3z5/u+PlK5duyohIcH9uk+fPnrnnXe0ZMkSPfXUU1muM3fuXIWHh6tt27Ye4126dNGSJUs0bdo0j19C58yZo0aNGiklJSVbdQ4dOlRr1qzRggUL1KNHD49znTp10vDhw5WWlpata92I3bt3S5L69eunBg0a3JJ7FC9eXMWLF78l186OMmXKqFChQpo7d66aNWvmHv/999/11ltvqXv37po1a1aO1HL+/HkFBwerYsWKOXI/APirIYkAcFNOnTqlYsWKeTQQV/j5/d8fMa1atdIdd9yhefPmZZm3Z88ebd68WT169MhynUcffVSStGzZMvfY2bNn9c4776h3797ZqjE5OVmzZ89Wq1atsjQQV0RFRalWrVru14cOHdJjjz2mEiVKyOVyqWrVqnr11Vc9ljwdOHBAhmHolVde0cSJE1W+fHkVLFhQjRo10rfffuueFxMTo8cee0yS1LBhQxmGoZ49e0qSypUr5/7vVjExMR7LtDIzM/XSSy+pSpUqCgoKUpEiRVSrVi299tpr7jnXWs40d+5c1a5dW4GBgQoNDdVDDz2kPXv2eMzp2bOnChYsqP3796tNmzYqWLCgSpcurWHDhik9Pf26n69V79699e6773os1Vq+fLkkXXU52/79+9WrVy9FRUUpODhYpUqVUvv27bVz5073nC+++EJ33323JKlXr17uZVFjxozxqH3nzp1q2bKlChUq5G5i/rycafny5TIMQ1OnTvWoY/To0fL399e6deuy/V4B4K+MJgLATWnUqJE2b96sQYMGafPmzcrIyLjqPD8/P/Xs2VPbt2/X999/73HuSmNxtaagcOHCevjhhzV37lz32LJly+Tn56cuXbpkq8bPP/9cGRkZ6tixY7bmnzhxQtHR0Vq7dq3+9a9/6YMPPlDz5s317LPPXjVFmTZtmtatW6fJkydryZIlSk1NVZs2bXT27FlJ0vTp0/X888+73+umTZs0atSobNVyRUJCgsaMGaNHH31Uq1ev1ooVK9SnTx/bfRXx8fHq06ePqlevrnfffVevvfaafvjhBzVq1Eg//fSTx9yMjAx16NBBzZo10/vvv6/evXtr0qRJmjBhQrbr7Nq1q/z9/T2avjlz5ujhhx++6nKmo0ePKiwsTOPHj9cnn3yiadOmKV++fGrYsKH27t0rSapbt677fyPPP/+8Nm3apE2bNqlv377u61y8eFEdOnTQ/fffr/fff19jx469Zn0DBgzQsGHDtHXrVknS+vXr9dJLL+mf//ynWrRoke33CgB/aSYA3ISTJ0+a99xzjynJlGQGBASY0dHRZnx8vPn77797zP3ll19MwzDMQYMGuccyMjLMiIgIs3Hjxh5z582bZ0oyt2zZYn7++eemJHPXrl2maZrm3Xffbfbs2dM0TdOsXr262aRJk+vWOH78eFOS+cknn2TrPf3jH/8wJZmbN2/2GH/iiSdMwzDMvXv3mqZpmklJSaYks2bNmualS5fc87777jtTkrls2bKrvh+rsmXLmrGxsVlqaNKkicf7ateunXnXXXddt+4r90hKSjJN0zTPnDljBgUFmW3atPGYd+jQIdPlcpndunVzj8XGxpqSzDfffNNjbps2bcwqVapc975X6q1evbr7WvXr1zdN0zR3795tSjK/+OILc8uWLaYkc968ede8zqVLl8yLFy+aUVFR5jPPPOMev97PXql97ty5Vz1XtmxZj7ELFy6YderUMcuXL2/++OOPZnh4uNmkSROPf4cAgOsjiQBwU8LCwrRhwwZt2bJF48eP14MPPqh9+/YpLi5ONWvW1MmTJ91zy5cvr6ZNm2rJkiW6ePGiJOnjjz9WcnLydZcmNWnSRBUrVtTcuXO1c+dObdmyJdtLmW7E+vXrVa1atSx7F3r27CnTNLNsHG7btq38/f3dr68sizp48KDXamrQoIG+//57DRw4UGvWrMnWXpBNmzYpLS0ty3Kp0qVL6/7779dnn33mMW4Yhtq3b+8xVqtWLcfvo3fv3tq6dat27typOXPmqGLFirrvvvuuOvfSpUsaN26cqlWrpvz58ytfvnzKnz+/fvrppyxLruz87W9/y9Y8l8ulN998U6dOnVLdunVlmqaWLVvm8e8QAHB9NBEAvKJ+/fp67rnn9NZbb+no0aN65plndODAAY/N1dIfG6xPnTqlDz74QNIfy3sKFiyozp07X/PahmGoV69eWrx4sV5//XVVrlxZ9957b7ZrK1OmjCQpKSkpW/NPnTp11W84ioyMdJ+3CgsL83jtcrkkyasbtePi4vTKK6/o22+/VevWrRUWFqZmzZq5l+RczZU6r/Ve/vw+goODFRgY6DHmcrl04cIFR7Xed999ioqK0syZM7Vo0SL17t1bhmFcde7QoUM1atQodezYUatWrdLmzZu1ZcsW1a5d29HnFxwc7OjbnypVqqR7771XFy5cUPfu3X32jVYAkFfRRADwuoCAAI0ePVqStGvXLo9znTp1UtGiRTV37lydOHFCH374obp06aKCBQte95o9e/bUyZMn9frrr6tXr16O6mnatKkCAgK0cuXKbM0PCwvTsWPHsowfPXpUklSsWDFH97+ewMDAq25ctiY4kpQvXz4NHTpU27dv1+nTp7Vs2TIdPnxYrVq10vnz56967SvNzbXeizffx5/16tVLM2bM0OnTpxUbG3vNeYsXL1aPHj00btw4tWrVSg0aNFD9+vWzvH8712pSrmX27NlavXq1GjRooKlTp2rz5s2Ofh4A/upoIgDclKv9girJvRTlyt/eXxEYGKhu3bpp7dq1mjBhgjIyMrK1NKlUqVIaPny42rdvf91fSq8mIiJCffv21Zo1a7Rw4cKrzvn555/1ww8/SJKaNWumH3/8Udu3b/eYs3DhQhmGoaZNmzq6//WUK1fOfd8r9u3b595UfDVFihTRww8/rCeffFKnT5++5sPlGjVqpKCgIC1evNhj/MiRI1q/fr3H17B6W2xsrNq3b6/hw4erVKlS15xnGIY7ubli9erV+vXXXz3GvJnu7Ny5U4MGDVKPHj20YcMG1apVS126dLnmM0wAAFnxnAgAN+XKV7e2b99ed955pzIzM5WYmKhXX31VBQsW1ODBg7P8TJ8+fTRt2jRNnDhRd955p6Kjo7N1r/Hjx99wnRMnTtQvv/yinj17as2aNXrooYcUHh6ukydPat26dZo3b56WL1+uWrVq6ZlnntHChQvVtm1bvfjiiypbtqxWr16t6dOn64knnlDlypVvuI4/e/zxx/XYY49p4MCB+tvf/qaDBw8qISEhy/Me2rdvrxo1aqh+/foqXry4Dh48qMmTJ6ts2bKKioq66rWLFCmiUaNG6Z///Kd69OihRx99VKdOndLYsWMVGBjoTotuhcjIyGwlP+3atdP8+fN15513qlatWtq2bZv+/e9/64477vCYV7FiRQUFBWnJkiWqWrWqChYsqMjIyCxNqp3U1FR17txZ5cuX1/Tp05U/f369+eabqlu3rnr16pXttAoA/upoIgDclOeff17vv/++Jk2apGPHjik9PV0lS5ZU8+bNFRcXp6pVq2b5mTp16qhOnTrasWPHLd0gbRUYGKjVq1dryZIlWrBggfr376+UlBQVLVpU9evX19y5c92biosXL66NGzcqLi5OcXFxSklJUYUKFZSQkKChQ4d6ta5u3brp6NGjev311zVv3jzVqFFDM2bMyPIVpU2bNtU777yj2bNnKyUlRREREWrRooVGjRqlgICAa14/Li5OJUqU0H/+8x+tWLFCQUFBiomJ0bhx467ZfOSk1157TQEBAYqPj9e5c+dUt25dvfvuu+6vxL0iODhYc+fO1dixY9WyZUtlZGRo9OjR7mdFZNeAAQN06NAhbdmyRQUKFJAkVahQQbNnz9YjjzyiyZMna8iQIV56dwBw+zJM0zR9XQQAAACAvIM9EQAAAAAcoYkAAAAA4AhNBAAAAABHaCIAAAAAOEITAQAAAMARmggAAAAAjtBEAAAAAHDktnzYXNBDs31dAgB41eznH/B1CQDgVd3r3WE/yUeC6jyVY/dK2zE1x+7lTSQRAAAAABy5LZMIAAAA4IYZ/D27HT4hAAAAAI6QRAAAAABWhuHrCnI9kggAAAAAjpBEAAAAAFbsibDFJwQAAADAEZIIAAAAwIo9EbZIIgAAAAA4QhIBAAAAWLEnwhafEAAAAABHSCIAAAAAK/ZE2CKJAAAAAOAISQQAAABgxZ4IW3xCAAAAAByhiQAAAADgCMuZAAAAACs2VtsiiQAAAADgCEkEAAAAYMXGalt8QgAAAAAcIYkAAAAArNgTYYskAgAAAIAjJBEAAACAFXsibPEJAQAAAHCEJAIAAACwYk+ELZIIAAAAAI6QRAAAAABW7ImwxScEAAAAwBGSCAAAAMCKJMIWnxAAAAAAR0giAAAAACs/vp3JDkkEAAAAAEdIIgAAAAAr9kTY4hMCAAAA4AhNBAAAAABHWM4EAAAAWBlsrLZDEgEAAADAEZIIAAAAwIqN1bb4hAAAAAA4QhIBAAAAWLEnwhZJBAAAAABHSCIAAAAAK/ZE2OITAgAAAOAISQQAAABgxZ4IWyQRAAAAABwhiQAAAACs2BNhi08IAAAAgCMkEQAAAIAVeyJskUQAAAAAcIQkAgAAALBiT4QtPiEAAAAAjpBEAAAAAFbsibBFEgEAAADAEZIIAAAAwIo9Ebb4hAAAAAA4QhMBAAAAwBGWMwEAAABWLGeyxScEAAAAwBGSCAAAAMCKr3i1RRIBAAAAwBGSCAAAAMCKPRG2+IQAAAAAOEISAQAAAFixJ8IWSQQAAAAAR0giAAAAACv2RNjiEwIAAADygK+++krt27dXZGSkDMPQypUrrzm3f//+MgxDkydP9hhPT0/X008/rWLFiqlAgQLq0KGDjhw54rgWmggAAADAyjBy7nAgNTVVtWvX1tSpU687b+XKldq8ebMiIyOznBsyZIjee+89LV++XF9//bXOnTundu3a6fLly45qYTkTAAAAkAe0bt1arVu3vu6cX3/9VU899ZTWrFmjtm3bepw7e/as5syZo0WLFql58+aSpMWLF6t06dL69NNP1apVq2zXQhIBAAAAWBiGkWNHenq6UlJSPI709PQbqjszM1OPP/64hg8frurVq2c5v23bNmVkZKhly5buscjISNWoUUMbN250dC+aCAAAAMBH4uPjFRIS4nHEx8ff0LUmTJigfPnyadCgQVc9n5ycrPz586to0aIe4+Hh4UpOTnZ0L5YzAQAAABZGDj4nIi4uTkOHDvUYc7lcjq+zbds2vfbaa9q+fbvj+k3TdPwzJBEAAACAj7hcLhUuXNjjuJEmYsOGDTp+/LjKlCmjfPnyKV++fDp48KCGDRumcuXKSZIiIiJ08eJFnTlzxuNnjx8/rvDwcEf3o4kAAAAArIwcPLzk8ccf1w8//KDExET3ERkZqeHDh2vNmjWSpHr16ikgIEDr1q1z/9yxY8e0a9cuRUdHO7ofy5kAAACAPODcuXPav3+/+3VSUpISExMVGhqqMmXKKCwszGN+QECAIiIiVKVKFUlSSEiI+vTpo2HDhiksLEyhoaF69tlnVbNmTfe3NWUXTQQAAACQB2zdulVNmzZ1v76ylyI2Nlbz58/P1jUmTZqkfPnyqXPnzkpLS1OzZs00f/58+fv7O6qFJgIAAACwyMmN1U7ExMTINM1szz9w4ECWscDAQE2ZMkVTpky5qVrYEwEAAADAEZIIAAAAwCK3JhG5CUkEAAAAAEdIIgAAAAALkgh7JBEAAAAAHCGJAAAAACxIIuyRRAAAAABwhCQCAAAAsCKIsEUSAQAAAMARkggAAADAgj0R9kgiAAAAADhCEgEAAABYkETYI4kAAAAA4AhJBAAAAGBBEmGPJAIAAACAIyQRAAAAgAVJhD2SCAAAAACOkEQAAAAAVgQRtkgiAAAAADhCEwEAAADAEZYzAQAAABZsrLZHEgEAAADAEZIIAAAAwIIkwh5JBAAAAABHSCIAAAAAC5IIeyQRAAAAABwhiQAAAACsCCJskUQAAAAAcIQkAgAAALBgT4Q9kggAAAAAjpBEAAAAABYkEfZIIgAAAAA4QhIBAAAAWJBE2COJAAAAAOAISQQAAABgQRJhjyQCAAAAgCMkEQAAAIAVQYQtkggAAAAAjtBEAAAAAHCE5UwAAACABRur7ZFEAAAAAHCEJAIAAACwIImwRxIBAAAAwBGSCAAAAMCCJMIeSQQAAAAAR0giAAAAACuCCFskEQAAAAAcIYkAAAAALNgTYY8kAgAAAIAjJBEAAACABUmEPZIIAAAAAI6QRAAAAAAWJBH2aCKAP2lcLULPdKyluhXDVDK0gDrHr9Oq7w5ede6UAY3Vt1VVDZ+zSVM/3O0e792iirrcV0l3VQhT4eD8iui+UGfPX8yptwAA15R5+bK+eGeBdn3zmc79dloFi4SpdpOWuq/jYzL8/ligsOe7Ddr22Yc6lrRPaedS9PdxMxVRrpKPKweQm7CcCfiTAoH5tPPAKT0za9N157VvUFZ3Vy6ho6dSs5wLduXTuh2H9e93Em9RlQBwY75ZtVzbPl2lB3o+rYGvzFPzbv206cM39d2a99xzMtIvqHSV6mr2aF8fVgr4jmEYOXbkVSQRwJ+s3X5Ea7cfue6cyNBgTeoXrfYvfqz3nm+V5fyVVOLe6iVvSY0AcKOO/LRbVepHq3Kd/5EkFSkeoV0bP9fRpH3uObXubSFJ+u1Esk9qBJD7kUQADhmGNGdIjCa9/4P2HP7N1+UAgCOlq9RU0q4dOnXssCQp+eDPOrx3p6LuaujjyoBcxMjBI4/yaRJx5MgRzZgxQxs3blRycrIMw1B4eLiio6M1YMAAlS5d2vYa6enpSk9P9xgzL2fI8A+4VWXjL27YQ7V16XKmpln2QABAXtG4fVeln0/VtGd7yc/PT5mZmbq/c2/ViL7f16UByEN81kR8/fXXat26tUqXLq2WLVuqZcuWMk1Tx48f18qVKzVlyhR9/PHHaty48XWvEx8fr7Fjx3qM+Vdpr4CqHW5l+fiLqlMhTE+2q67oYSt9XQoA3JDdmz7Xzq8/Vacn/6nid5TT/x78WWsWTVOhomGqfV/W5ZnAX1Fe3quQU3zWRDzzzDPq27evJk2adM3zQ4YM0ZYtW657nbi4OA0dOtRjrMRjS7xWJ2DVuFqESoQEad+sru6xfP5+Gt+zoZ5qX0N39l/hw+oAwN6nS99Q4w5d3clDeJkK+u3k/+rr95fRRADINp81Ebt27dLixYuveb5///56/fXXba/jcrnkcrk8xljKhFtl6Zf7tf6Hox5jq154QEu/3K+Fn+27xk8BQO6RcfGCDMNzS6Sfn59MM9NHFQHIi3zWRJQsWVIbN25UlSpVrnp+06ZNKlmSb7ZBzisQmE8VIwq7X5cLL6Ra5UJ15ly6Dp9M1enfPffgZFzO1P+eOa+fjp51j4UXCVJ4kSBVLPnHdWqULarf0zJ0+GSqzpzz/HkAyEmV6zbShveXqHCxEipxRzklH9ivbz96W3fFPOCek3YuRWdPHtfvZ05JknsTdsEioSpYJNQndQM5ieVM9nzWRDz77LMaMGCAtm3bphYtWig8PFyGYSg5OVnr1q3T7NmzNXnyZF+Vh7+wuhWLa+1Lbd2vE3r/8TWIi9bv09+nfJWta/RtVVXPd63rfv3puPaSpH7/+VKLP//Ji9UCgDMPxD6tL96ap4/nvabUs7+pUNEw1W3WTk06Pe6es3fbRn0w89/u1+9MeUmSdF+nHop5ODbHawaQ+ximaZq+uvmKFSs0adIkbdu2TZcvX5Yk+fv7q169eho6dKg6d+58Q9cNemi2N8sEAJ+b/fwD9pMAIA/pXu8OX5dwTZWe/TjH7rX/ldY5di9v8ulXvHbp0kVdunRRRkaGTp48KUkqVqyYAgLY0wAAAADkVrniidUBAQHsfwAAAECuwJ4IezyxGgAAAIAjuSKJAAAAAHILggh7JBEAAAAAHCGJAAAAACzYE2GPJAIAAACAIyQRAAAAgAVBhD2SCAAAAACOkEQAAAAAFn5+RBF2SCIAAAAAOEISAQAAAFiwJ8IeSQQAAAAAR0giAAAAAAueE2GPJAIAAACAIzQRAAAAABxhORMAAABgwWomeyQRAAAAABwhiQAAAAAs2FhtjyQCAAAAgCM0EQAAAICFYRg5djjx1VdfqX379oqMjJRhGFq5cqX7XEZGhp577jnVrFlTBQoUUGRkpHr06KGjR496XCM9PV1PP/20ihUrpgIFCqhDhw46cuSI48+IJgIAAADIA1JTU1W7dm1NnTo1y7nz589r+/btGjVqlLZv3653331X+/btU4cOHTzmDRkyRO+9956WL1+ur7/+WufOnVO7du10+fJlR7WwJwIAAACwyK1bIlq3bq3WrVtf9VxISIjWrVvnMTZlyhQ1aNBAhw4dUpkyZXT27FnNmTNHixYtUvPmzSVJixcvVunSpfXpp5+qVatW2a6FJAIAAADwkfT0dKWkpHgc6enpXrn22bNnZRiGihQpIknatm2bMjIy1LJlS/ecyMhI1ahRQxs3bnR0bZoIAAAAwCIn90TEx8crJCTE44iPj7/p93DhwgX94x//ULdu3VS4cGFJUnJysvLnz6+iRYt6zA0PD1dycrKj67OcCQAAAPCRuLg4DR061GPM5XLd1DUzMjLUtWtXZWZmavr06bbzTdN0vMmbJgIAAACwyMk9ES6X66abBquMjAx17txZSUlJWr9+vTuFkKSIiAhdvHhRZ86c8Ugjjh8/rujoaEf3YTkTAAAAcBu40kD89NNP+vTTTxUWFuZxvl69egoICPDYgH3s2DHt2rXLcRNBEgEAAABY5NYnVp87d0779+93v05KSlJiYqJCQ0MVGRmphx9+WNu3b9eHH36oy5cvu/c5hIaGKn/+/AoJCVGfPn00bNgwhYWFKTQ0VM8++6xq1qzp/ram7KKJAAAAAPKArVu3qmnTpu7XV/ZSxMbGasyYMfrggw8kSXfddZfHz33++eeKiYmRJE2aNEn58uVT586dlZaWpmbNmmn+/Pny9/d3VAtNBAAAAGCRS4MIxcTEyDTNa56/3rkrAgMDNWXKFE2ZMuWmamFPBAAAAABHSCIAAAAAi9y6JyI3IYkAAAAA4AhJBAAAAGBBEGGPJAIAAACAIzQRAAAAABxhORMAAABgwcZqeyQRAAAAABwhiQAAAAAsCCLskUQAAAAAcIQkAgAAALBgT4Q9kggAAAAAjpBEAAAAABYEEfZIIgAAAAA4QhIBAAAAWLAnwh5JBAAAAABHSCIAAAAAC4IIeyQRAAAAABwhiQAAAAAs2BNhjyQCAAAAgCMkEQAAAIAFSYQ9kggAAAAAjpBEAAAAABYEEfZIIgAAAAA4QhMBAAAAwBGWMwEAAAAWbKy2RxIBAAAAwBGSCAAAAMCCIMIeSQQAAAAAR0giAAAAAAv2RNgjiQAAAADgCEkEAAAAYEEQYY8kAgAAAIAjJBEAAACAhR9RhC2SCAAAAACOkEQAAAAAFgQR9kgiAAAAADhCEgEAAABY8JwIeyQRAAAAABwhiQAAAAAs/AgibJFEAAAAAHCEJAIAAACwYE+EPZIIAAAAAI6QRAAAAAAWBBH2SCIAAAAAOEITAQAAAMARljMBAAAAFoZYz2SHJAIAAACAIyQRAAAAgAUPm7NHEgEAAADAEZIIAAAAwIKHzdkjiQAAAADgCEkEAAAAYEEQYY8kAgAAAIAjJBEAAACAhR9RhC2SCAAAAACOkEQAAAAAFgQR9kgiAAAAADhCEgEAAABY8JwIeyQRAAAAABwhiQAAAAAsCCLskUQAAAAAcIQkAgAAALDgORH2SCIAAAAAOEITAQAAAMCRbC1n+uCDD7J9wQ4dOtxwMQAAAICvsZjJXraaiI4dO2brYoZh6PLlyzdTDwAAAIBcLltNRGZm5q2uAwAAAMgVeNicvZvaE3HhwgVv1QEAAAAgj3DcRFy+fFn/+te/VKpUKRUsWFC//PKLJGnUqFGaM2eO1wsEAAAAcpKfkXNHXuW4iXj55Zc1f/58JSQkKH/+/O7xmjVravbs2V4tDgAAAEDu47iJWLhwod544w11795d/v7+7vFatWrpv//9r1eLAwAAAHKaYRg5duRVjpuIX3/9VZUqVcoynpmZqYyMDK8UBQAAACD3ctxEVK9eXRs2bMgy/tZbb6lOnTpeKQoAAADwFcPIuSOvytZXvFqNHj1ajz/+uH799VdlZmbq3Xff1d69e7Vw4UJ9+OGHt6JGAAAAALmI4ySiffv2WrFihT766CMZhqEXXnhBe/bs0apVq9SiRYtbUSMAAACQY9gTYc9xEiFJrVq1UqtWrbxdCwAAAIA84IaaCEnaunWr9uzZI8MwVLVqVdWrV8+bdQEAAAA+kZef35BTHDcRR44c0aOPPqpvvvlGRYoUkST99ttvio6O1rJly1S6dGlv1wgAAAAgF3G8J6J3797KyMjQnj17dPr0aZ0+fVp79uyRaZrq06fPragRAAAAyDHsibDnOInYsGGDNm7cqCpVqrjHqlSpoilTpqhx48ZeLQ4AAABA7uO4iShTpsxVHyp36dIllSpVyitFAQAAAL6Sd/OBnON4OVNCQoKefvppbd26VaZpSvpjk/XgwYP1yiuveL1AAAAAALlLtpqIokWLKjQ0VKGhoerVq5cSExPVsGFDBQYGyuVyqWHDhtq+fbt69+59q+sFAAAAbik/w8ixw4mvvvpK7du3V2RkpAzD0MqVKz3Om6apMWPGKDIyUkFBQYqJidHu3bs95qSnp+vpp59WsWLFVKBAAXXo0EFHjhxx/BllaznT5MmTHV8YAAAAgPekpqaqdu3a6tWrl/72t79lOZ+QkKCJEydq/vz5qly5sl566SW1aNFCe/fuVaFChSRJQ4YM0apVq7R8+XKFhYVp2LBhateunbZt2yZ/f/9s15KtJiI2NjbbFwQAAADgfa1bt1br1q2ves40TU2ePFkjR45Up06dJEkLFixQeHi4li5dqv79++vs2bOaM2eOFi1apObNm0uSFi9erNKlS+vTTz919DBpx3sirNLS0pSSkuJxAAAAAHmZYeTckZ6enuX36fT0dMc1JyUlKTk5WS1btnSPuVwuNWnSRBs3bpQkbdu2TRkZGR5zIiMjVaNGDfec7HLcRKSmpuqpp55SiRIlVLBgQRUtWtTjAAAAAJA98fHxCgkJ8Tji4+MdXyc5OVmSFB4e7jEeHh7uPpecnKz8+fNn+Z3dOie7HDcRI0aM0Pr16zV9+nS5XC7Nnj1bY8eOVWRkpBYuXOj0cgAAAECukpMPm4uLi9PZs2c9jri4uJuq3co0TduH2mVnzp85fk7EqlWrtHDhQsXExKh379669957ValSJZUtW1ZLlixR9+7dnV4SAAAA+EtyuVxyuVw3fZ2IiAhJf6QNJUuWdI8fP37cnU5ERETo4sWLOnPmjEcacfz4cUVHRzu6n+Mk4vTp0ypfvrwkqXDhwjp9+rQk6Z577tFXX33l9HIAAABArpKTeyK8pXz58oqIiNC6devcYxcvXtSXX37pbhDq1aungIAAjznHjh3Trl27HDcRjpOIChUq6MCBAypbtqyqVaumN998Uw0aNNCqVatUpEgRp5cDAAAAkA3nzp3T/v373a+TkpKUmJio0NBQlSlTRkOGDNG4ceMUFRWlqKgojRs3TsHBwerWrZskKSQkRH369NGwYcMUFham0NBQPfvss6pZs6b725qyy3ET0atXL33//fdq0qSJ4uLi1LZtW02ZMkWXLl3SxIkTnV4OAAAAyFWcPgQup2zdulVNmzZ1vx46dKikPx7HMH/+fI0YMUJpaWkaOHCgzpw5o4YNG2rt2rXuZ0RI0qRJk5QvXz517txZaWlpatasmebPn+/oGRGSZJimad7Mmzl06JC2bt2qihUrqnbt2jdzKa8Jemi2r0sAAK+a/fwDvi4BALyqe707fF3CNT3xzo85dq8Zf6uWY/fyppt6ToQklSlTRp06dVJoaKh69+7tjZoAAAAAn8mLeyJy2k03EVecPn1aCxYs8NblAAAAAORSjvdEAAAAALczp89M+CvyWhIBAAAA4K/htkwizrzV19clAIBXFb37KV+XAABe1X3HVF+XcE38Lbu9bDcRnTp1uu7533777WZrAQAAAJAHZLuJCAkJsT3fo0ePmy4IAAAA8CX2RNjLdhMxb968W1kHAAAAgDzittwTAQAAANwoP4IIW+wbAQAAAOAITQQAAAAAR1jOBAAAAFiwnMkeSQQAAAAAR26oiVi0aJEaN26syMhIHTx4UJI0efJkvf/++14tDgAAAMhphmHk2JFXOW4iZsyYoaFDh6pNmzb67bffdPnyZUlSkSJFNHnyZG/XBwAAACCXcdxETJkyRbNmzdLIkSPl7+/vHq9fv7527tzp1eIAAACAnOZn5NyRVzluIpKSklSnTp0s4y6XS6mpqV4pCgAAAEDu5biJKF++vBITE7OMf/zxx6pWrZo3agIAAAB8xjBy7sirHH/F6/Dhw/Xkk0/qwoULMk1T3333nZYtW6b4+HjNnj37VtQIAAAAIBdx3ET06tVLly5d0ogRI3T+/Hl169ZNpUqV0muvvaauXbveihoBAACAHOOXlyOCHHJDD5vr16+f+vXrp5MnTyozM1MlSpTwdl0AAAAAcqmbemJ1sWLFvFUHAAAAkCvwNGZ7jpuI8uXLX/fBGL/88stNFQQAAAAgd3PcRAwZMsTjdUZGhnbs2KFPPvlEw4cP91ZdAAAAgE+wJcKe4yZi8ODBVx2fNm2atm7detMFAQAAAMjdvLbkq3Xr1nrnnXe8dTkAAADAJ/wMI8eOvMprTcTbb7+t0NBQb10OAAAAQC7leDlTnTp1PDZWm6ap5ORknThxQtOnT/dqcQAAAEBOy8MBQY5x3ER07NjR47Wfn5+KFy+umJgY3Xnnnd6qCwAAAEAu5aiJuHTpksqVK6dWrVopIiLiVtUEAAAA+IwfSYQtR3si8uXLpyeeeELp6em3qh4AAAAAuZzjjdUNGzbUjh07bkUtAAAAAPIAx3siBg4cqGHDhunIkSOqV6+eChQo4HG+Vq1aXisOAAAAyGl5+atXc0q2m4jevXtr8uTJ6tKliyRp0KBB7nOGYcg0TRmGocuXL3u/SgAAAAC5RrabiAULFmj8+PFKSkq6lfUAAAAAPkUQYS/bTYRpmpKksmXL3rJiAAAAAOR+jvZEGLRlAAAAuM3xFa/2HDURlStXtm0kTp8+fVMFAQAAAMjdHDURY8eOVUhIyK2qBQAAAPA5Q0QRdhw1EV27dlWJEiVuVS0AAAAA8oBsNxHshwAAAMBfAXsi7GX7idVXvp0JAAAAwF9btpOIzMzMW1kHAAAAkCuQRNjLdhIBAAAAAJLDjdUAAADA7Y69wPZIIgAAAAA4QhIBAAAAWLAnwh5JBAAAAABHSCIAAAAAC7ZE2COJAAAAAOAITQQAAAAAR1jOBAAAAFj4sZ7JFkkEAAAAAEdIIgAAAAALvuLVHkkEAAAAAEdIIgAAAAALtkTYI4kAAAAA4AhJBAAAAGDhJ6IIOyQRAAAAABwhiQAAAAAs2BNhjyQCAAAAgCMkEQAAAIAFz4mwRxIBAAAAwBGSCAAAAMDCj00RtkgiAAAAADhCEgEAAABYEETYI4kAAAAA4AhJBAAAAGDBngh7JBEAAAAAHCGJAAAAACwIIuyRRAAAAABwhCYCAAAAgCMsZwIAAAAs+Ft2e3xGAAAAABwhiQAAAAAsDHZW2yKJAAAAAOAISQQAAABgQQ5hjyQCAAAAgCMkEQAAAICFH3sibJFEAAAAAHCEJAIAAACwIIewRxIBAAAAwBGaCAAAAMDCMHLucOLSpUt6/vnnVb58eQUFBalChQp68cUXlZmZ6Z5jmqbGjBmjyMhIBQUFKSYmRrt37/byJ0QTAQAAAOQJEyZM0Ouvv66pU6dqz549SkhI0L///W9NmTLFPSchIUETJ07U1KlTtWXLFkVERKhFixb6/fffvVoLeyIAAAAAi9z6xOpNmzbpwQcfVNu2bSVJ5cqV07Jly7R161ZJf6QQkydP1siRI9WpUydJ0oIFCxQeHq6lS5eqf//+XquFJAIAAADwkfT0dKWkpHgc6enpV517zz336LPPPtO+ffskSd9//72+/vprtWnTRpKUlJSk5ORktWzZ0v0zLpdLTZo00caNG71aN00EAAAAYOGXg0d8fLxCQkI8jvj4+KvW9dxzz+nRRx/VnXfeqYCAANWpU0dDhgzRo48+KklKTk6WJIWHh3v8XHh4uPuct7CcCQAAAPCRuLg4DR061GPM5XJdde6KFSu0ePFiLV26VNWrV1diYqKGDBmiyMhIxcbGuuf9eTmWaZpeX6JFEwEAAABY5OSeCJfLdc2m4c+GDx+uf/zjH+rataskqWbNmjp48KDi4+MVGxuriIgISX8kEiVLlnT/3PHjx7OkEzeL5UwAAABAHnD+/Hn5+Xn++u7v7+/+itfy5csrIiJC69atc5+/ePGivvzyS0VHR3u1FpIIAAAAIA9o3769Xn75ZZUpU0bVq1fXjh07NHHiRPXu3VvSHwnKkCFDNG7cOEVFRSkqKkrjxo1TcHCwunXr5tVaaCIAAAAAi9z5Ba/SlClTNGrUKA0cOFDHjx9XZGSk+vfvrxdeeME9Z8SIEUpLS9PAgQN15swZNWzYUGvXrlWhQoW8Wothmqbp1SvmAhcu+boCAPCuonc/5esSAMCr0nZM9XUJ1/RW4tEcu9cjd0Xm2L28iSQCAAAAsMitD5vLTdhYDQAAAMARkggAAADAgr9lt8dnBAAAAMARkggAAADAgj0R9kgiAAAAADhCEgEAAABYkEPYI4kAAAAA4AhJBAAAAGDBlgh7JBEAAAAAHCGJAAAAACz82BVhiyQCAAAAgCMkEQAAAIAFeyLskUQAAAAAcIQkAgAAALAw2BNhiyQCAAAAgCMkEQAAAIAFeyLskUQAAAAAcIQmAgAAAIAjLGcCAAAALHjYnD2SCAAAAACOkEQAAAAAFmystkcSAQAAAMARkggAAADAgiTCHkkEAAAAAEdIIgAAAAALg29nskUSAQAAAMARkggAAADAwo8gwhZJBAAAAABHSCIAAAAAC/ZE2COJAAAAAOAISQQAAABgwXMi7JFEAAAAAHCEJAIAAACwYE+EPZIIAAAAAI6QRAAAAAAWPCfCHkkEAAAAAEdoIgAAAAA4wnImAAAAwIKN1fZIIgAAAAA4QhIBAAAAWPCwOXskEYADc2bNVO3qVZQQ/7J7zDRNzZg2Rc1j7lGDurXUp+fj2r//Jx9WCQCeGtetqLcn99cva19W2o6pah9Ty+P8G2MfU9qOqR7HlwuGecxZM2twljkLx/fKybcBIBchiQCyadfOH/T2WytUuXIVj/F5c2Zp0YJ5evHl8SpbrpxmzZyhAX176f3Vn6hAgYI+qhYA/k+BIJd27vtViz74Vstf7XfVOWu+2a3+oxe7X1/MuJxlzpx3vtG/Znzofp2WnuH9YoFcgCDCHk0EkA3nU1MV99xwjR77kmbNnOEeN01TSxYtVN+/D1DzFi0lSS+Nm6D774vWR6s/1COdu/qqZABwW/vNj1r7zY/XnXPx4iX976nfrzsn7cJF2zkA/hpYzgRkw7iXXtR99zXR/zSK9hj/9cgRnTx5Qo0a3+Mey58/v+rVv1vf79iR02UCwA27t36UDn4Wrx9WvqBpox5V8aJZk9Quberr8Prx2vb2SMU/85AKBrt8UClw6/kZRo4deVWuTiIOHz6s0aNHa+7cudeck56ervT0dI8x098ll4s/2OAdH3+0Wnv2/KilK97Ocu7kyROSpLCwMI/xsLBiOnr0aI7UBwA3a+03P+rddTt06NhplSsVphcGttPHbwxSdLcEXcy4JEla/tEWHTh6Sv97MkXVK0Xqxafbq2blUmr3xFQfVw/AF3J1EnH69GktWLDgunPi4+MVEhLicfx7QnwOVYjbXfKxY0oY/7LGjf/3dRtT409/k2CaJt/sACDPeHvtdn3y9W79+PMxffTVLnV8arqiypZQ63uru+fMe2+jPt+8Vz/+fExvrdmmbsPnqNn/3Km77rzDh5UDt4aRg0de5dMk4oMPPrju+V9++cX2GnFxcRo6dKjHmOlPCgHv+PHH3Tp96pQe7dzJPXb58mVt27pFy5ct0fsffiJJOnnypIoXL+Gec/r0KYWFFcvxegHAG5JPpujQsdOqVKb4Nefs2HNYFzMuqVKZEkr875EcrA5AbuDTJqJjx44yDEOmaV5zzp//hvfPXK6sS5cuXPJKeYAa/s//6O2VqzzGRo+MU7kKFdSrTz/dUbq0ihUrrm83fqOqVatJkjIuXtS2rVs0eOizvigZAG5aaEgB3RFeVMdOplxzTrWKJZU/IJ+OnTybg5UBOSQvRwQ5xKdNRMmSJTVt2jR17NjxqucTExNVr169nC0KsChQoKCioip7jAUFB6tISBH3ePfHe2jOrJkqU7acypQtqzlvzFRgYKDatG3ni5IBIIsCQflVsfT/pQrlSoWpVuVSOpNyXqfPpur5AW218rNEHTtxVmUjw/Ti0+116rdz+mD995Kk8ncUU9c29bXm6x918sw5Va0YofHPdNKOPYe1KdF+1QCA249Pm4h69epp+/bt12wi7FIKIDfo1aef0tPTNe5fY5WSclY1a9XWjFlzeUYEgFyjbrWyWjt7sPt1wrN/kyQt+uBbDRq3QtUrRapbuwYqUihIySdT9OWWfXr8ubk6d/6PLy7JyLikpg2q6MlHm6pgcH4dSf5Nn3y9Sy/P/FiZmfz/NG4/BlGELcP04W/pGzZsUGpqqh544IGrnk9NTdXWrVvVpEkTR9dlOROA203Ru5/ydQkA4FVpO3LvN3tt/jnnluk1rBiSY/fyJp8mEffee+91zxcoUMBxAwEAAADcDL5h0V6u/opXAAAAALlPrn7YHAAAAJDTCCLskUQAAAAAcIQkAgAAALAiirBFEgEAAADAEZoIAAAAAI6wnAkAAACw4GFz9kgiAAAAADhCEgEAAABY8LA5eyQRAAAAABwhiQAAAAAsCCLskUQAAAAAcIQkAgAAALAiirBFEgEAAADAEZIIAAAAwILnRNgjiQAAAADgCEkEAAAAYMFzIuyRRAAAAABwhCQCAAAAsCCIsEcSAQAAAMARkggAAADAiijCFkkEAAAAAEdIIgAAAAALnhNhjyQCAAAAgCM0EQAAAAAcYTkTAAAAYMHD5uyRRAAAAABwhCQCAAAAsCCIsEcSAQAAAMARkggAAADAiijCFkkEAAAAkEf8+uuveuyxxxQWFqbg4GDddddd2rZtm/u8aZoaM2aMIiMjFRQUpJiYGO3evdvrddBEAAAAABZGDv7jxJkzZ9S4cWMFBATo448/1o8//qhXX31VRYoUcc9JSEjQxIkTNXXqVG3ZskURERFq0aKFfv/9d69+RixnAgAAAPKACRMmqHTp0po3b557rFy5cu7/bpqmJk+erJEjR6pTp06SpAULFig8PFxLly5V//79vVYLSQQAAABgYRg5d6SnpyslJcXjSE9Pv2pdH3zwgerXr69HHnlEJUqUUJ06dTRr1iz3+aSkJCUnJ6tly5buMZfLpSZNmmjjxo1e/YxoIgAAAAAfiY+PV0hIiMcRHx9/1bm//PKLZsyYoaioKK1Zs0YDBgzQoEGDtHDhQklScnKyJCk8PNzj58LDw93nvIXlTAAAAIBFTn45U1xcnIYOHeox5nK5rjo3MzNT9evX17hx4yRJderU0e7duzVjxgz16NHDPc/40yO3TdPMMnazSCIAAAAAH3G5XCpcuLDHca0momTJkqpWrZrHWNWqVXXo0CFJUkREhCRlSR2OHz+eJZ24WTQRAAAAgJWRg4cDjRs31t69ez3G9u3bp7Jly0qSypcvr4iICK1bt859/uLFi/ryyy8VHR3t7GY2WM4EAAAA5AHPPPOMoqOjNW7cOHXu3Fnfffed3njjDb3xxhuS/ljGNGTIEI0bN05RUVGKiorSuHHjFBwcrG7dunm1FpoIAAAAwMLp8xtyyt1336333ntPcXFxevHFF1W+fHlNnjxZ3bt3d88ZMWKE0tLSNHDgQJ05c0YNGzbU2rVrVahQIa/WYpimaXr1irnAhUu+rgAAvKvo3U/5ugQA8Kq0HVN9XcI1/ffY+Ry7150lg3PsXt5EEgEAAABYePmLjG5LbKwGAAAA4AhNBAAAAABHWM4EAAAAWLCayR5JBAAAAABHSCIAAAAAK6IIWyQRAAAAABwhiQAAAAAscuvD5nITkggAAAAAjpBEAAAAABY8bM4eSQQAAAAAR0giAAAAAAuCCHskEQAAAAAcIYkAAAAArIgibJFEAAAAAHCEJAIAAACw4DkR9kgiAAAAADhCEgEAAABY8JwIeyQRAAAAABwhiQAAAAAsCCLskUQAAAAAcIQkAgAAALAiirBFEgEAAADAEZoIAAAAAI6wnAkAAACw4GFz9kgiAAAAADhCEgEAAABY8LA5eyQRAAAAABwhiQAAAAAsCCLskUQAAAAAcIQkAgAAALBgT4Q9kggAAAAAjpBEAAAAAB6IIuyQRAAAAABwhCQCAAAAsGBPhD2SCAAAAACOkEQAAAAAFgQR9kgiAAAAADhCEgEAAABYsCfCHkkEAAAAAEdIIgAAAAALg10RtkgiAAAAADhCEwEAAADAEZYzAQAAAFasZrJFEgEAAADAEZIIAAAAwIIgwh5JBAAAAABHSCIAAAAACx42Z48kAgAAAIAjJBEAAACABQ+bs0cSAQAAAMARkggAAADAiiDCFkkEAAAAAEdIIgAAAAALggh7JBEAAAAAHCGJAAAAACx4ToQ9kggAAAAAjpBEAAAAABY8J8IeSQQAAAAAR0giAAAAAAv2RNgjiQAAAADgCE0EAAAAAEdoIgAAAAA4QhMBAAAAwBE2VgMAAAAWbKy2RxIBAAAAwBGSCAAAAMCCh83ZI4kAAAAA4AhJBAAAAGDBngh7JBEAAAAAHCGJAAAAACwIIuyRRAAAAABwhCQCAAAAsCKKsEUSAQAAAMARkggAAADAgudE2COJAAAAAOAISQQAAABgwXMi7JFEAAAAAHCEJAIAAACwIIiwRxIBAAAAwBGSCAAAAMCKKMIWSQQAAAAAR2giAAAAADhCEwEAAABYGDn4z42Kj4+XYRgaMmSIe8w0TY0ZM0aRkZEKCgpSTEyMdu/e7YVPJCuaCAAAACAP2bJli9544w3VqlXLYzwhIUETJ07U1KlTtWXLFkVERKhFixb6/fffvV4DTQQAAABgYRg5dzh17tw5de/eXbNmzVLRokXd46ZpavLkyRo5cqQ6deqkGjVqaMGCBTp//ryWLl3qxU/nDzQRAAAAgI+kp6crJSXF40hPT7/m/CeffFJt27ZV8+bNPcaTkpKUnJysli1busdcLpeaNGmijRs3er3u2/IrXgNvy3eF3CY9PV3x8fGKi4uTy+XydTm4zaXtmOrrEvAXwJ9rwB9y8nfJMS/Fa+zYsR5jo0eP1pgxY7LMXb58ubZv364tW7ZkOZecnCxJCg8P9xgPDw/XwYMHvVfw/0cSAdyg9PR0jR079rp/WwAAeQl/rgE5Ly4uTmfPnvU44uLissw7fPiwBg8erMWLFyswMPCa1zP+tEbKNM0sY97A39kDAAAAPuJyubKV/G3btk3Hjx9XvXr13GOXL1/WV199palTp2rv3r2S/kgkSpYs6Z5z/PjxLOmEN5BEAAAAALlcs2bNtHPnTiUmJrqP+vXrq3v37kpMTFSFChUUERGhdevWuX/m4sWL+vLLLxUdHe31ekgiAAAAgFyuUKFCqlGjhsdYgQIFFBYW5h4fMmSIxo0bp6ioKEVFRWncuHEKDg5Wt27dvF4PTQRwg1wul0aPHs3mQwC3Df5cA/K2ESNGKC0tTQMHDtSZM2fUsGFDrV27VoUKFfL6vQzTNE2vXxUAAADAbYs9EQAAAAAcoYkAAAAA4AhNBAAAAABHaCIAAAAAOEITAdyg6dOnq3z58goMDFS9evW0YcMGX5cEADfkq6++Uvv27RUZGSnDMLRy5UpflwQgl6OJAG7AihUrNGTIEI0cOVI7duzQvffeq9atW+vQoUO+Lg0AHEtNTVXt2rU1depUX5cCII/gK16BG9CwYUPVrVtXM2bMcI9VrVpVHTt2VHx8vA8rA4CbYxiG3nvvPXXs2NHXpQDIxUgiAIcuXryobdu2qWXLlh7jLVu21MaNG31UFQAAQM6hiQAcOnnypC5fvqzw8HCP8fDwcCUnJ/uoKgAAgJxDEwHcIMMwPF6bppllDAAA4HZEEwE4VKxYMfn7+2dJHY4fP54lnQAAALgd0UQADuXPn1/16tXTunXrPMbXrVun6OhoH1UFAACQc/L5ugAgLxo6dKgef/xx1a9fX40aNdIbb7yhQ4cOacCAAb4uDQAcO3funPbv3+9+nZSUpMTERIWGhqpMmTI+rAxAbsVXvAI3aPr06UpISNCxY8dUo0YNTZo0Sffdd5+vywIAx7744gs1bdo0y3hsbKzmz5+f8wUByPVoIgAAAAA4wp4IAAAAAI7QRAAAAABwhCYCAAAAgCM0EQAAAAAcoYkAAAAA4AhNBAAAAABHaCIAAAAAOEITAQAAAMARmggAuEljxozRXXfd5X7ds2dPdezYMcfrOHDggAzDUGJi4i27x5/f643IiToBALcWTQSA21LPnj1lGIYMw1BAQIAqVKigZ599Vqmpqbf83q+99prmz5+frbk5/Qt1TEyMhgwZkiP3AgDcvvL5ugAAuFUeeOABzZs3TxkZGdqwYYP69u2r1NRUzZgxI8vcjIwMBQQEeOW+ISEhXrkOAAC5FUkEgNuWy+VSRESESpcurW7duql79+5auXKlpP9bljN37lxVqFBBLpdLpmnq7Nmz+vvf/64SJUqocOHCuv/++/X99997XHf8+PEKDw9XoUKF1KdPH124cMHj/J+XM2VmZmrChAmqVKmSXC6XypQpo5dfflmSVL58eUlSnTp1ZBiGYmJi3D83b948Va1aVYGBgbrzzjs1ffp0j/t89913qlOnjgIDA1W/fn3t2LHjpj+z5557TpUrV1ZwcLAqVKigUaNGKSMjI8u8mTNnqnTp0goODtYjjzyi3377zeO8Xe0AgLyNJALAX0ZQUJDHL8T79+/Xm2++qXfeeUf+/v6SpLZt2yo0NFQfffSRQkJCNHPmTDVr1kz79u1TaGio3nzzTY0ePVrTpk3Tvffeq0WLFuk///mPKlSocM37xsXFadasWZo0aZLuueceHTt2TP/9738l/dEINGjQQJ9++qmqV6+u/PnzS5JmzZql0aNHa+rUqapTp4527Nihfv36qUCBAoqNjVVqaqratWun+++/X4sXL1ZSUpIGDx58059RoUKFNH/+fEVGRmrnzp3q16+fChUqpBEjRmT53FatWqWUlBT16dNHTz75pJYsWZKt2gEAtwETAG5DsbGx5oMPPuh+vXnzZjMsLMzs3LmzaZqmOXr0aDMgIMA8fvy4e85nn31mFi5c2Lxw4YLHtSpWrGjOnDnTNE3TbNSokTlgwACP8w0bNjRr16591XunpKSYLpfLnDVr1lXrTEpKMiWZO3bs8BgvXbq0uXTpUo+xf/3rX2ajRo1M0zTNmTNnmqGhoWZqaqr7/IwZM656LasmTZqYgwcPvub5P0tISDDr1avnfj169GjT39/fPHz4sHvs448/Nv38/Mxjx45lq/ZrvWcAQN5BEgHgtvXhhx+qYMGCunTpkjIyMvTggw9qypQp7vNly5ZV8eLF3a+3bdumc+fOKSwszOM6aWlp+vnnnyVJe/bs0YABAzzON2rUSJ9//vlVa9izZ4/S09PVrFmzbNd94sQJHT58WH369FG/fv3c45cuXXLvt9izZ49q166t4OBgjzpu1ttvv63Jkydr//79OnfunC5duqTChQt7zClTpozuuOMOj/tmZmZq79698vf3t60dAJD30UQAuG01bdpUM2bMUEBAgCIjI7NsnC5QoIDH68zMTJUsWVJffPFFlmsVKVLkhmoICgpy/DOZmZmS/lgW1LBhQ49zV5ZdmaZ5Q/Vcz7fffquuXbtq7NixatWqlUJCQrR8+XK9+uqr1/05wzDc/5md2gEAeR9NBIDbVoECBVSpUqVsz69bt66Sk5OVL18+lStX7qpzqlatqm+//VY9evRwj3377bfXvGZUVJSCgoL02WefqW/fvlnOX9kDcfnyZfdYeHi4SpUqpV9++UXdu3e/6nWrVaumRYsWKS0tzd2oXK+O7Pjmm29UtmxZjRw50j128ODBLPMOHTqko0ePKjIyUpK0adMm+fn5qXLlytmqHQCQ99FEAMD/17x5czVq1EgdO3bUhAkTVKVKFR09elQfffSROnbsqPr162vw4MGKjY1V/fr1dc8992jJkiXavXv3NTdWBwYG6rnnntOIESOUP39+NW7cWCdOnNDu3bvVp08flShRQkFBQfrkk090xx13KDAwUCEhIRozZowGDRqkwoULq3Xr1kpPT9fWrVt15swZDR06VN26ddPIkSPVp08fPf/88zpw4IBeeeWVbL3PEydOZHkuRUREhCpVqqRDhw5p+fLluvvuu7V69Wq99957V31PsbGxeuWVV5SSkqJBgwapc+fOioiIkCTb2gEAeR9f8QoA/59hGProo4903333qXfv3qpcubK6du2qAwcOKDw8XJLUpUsXvfDCC3ruuedUr149HTx4UE888cR1rztq1CgNGzZML7zwgqpWraouXbro+PHjkqR8+fLpP//5j2bOnKnIyEg9+OCDkqS+fftq9uzZmj9/vmrWrKkmTZpo/vz57q+ELViwoFatWqUff/xRderU0ciRIzVhwoRsvc+lS5eqTp06Hsfrr7+uBx98UM8884yeeuop3XXXXdq4caNGjRqV5ecrVaqkTp06qU2bNmrZsqVq1Kjh8RWudrUDAPI+w7wVC2sBAAAA3LZIIgAAAAA4QhMBAAAAwBGaCAAAAACO0EQAAAAAcIQmAgAAAIAjNBEAAAAAHKGJAAAAAOAITQQAAAAAR2giAAAAADhCEwEAAADAEZoIAAAAAI78P4nbEVKwiz84AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVM Model\n",
    "model = LinearSVC(labelCol=\"class\")\n",
    "model_name = \"SVM\"\n",
    "\n",
    "print(f\"Training {model_name}...\")\n",
    "\n",
    "# Pipeline construction\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, model])\n",
    "\n",
    "# Train the model using the training data\n",
    "trained_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = trained_model.transform(test_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"{model_name} Test Accuracy = {accuracy:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"class\").rdd.map(lambda row: (row[0], row[1]))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "labels = predictions.select(\"class\").distinct().collect()\n",
    "labels = [row['class'] for row in labels]\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1Score = metrics.fMeasure(label)\n",
    "    print(f\"Class {label}:\")\n",
    "    print(f\"  Precision = {precision:.2f}\")\n",
    "    print(f\"  Recall = {recall:.2f}\")\n",
    "    print(f\"  F1 Score = {f1Score:.2f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "confusion_matrix = confusion_matrix.astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{model_name} Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Por questões de produtividade, devem ser considerados dois conjuntos de dados aquando do desenvolvimento da solução. Assim, para além dos dados originais na sua integra, deve ser utilizado um conjunto de dados de menor dimensão (sub-conjunto dos anteriores), para o caso de tarefas intensivas e frequentes, inerentes ao próprio processo de desenvolvimento da solução.\n",
    "\n",
    "• Cada notebook (ou módulo) deverá ser autónomo em termos de fontes de dados. Sugere se que estruturem o código por forma a ler e gravar os dados entre cada uma das etapas do projeto. Isto é particularmente importante para a parte da visualização: a geração\n",
    "de um gráfico ou tabela não deverá implicar a realização da simulação/processamento no mesmo instante. Preferencialmente deverá importar os dados já processados a partir de ficheiros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/naveedhn/amazon-product-review-spam-and-non-spam/data?select=Clothing_Shoes_and_Jewelry     https://ieeexplore.ieee.org/abstract/document/9027828\n",
    "\n",
    "https://www.kaggle.com/code/abhilashsampath/amazon-review-spam-detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
